% -*- root: Dissertation.tex -*-
\documentclass[Dissertation.tex]{subfiles} 
\begin{document}

\chapter{Extra Numerics}
\label{chp:ExtraNumerics}


\section{Short-Range Code Nonlinear Parameter Optimization}
\label{sec:BoundOptimization}

A powerful property of the Rayleigh-Ritz variational method is the 
ability to systematically improve the wavefunction to lower the upper bound 
on the energy. By either adding terms to the expansion in
\ref{eq:BoundWavefn_psi} or changing the nonlinear parameters $\alpha$,
$\beta$ and $\gamma$, the energy can be reduced and a possible minimum
found. This is a three-dimensional optimization problem, and we tried 
multiple methods for the nonlinear parameter optimization.

\subsection{Newton's Method}
\label{sec:Newton}
From Ref. \cite{Yan1999} (and easily derived), if the wavefunction $\Psi$
is properly normalized, the energy can be minimized by setting the
derivative with respect to each of the nonlinear parameters equal to 0 by
\beq
\frac{\partial E}{\partial \alpha} = 2 \left< \Psi \Big| H \Big| \frac{\partial\Psi}{\partial \alpha} \right> - 2 \left< \Psi \Big| \frac{\partial\Psi}{\partial \alpha} \right>.
\label{eq:EnergyDerivativeNorm}
\eeq
With this, the wavefunction is minimized using the 1-D Newton's method for 
each parameter separately \cite{Sauer2006}. Newton's method was unstable for 
anything other than a small basis set ($\omega \leq 4$), so I evaluated other 
methods. Newton's method for n-dimensions can be used as well, though it 
requires computation of the Jacobian.

\subsection{Broyden's Method}
\label{sec:Broyden}

The next method I tried was Broyden's method \cite{Sauer2006}, which can 
solve for all three nonlinear parameters simultaneously. This was more stable 
than than 1-D Newton method. The second Broyden's method, sometimes referred 
to as the ``bad Broyden's method'' was used here. As Kvaalen points out, this 
method is perfectly usable and can be faster than the first Broyden's method 
\cite{Kvaalen1991}. \Cref{table:NonlinearOptimized3SBroyden,BroydenPWaveSingOpt}
show the nonlinear parameters used for $^3$S and $^1$P. All other partial waves
used the simplex method described in \cref{sec:Simplex}. This is because I
already had a number of results for $^3$S and $^1$P, so running everything
again for simplex-optimized nonlinear parameters was unnecessary. For $^3$S,
I was able to use even more terms than I could use for $^1$S \cref{tab:Nonlinear},
and I was able to use the same number of terms for $^1$P and $^3$P.

\setlength{\abovecaptionskip}{6pt}   % 0.5cm as an example
\setlength{\belowcaptionskip}{6pt}   % 0.5cm as an example
\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$\omega$ & $\alpha$ & $\beta$ & $\gamma$ \\ [0.5ex]
\midrule
1 & 0.264440 & 0.831645 & 0.498871 \\
2 & 0.356175 & 0.452426 & 0.829591 \\
3 & 0.347611 & 0.467298 & 0.814971 \\
4 & 0.323300 & 0.333783 & 0.974653 \\
\bottomrule
\end{tabular}
\caption{Broyden optimized $^3S$ nonlinear parameters}
\label{table:NonlinearOptimized3SBroyden}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$\omega$ & $\alpha$ & $\beta$ & $\gamma$ \\ [0.5ex]
\midrule
1 & 0.47767 & 0.50273 & 0.97498 \\
2 & 0.48253 & 0.49342 & 0.96874 \\
3 & 0.42803 & 0.43099 & 0.98993 \\
4 & 0.39740 & 0.37617 & 0.96205 \\
\bottomrule
\end{tabular}
\caption{Broyden optimized $^1$P nonlinear parameters}
\label{tab:BroydenPWaveSingOpt}
\end{table}



\subsection{Simplex Method}
\label{sec:Simplex}

%\setlength{\abovecaptionskip}{6pt}   % 0.5cm as an example
%\setlength{\belowcaptionskip}{6pt}   % 0.5cm as an example
\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$\omega$ & $\alpha$ & $\beta$ & $\gamma$ \\ [0.5ex]
\midrule
0 & 0.30226 & 0.45479 & 1.07962 \\
1 & 0.53592 & 0.59453 & 1.02206 \\
2 & 0.57450 & 0.65222 & 0.98020 \\
3 & 0.58966 & 0.63150 & 0.97397 \\
4 & 0.58493 & 0.60995 & 0.98610 \\
5 & 0.58691 & 0.58045 & 1.03321 \\
\bottomrule
\end{tabular}
\caption{Simplex optimized $^1S$ nonlinear parameters}
\label{tab:NonlinearOptimized1SSimplex}
\end{table}

Broyden's method was more stable than Newton's method for this work, but I 
also tried the \texttt{gsl\_multimin\_fminimizer\_nmsimplex} routine from the 
GNU Scientific Library, which is an implementation of the simplex method
\cite{GSL,GSLsimplex}. This was the most stable of the three methods tried to 
optimize $\alpha$, $\beta$, and $\gamma$ simultaneously. I normally stopped 
at $\omega = 5$ for the optimization, and the S-wave singlet runs are shown 
in \cref{tab:NonlinearOptimized1SSimplex}. \Cref{tab:NonlinearOptimizedPD} 
has the optimized nonlinear parameters used for the P-wave and D-wave. Due to 
the slowness of the general short-range code (see \cref{sec:GeneralShort}), 
the F-wave through G-wave just use the parameters $\alpha = 0.5$, $\beta = 0.6$,
and $\gamma = 1.1$.

\begin{table}[H]
\small
\centering
\begin{tabular}{c c c c}
\toprule
Partial Wave & $\alpha$ & $\beta$ & $\gamma$ \\
\midrule
$^3$P & 0.310 & 0.311 & 0.995 \\
$^1$D & 0.359 & 0.368 & 0.976 \\
$^3$D & 0.356 & 0.365 & 0.976 \\
\bottomrule
\end{tabular}
\caption{Simplex optimized nonlinear parameters for the P-wave and D-wave}
\label{tab:NonlinearOptimizedPD}
\end{table}

With the work on the second formalism for the P-wave \cref{sec:PWaveOpt}, it 
was also possible to use the simplex method to optimize all 6 nonlinear 
parameters simultaneously instead of having to optimize each symmetry 
separately. %If the sector-based method of Drake and Yan \cite{Yan1995} is 
%ever implemented for this problem, it would be possible to optimize all 15 
%parameters simultaneously, but it may be much more computationally feasible 
%to optimize each sector separately.


\section{P-Wave Nonlinear Parameter Optimization}
\label{sec:PWaveOpt}

Using the simplex method described in \cref{sec:Simplex}, we optimized the nonlinear parameters for both the first and second formalisms. \Cref{tab:SimplexPWaveSingOpt,tab:SimplexPWaveTripOpt} show the results of these optimizations in the second and third columns. We also let each symmetry have its own set of nonlinear parameters for each symmetry in the fourth and fifth columns. This was inspired by the work of Yan and Ho \cite{Yan1999}, where they used 5 different sets of nonlinear parameters to calculate the PsH ground state energy.

\begin{table}[H]
\footnotesize
\centering
\begin{tabular}{c c c c c}
\toprule
\toprule
$\omega$ & 1st formalism / 1 set & 2nd formalism / 1 set & 1st formalism / 2 sets & 2nd formalism / 2 sets \\
\midrule
\midrule
 &  &  & 0.5341, 0.4536, 1.0139 & 0.5668, 0.4686, 0.9787 \\
1 & 0.4776, 0.5027, 0.9749 & 0.4571, 0.5700, 0.9266 & 0.3792, 0.5455, 0.9816 & 0.6777, 0.8587, 0.4813 \\
 & $\textbf{-0.666819968640}$ & $\textbf{-0.663226610680}$ & $\textbf{-0.670015702237}$ & $\textbf{-0.665355147531}$ \\
\midrule
 &  &  & 0.5270, 0.4394, 1.0036 & 0.4497, 0.5039, 0.9459 \\
2 & 0.4825, 0.4934, 0.9687 & 0.4734, 0.5162, 0.9584 & 0.4087, 0.5213, 0.9704 & 0.3963, 1.0233, 0.4327 \\
 & $\textbf{-0.700681070987}$ & $\textbf{-0.699190666285}$ & $\textbf{-0.701936448530}$ & $\textbf{-0.700245066225}$ \\
\midrule
 &  &  & 0.4632, 0.3918, 1.001 & 0.4653, 0.4512, 0.9905 \\
3 & 0.4297, 0.4337, 0.9808 & 0.4317, 0.4564, 0.9621 & 0.3844, 0.4620, 0.9740 & 0.8745, 0.9796, 0.4957 \\
 & $\textbf{-0.718093418924}$ & $\textbf{-0.717282613790}$ & $\textbf{-0.718548496648}$ & $\textbf{-0.717931026880}$ \\
\midrule
 &  &  & 0.3954, 0.3505, 0.9997 & 0.3744, 0.3746, 0.9537 \\
4 & 0.3740, 0.3744, 0.9898 & 0.3803, 0.3951, 0.9648 & 0.3478, 0.39493, 0.9798 & 0.4078, 0.9010, 0.3351 \\
 & $\textbf{-0.727918723553}$ & $\textbf{-0.727281885394}$ & $\textbf{-0.728067443345}$ & $\textbf{-0.727981667586}$ \\
\midrule
 &  &  & 0.3401, 0.3181, 0.9983 & 0.3373, 0.3390, 0.9635 \\
5 & 0.3293, 0.3289, 0.9939 & 0.3371, 0.3468, 0.9680 & 0.3174, 0.3397, 0.9880 & 0.4299, 0.9452, 0.3023 \\
 & $\textbf{-0.734233160953}$ & $\textbf{-0.733680013812}$ & $\textbf{-0.734264232997}$ & $\textbf{-0.734219573964}$ \\
\bottomrule
\bottomrule
\end{tabular}
\caption{Simplex P-Wave Singlet Short-Range Optimization}
\label{tab:SimplexPWaveSingOpt}
\end{table}


\begin{table}[H]
\footnotesize
\centering
\begin{tabular}{c c c c c}
\toprule
\toprule
$\omega$ & 1st formalism / 1 set & 2nd formalism / 1 set & 1st formalism / 2 sets & 2nd formalism / 2 sets \\
\midrule
\midrule
 &  &  & 0.3832, 0.2911, 0.9894 & 0.3367, 0.3436, 0.9517 \\
1 & 0.3316, 0.3535, 0.9956 & 0.3302, 0.3540, 0.9909 & 0.2854, 0.3959, 1.0112 & 0.7606, 0.6593, 0.2304 \\
 & $\textbf{-0.624190839847}$ & $\textbf{-0.622936676609}$ & $\textbf{-0.629448013148}$ & $\textbf{-0.626442778437}$ \\
\midrule
 &  &  & 0.4056, 0.3294, 0.9799 & 0.3623, 0.3952, 0.9716 \\
2 & 0.3664, 0.3750, 0.9900 & 0.3679, 0.3896, 0.9753 & 0.3226, 0.4097, 1.0078 & 0.1961, 0.7724, 0.7692 \\
 & $\textbf{-0.674819577647}$ & $\textbf{-0.672880514312}$ & $\textbf{-0.676837948726}$ & $\textbf{-0.673438894110}$ \\
\midrule
 &  &  & 0.3874, 0.3314, 0.9823 & 0.3557, 0.3794, 0.9593 \\
3 & 0.3585, 0.3613, 0.9911 & 0.3639, 0.3810, 0.9690 & 0.3257, 0.3881, 1.0036 & 0.2080, 0.5994, 0.7711 \\
 & $\textbf{-0.704764841366}$ & $\textbf{-0.703191879865}$ & $\textbf{-0.705411707053}$ & $\textbf{-0.703452729525}$ \\
\midrule
 &  &  & 0.3539, 0.3192, 0.9879 & 0.3392, 0.3502, 0.9789 \\
4 & 0.3357, 0.3366, 0.9936 & 0.3427, 0.3554, 0.9680 & 0.3162, 0.3539, 0.9989 & 0.1624, 0.6564, 0.9512 \\
 & $\textbf{-0.721696022987}$ & $\textbf{-0.720513417195}$ & $\textbf{-0.721859567659}$ & $\textbf{-0.720596381145}$ \\
\midrule
 &  &  & 0.3184, 0.3038, 0.9935 & 0.3177, 0.3271, 0.9769 \\
5 & 0.3106, 0.3109, 0.9953 & 0.3182, 0.3275, 0.9682 & 0.3025, 0.3186, 0.9966 & 0.1142, 0.4274, 1.1088 \\
 & $\textbf{-0.731463030326}$ & $\textbf{-0.730592482596}$ & $\textbf{-0.731486455300}$ & $\textbf{-0.730615490923}$ \\
\bottomrule
\bottomrule
\end{tabular}
\caption{Simplex $^3$P Short-Range Optimization}
\label{tab:SimplexPWaveTripOpt}
\end{table}

From \cref{tab:SimplexPWaveSingOpt,tab:SimplexPWaveTripOpt}, we see that lower energy eigenvalues are always obtained with the first formalism with 1 set versus the second formalism with 1 set. Likewise, the first formalism has lower energy eigenvalues than the second formalism when both have 2 sets. For $\omega \geq 2$, the first formalism with 1 set even has lower energy eigenvalues than the second symmetry with 2 sets. We had trouble obtaining phase shifts with two different sets of nonlinear parameters, but for higher $\omega$, we also see that the energy does not change much. Using more than one set of nonlinear parameters could be explored further in future work, and we have done preliminary investigation into this for the S-wave.





\section{Nonlinear Parameters and Terms Used}
\label{sec:NonlinParam}

\begin{table}[H]
  \centering
  \begin{tabular}{cclccccc}
	\toprule
	Partial wave & $\omega$ & $N^\prime(\omega)$ & $\alpha$ & $\beta$ & $\gamma$ & $\mu$ & $m_\ell$ \\
	\midrule
	$^1S$                      & 7 & 1505        & 0.568 & 0.580 & 1.093 & 0.9 & 1 \\
	$^3S$                      & 7 & 1633        & 0.323 & 0.334 & 0.975 & 0.9 & 1 \\
	$^1P$                      & 7 & 1000        & 0.397 & 0.376 & 0.962 & 0.9 & 3 \\
	$^3P$                      & 7 & 1000        & 0.310 & 0.311 & 0.995 & 0.9 & 3 \\
	$^1D$ $(\kappa < 0.3)$     & 6 & 916         & 0.359 & 0.368 & 0.976 & 0.7 & 7 \\
	$^1D$ $(\kappa \geq 0.3)$  & 6 & 913         & 0.600 & 0.368 & 0.976 & 0.7 & 7 \\
	$^3D$ $(\kappa < 0.3)$     & 6 & 919         & 0.356 & 0.365 & 0.976 & 0.7 & 7 \\
	$^3D$ $(\kappa \geq 0.3)$  & 6 & 913         & 0.600 & 0.365 & 0.976 & 0.7 & 7 \\
	$^1F$ $(\kappa < 0.4)$     & 5 & $385^\star$ & 0.359 & 0.368 & 0.976 & 0.7 & 7 \\
	$^1F$ $(\kappa \geq 0.4)$  & 5 & 462         & 0.500 & 0.600 & 1.100 & 0.7 & 7 \\
	$^3F$ $(\kappa < 0.4)$     & 5 & $385^\star$ & 0.356 & 0.365 & 0.976 & 0.7 & 7 \\
    $^3F$ $(\kappa \geq 0.4)$  & 5 & 462         & 0.600 & 0.365 & 0.976 & 0.7 & 7 \\
	$^1G$ $(\kappa < 0.45)$    & 5 & 462         & 0.359 & 0.368 & 0.976 & 0.7 & 9 \\
    $^1G$ $(\kappa \geq 0.45)$ & 5 & 462         & 0.500 & 0.600 & 1.100 & 0.7 & 9 \\
	$^3G$ $(\kappa < 0.45)$    & 5 & 462         & 0.356 & 0.365 & 0.976 & 0.7 & 9 \\
    $^3G$ $(\kappa \geq 0.45)$ & 5 & 462         & 0.600 & 0.365 & 0.976 & 0.7 & 9 \\
	$^1H$ $(\kappa < 0.5)$     & 5 & 462         & 0.359 & 0.368 & 0.976 & 0.7 & 11 \\
	$^1H$ $(\kappa \geq 0.5)$  & 5 & 462         & 0.500 & 0.600 & 1.100 & 0.7 & 11 \\
	$^3H$ $(\kappa < 0.45)$    & 5 & 462         & 0.356 & 0.365 & 0.976 & 0.7 & 11 \\
    $^3H$ $(\kappa \geq 0.45)$ & 5 & 462         & 0.600 & 0.365 & 0.976 & 0.7 & 11 \\
	\bottomrule
  \end{tabular}
  \caption[Parameters for each partial wave]{Parameters for each partial wave. Numbers marked with a star indicate the
restriction in the $r_3$ power described in \cref{sec:Restricted}.}
  \label{tab:Nonlinear}
\end{table}



\section{L\"uchow and Kleindienst Algorithm}
\label{sec:LuchowBound}
The algorithm that L\"uchow and Kleindienst propose \cite{Luchow1992} is 
similar to Todd's algorithm. They use all terms through $\omega = 5$, since 
none of these have linear dependence within this set. This fact could be 
applied to Allan Todd's algorithm to speed up the calculation. The rest of 
the terms are partitioned up into blocks of 300-400 terms, except the last 
block, which will have modulus(N - 462, block size) terms. A key difference 
from Allan Todd's algorithm is that the terms should be ordered in terms of 
increasing $\omega$. In Allan Todd's algorithm, the initial ordering does not 
matter. The terms in the first block are added to the basis set for
$\omega = 5$, which has 462 terms. If we use a block with 300 terms, then 300 matrices 
of size $463 \times 463$ are created, and their eigenvalues are determined. Whichever 
term gives the lowest energy when added to this set is saved to the basis 
set. This continues with ever-increasing matrix sizes, until all terms in the 
block are used, or a linear dependence is noticed. L\"uchow and Kleindienst 
propose that linear dependences can be prevented by omitting terms that do 
not contribute significantly to the energy. The energy from the previous step 
is saved, and the energy from the current step is compared to this. If the 
difference in energies is small, e.g. $\Delta E < 10^{-10} E_h$ in their 
paper, then that term is omitted. Since the lowest energy is chosen at each 
step, when this $\Delta E$ is too small, the rest of the block can be omitted.

Two parameters that are adjustable in this algorithm are the block size and 
cutoff value for the energy. To determine the best values for each, we
set $\omega = 7$, and one of the parameters was held constant. To 
determine the optimal block size, an energy cutoff of $5\e{-9}$ was used.
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Block size & Used terms & Energy & Time (min)\\
\hline
200 & 1041 & -0.789 181 890 & 470 \\
250 & 1024 & -0.789 184 500 & 572 \\
300 & 960 &  -0.789 180 520 & 749 \\
350 & 999 &  -0.789 181 942 & unknown \\
400 & 1012 & -0.789 185 588 & 852 \\
500 & 985 &  -0.789 184 116 & 764 \\
\hline
\end{tabular}
\end{center}

For most trials, increasing the block size also increased the time it took to 
complete the computation. It can be seen that a block size of N-462 gives a 
modified version of Todd's algorithm, since we are testing all terms at each 
step. A block size of 1 would correspond to no reordering of terms, leading 
to including problematic terms. The cutoff value for $\Delta E$,
$E_{\text{cutoff}}$, is $10^{-10} E_h$ in L\"uchow and Kleindienst's paper
\cite{Luchow1992}, but a value of $10^{-10}$ in our calculations led to LAPACK 
errors due to linear dependences. The block size was set to 400, and it was
determined that an energy cutoff of $10^{-9}$ was the largest value that caused linear 
dependences.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Energy cutoff & Used terms & Energy\\
\hline
2\e{-9} & 1116 & -0.789 185 858 \\
3\e{-9} & 1053 & -0.789 186 346 \\
4\e{-9} & 1043 & -0.789 185 173 \\
5\e{-9} & 1012 & -0.789 185 588 \\
1\e{-8} & 963 &  -0.789 188 024 \\
\hline
\end{tabular}
\end{center}

Unfortunately, the intuition that a larger block size and smaller energy 
cutoff will yield a better final energy is not necessarily correct. The best 
value of the energy was actually obtained for the largest tested energy 
cutoff of $10^{-8}$. A block size of 500 also gave a worse energy than did a 
block size of 400. More tests need to be performed to determine the optimal 
value of both parameters, and these could possibly change for higher values 
of $\omega$ or for a different system.

We decided to use the Todd algorithm (\cref{sec:ToddBound}) due to the better 
energy obtained and the larger set of terms returned. The Todd algorithm is 
also more consistent between runs. The trade-off is the increased time to 
perform a calculation, especially for large $\omega$.

\begin{figure}[H]
	\centering
	{\includegraphics[height=2in]{Luchow}}
	\caption{Diagram of L\"uchow and Kleindienst procedure}
	\label{fig:Luchow}
\end{figure}



\section{Gaussian Quadratures}
\label{sec:GaussQuad}
\todoi{Mention looking into \cite{Bailey2004,Ma1996,Mori2001,Odrzywolek2011,Fukuda2005}}
Gaussian quadratures are used to integrate many classes of integrals. In their
most general form, these quadratures are given by \cite[p.887]{Abramowitz1965}
\beq
\label{eq:GeneralQuadratures}
\int_a^b W(x) f(x) dx \approx \sum_{i=1}^n w_i f(x_i).
\eeq
Gaussian quadratures are particularly attractive, since they give exact 
results for polynomials up to degree $2n-1$. The weight function $W(x)$ can 
be chosen for certain classes of integrals. Three main types of weight 
functions are used in this work. For a discussion on the number of quadrature 
points used, refer to \cref{sec:QuadraturePoints}.


\subsection{Gauss-Legendre Quadrature}
\label{sec:GaussLegendre}
If the weight function is chosen as $W(x)=1$, and the integration interval is $(-1,1)$, this is known as Gauss-Legendre quadrature (or sometimes known simply as Gaussian quadrature). The orthogonal polynomials used are the Legendre polynomials, $P_n(x)$. \Cref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussLeg}
\int_{-1}^1 f(x) dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where the $x_i$ abscissas are the $i^{th}$ zeros of $L_n(x)$, and the weights are given by
\beq
\label{eq:GaussLegWeights}
w_i = \frac{2}{(1-x_i^2)[P^\prime_n(x_i)]^2}.
\eeq
The limits of integration must be from $-1$ to $1$, but this is generalized by using the transformation \cite{Abramowitz1965}
\begin{align}
\label{eq:GaussLegGen}
\int_a^b f(x) dx &= \frac{b-a}{2} \int_{-1}^1 f \left(\frac{b-a}{2} x + \frac{a+b}{2}\right) dx \\
&\approx \frac{b-a}{2} \sum_{i=1}^n w_i f \left(\frac{b-a}{2} x_i + \frac{a+b}{2}\right).
\end{align}


\subsection{Gauss-Laguerre Quadrature}
\label{sec:GaussLag}
The Gauss-Legendre quadrature cannot be used on semi-infinite intervals, so we use the Gauss-Laguerre quadrature in these cases. The orthogonal polynomials in this case are the Laguerre polynomials, $L_n(x)$, and the weight function is $W(x) = e^{-x}$. \Cref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussLag}
\int_0^\infty e^{-x} f(x) dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where the $x_i$ abscissas are the $i^{th}$ zeros of $L_n(x)$, and the weights are given by
\beq
\label{eq:GaussLagWeights}
w_i = \frac{x_i}{(n+1)^2 [L_{n+1}(x_i)]^2}.
\eeq

When the integration is over the interval $(a,\infty)$, \cref{eq:GaussLag} is easily transformed by
\beq
\label{eq:GaussLagGen1}
\int_a^\infty e^{-x} f(x) dx = \int_0^\infty e^{-(x+a)} f(x+a) dx = e^{-a} \int_0^\infty e^{-x} f(x+a) dx \approx e^{-a} \sum_{i=1}^n w_i f(x_i+a).
\eeq

\noindent A more general form of this is obtained by using a coefficient in the exponential, i.e.
\beq
\label{eq:GaussLagGen2}
\int_a^\infty e^{-m x} f(x) dx = \frac{1}{m} \int_a^\infty e^{-y} f\left(\frac{y}{m}\right) dy,
\eeq
where we have defined $y = m x$.  This allows for \cref{eq:GaussLagGen1} to be generalized to
\begin{align}
\label{eq:GaussLagGen}
\nonumber \int_a^\infty e^{-m x} f(x) dx &= \frac{1}{m} \int_{ma}^\infty e^{-y} f\left(\frac{y}{m}\right) dy = \frac{1}{m} \int_0^\infty e^{-(y+ma)} f\left(\frac{y}{m}+a\right) dy \\
& = \frac{e^{-ma}}{m} \int_0^\infty e^{-y} f\left(\frac{y}{m}+a\right) dy \approx \frac{e^{-ma}}{m} \sum_{i=1}^n w_i f\left(\frac{y_i}{m}+a\right).
\end{align}
The $y_i$ abscissas and $w_i$ weights are the same as the less general case in
\cref{eq:GaussLag,eq:GaussLagWeights}. This general form of Gauss-Laguerre
quadrature is what we use for our semi-infinite integrations.


\subsection{Chebyshev--Gauss Quadrature}
\label{sec:ChebyshevGauss1}
If the weight function is chosen as $W(x)=\frac{1}{\sqrt{1-x^2}}$, and the
integration interval is $(-1,1)$, this is known as Chebyshev-Gauss quadrature.
The orthogonal polynomials used are the Chebyshev polynomials of the first
kind, $T_n(x)$. \Cref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussCheb}
\int_{-1}^1 \frac{f(x)}{\sqrt{1-x^2}} dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where
\beq
\label{eq:GaussChebAbsWeights}
x_i = \cos\left(\frac{2i-1}{n}\pi\right) \text{ and } w_i = \frac{\pi}{n}.
\eeq
This quadrature is used for the internal angular integrations. A discussion of
how to use this for these integrations is found in \cref{sec:ChebyshevGauss}.



\section{Selection of Quadrature Points}
\label{sec:SelQuadPoints}

\todoi{Show how compare to Peter's set - including information from the Mathematica notebook}

The number of quadrature points for each coordinate in the 6-dimensional 
integrations is critical to have fully converged results. In our testing, the 
$r_1$ coordinate (the coordinate of $e^+$) was the most important, requiring 
more integration points than any other. The $r_2$ and $r_3$ coordinates were 
the second most important, and the interparticle terms ($r_{12}$, $r_{13}$ 
and $r_{23}$) were the least important. These results only apply to the
long-long and long-short terms, as the short-short terms are integrated using 
the same asymptotic expansion method as the bound state problem
(see \cref{sec:MatrixShort}).

To determine this, we held the number of integration points fixed, except for 
one coordinate, which we increased in steps. The difference in the output 
between steps was used to analyze how important each coordinate was. Also, 
some terms are more sensitive to the number of integration points than other. 
For instance, the $(\bar{S}_\ell,\mathcal{L} \bar{S}_\ell)$ term converges 
relatively quickly, while the $(\bar{C}_\ell,\mathcal{L}_\ell \bar{C}_\ell)$ 
term requires more integration points.

We also used a sample input file from Van Reeth for the number of quadrature 
points he used in his code. In figure \ref{fig:OriginalPhaseShifts-pvr}, this 
set of points was used. After approximately 525 short-range terms, it is 
immediately apparent that the phase shifts diverge, with a particularly large 
deviation in the inverse Kohn output. The Kohn and complex Kohn methods 
converge again shortly before 600 terms and stay converged up to 1000 terms.

Adding 5 points to all integrations yields a much more well-behaved graph in 
figure \ref{fig:OriginalPhaseShifts-pvrplus5}. There are slight variations, 
one of which can be seen in the inset graph at about 540 terms, but the 
results are overall converged up to 1100 terms.

Increasing the number of points can have a huge impact on performance. Adding 
5 points to an $\omega = 0$ calculation takes 3.5 hours, while adding 10 
points makes it take 6.1 hours, or approximately twice the time. So we have 
to make a trade-off between accuracy and performance.

To determine an optimal set of points, 10 points were added to Van Reeth's 
set in all coordinates, and a run was done with this set. Then 5 points were 
subtracted from one coordinate from this set for each run, once for each 
coordinate. The difference in the matrix elements and phase shifts was small 
for the 2nd, 3rd and 4th coordinates. A final run using 5 points extra from 
Van Reeth's set for the 2nd, 3rd and 4th coordinates, along with 10 points 
extra in all other coordinates, showed small differences from the first set 
that had 10 points extra in all coordinates. Reducing the points of any of 
the other coordinates resulted in larger differences from the set with 10 
extra points in all coordinates. This final set, referred to as the ``optimal 
set'', is used in all of our current runs. Increasing the number of 
integration points further than could potentially lead to better accuracy, 
though numerical instabilities start appearing with 15 extra points over the 
base set.


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{OriginalPhaseShifts-pvr}
	\caption{Phase shifts with original ordering $(\omega = 7)$}
	\label{fig:OriginalPhaseShifts-pvr}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{OriginalPhaseShifts-pvrplus5}
	\caption{Phase shifts with original ordering and 5 extra points $(\omega = 7)$}
	\label{fig:OriginalPhaseShifts-pvrplus5}
\end{figure}



\section{Selection of Quadrature Points}
\label{sec:SelQuadPoints2}

\begin{figure}[H]
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/ColorKey.pdf}}
	\caption{Color key for differences}
	\label{fig:ColorKey}
\end{figure}

For the P-wave, we noticed that the phase shifts for smaller $\kappa$ values were more sensitive to the number of integration points than higher $\kappa$ values. In general, the smaller the value of $\kappa$, the smaller the phase shift will be. The smaller phase shifts for the P-wave also caused it to be more sensitive than the S-wave.
\todoi{Mention how DevIL \cite{DevIL} used for these}

\subsection{Comparison Program}
I wrote the program \emph{Comparison} \cite{} to visually compare two runs with different numbers of integration points. This program calculates the relative difference between similar matrix elements of the two input files. The relative difference is given by
\beq
\text{diff}_{rel} = \left| \frac{elem_1 - elem_2}{(elem_1 + elem_2) / 2} \right|.
\eeq
Since the values of matrix elements can range over many orders of magnitude, the relative difference is an appropriate measure of the change caused by varying the number of integration points. The other option would have been to use the absolute difference, which is given by 
\beq
\text{diff}_{abs} = \left| elem_1 - elem_2 \right|.
\eeq
However, this is a poor measure of the differences, and the relative differences give much more useful information for our purposes.

The \emph{Comparison} program calculates these relative differences for each matrix element between the two input files and creates an image using the colors in \cref{fig:ColorKey}. So if a column of pixels in the output image is yellow, then the relative difference for that matrix element is on the order of $10^{-6}$. This is an example image from running this program:
\begin{figure}[H]
	\centering
	\resizebox{0.8\textwidth}{!}{\includegraphics{QuadPoints/Example.png}}
	\caption{Example matrix element comparison image}
	\label{fig:QuadExample}
\end{figure}
This only looks at the output from the long-range programs, and this is the first row (or column) of the $A$ matrix. The \emph{Comparison} program outputs a vertical line for each matrix element to make them more visible than outputting a single pixel. More examples and descriptions of the command line syntax are available on the \htmladdnormallink{Wiki}{http://129.120.30.66/wiki/index.php/Long-range_graphical_comparison_program} \cite{Wiki}.

\section{Application to the P-Wave}

Using the comparison program, we found that the $1/r_{23}$ term in the potential needed more integration points when $q_i = 0$. The integrations for the other three potential terms already produced reasonably converged results. All runs in this section are performed with $\kappa = 0.1$.

If we took the set of integration points that we used in the S-wave problem as our base set, then compared this with the same set but an extra 5 points in the $1^{st}$ coordinate, the resulting difference image for the $A$ matrix looks like this:
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/BasevsBaseplus5.png}}
	\caption{Base set versus base set plus 5 in $r_1$}
	\label{fig:BasevsBaseplus5}
\end{figure}
\noindent Notice that there is little blue and plenty of yellows, oranges and reds. The goal is to get as much black and blue as possible, though it is impossible to get entirely black and blue with our limited precision. It is evident that the matrix elements are not well-converged when we compare the phase shifts from these two runs. For the first run with the base set, the phase shift is 0.02346176. The corresponding phase shift for the second run is 0.02294583. We desire three significant figures in our phase shifts, so this is certainly not good enough.

Doing a similar run with the base set and another with the base set plus 10 in $r_1$, the difference image is given by figure \ref{fig:BasevsBaseplus10}.
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/BasevsBaseplus10.png}}
	\caption{Base set versus base set plus 10 in $r_1$}
	\label{fig:BasevsBaseplus10}
\end{figure}
\noindent This image is not much different from the previous image. Even when we compare the ``plus 5'' run with the ``plus 10'' run \textbf{@TODO: Where is this image?}, the image does not change much. The phase shift for the ``plus 10'' run is 0.02257283, which is still significantly different from the ``plus 5'' run.

We tried adding up to 25 extra points to the $1^{st}$ coordinate of the base set, but the matrix elements (and consequently, the phase shifts) still did not converge well enough. To perform the Gauss-Laguerre and Gauss-Legendre quadratures described in section \ref{sec:GaussQuad}, we have to compute the abscissae and weights for the Laguerre and Legendre polynomials. The weights depend on the abscissae, and the abscissae depend only on $n$, the number of quadrature points to use. This allows us to hardcode values for the abscissae and weights to speed up computations marginally. Previously, the long-range code used hardcoded values for some multiples of 5 but not all that were used. This code also did not have values hardcoded when the value of $n$ was not a multiple of 5. The integration points were then changed to use values that were multiples of 5, and all were hardcoded. The hardcoded values were also done in extended precision by using \emph{Mathematica}.

Figure \ref{fig:Base5vsBase5hardcoderound} shows the changes going from a previous ``plus 5'' run to a ``plus 5'' run with all hardcoded abscissae and weights. There is a relatively large difference in the matrix elements by making this change.
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/Base5vsBase5hardcoderound.png}}
	\caption{Changes going to hardcoded abscissae and weights}
	\label{fig:Base5vsBase5hardcoderound}
\end{figure}

The results were much better for the base set versus the ``plus 5'' set when hardcoded values for the abscissae and weight were used. Figure \ref{fig:BasehardcodevsBase5hardcode} shows this comparison, which has the large sections of black. All black and dark blue values are for terms where $q_i > 0$. This integration is done separately, as discussed in section \ref{sec:Swaveqigt0} for the S-wave.
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/BasehardcodevsBase5hardcode.png}}
	\caption[Base set with hardcoded values vs. base set plus 5 with hardcoded values]{Base set with hardcoded values versus base set plus 5 with hardcoded values}
	\label{fig:BasehardcodevsBase5hardcode}
\end{figure}

We then completed a series of runs where we increased the number of integration points for each coordinate for $\omega = 2$. Figure \ref{fig:PlusCoord1-d} shows that the $1/r_{23}$ integration needs an extra 15 points in the first coordinate. The runs for the other coordinates were all performed with an extra 15 points in the first coordinate as well. From figures \ref{fig:PlusCoord2} through \ref{fig:PlusCoord8}, we see that the $5^{th}$ through $8^{th}$ coordinates possibly need more integration points.

\begin{figure}[H]
\centering
\subfloat[Part 1][Base vs. plus 5]{\includegraphics[height=1.2in]{QuadPoints/R23-BasevsBaseplus5-1st.png} \label{fig:PlusCoord1-a}}
\subfloat[Part 2][Plus 5 vs. plus 10]{\includegraphics[height=1.2in]{QuadPoints/R23-Baseplus5vsBaseplus10-1st.png} \label{fig:PlusCoord1-b}}
\subfloat[Part 3][Plus 10 vs. plus 15]{\includegraphics[height=1.2in]{QuadPoints/R23-Baseplus10vsBaseplus15-1st.png} \label{fig:PlusCoord1-c}}
\subfloat[Part 4][Plus 15 vs. plus 20]{\includegraphics[height=1.2in]{QuadPoints/R23-Baseplus15vsBaseplus20-1st.png} \label{fig:PlusCoord1-d}}
\caption{Comparison of extra points in $1^{st}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord1}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-2nd-plus5vsplus10.png} \label{fig:PlusCoord2-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-2nd-plus5vsplus10.png} \label{fig:PlusCoord2-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-2nd-plus5vsplus10.png} \label{fig:PlusCoord2-c}}
\caption{Comparison of extra points in $2^{nd}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord2}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-3rd-plus5vsplus10.png} \label{fig:PlusCoord3-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-3rd-plus5vsplus10.png} \label{fig:PlusCoord3-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-3rd-plus5vsplus10.png} \label{fig:PlusCoord3-c}}
\caption{Comparison of extra points in $3^{rd}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord3}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-4th-plus5vsplus10.png} \label{fig:PlusCoord4-a}}
\caption{Comparison of extra points in $4^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord4}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-5th-plus5vsplus10.png} \label{fig:PlusCoord5-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-5th-plus5vsplus10.png} \label{fig:PlusCoord5-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-5th-plus5vsplus10.png} \label{fig:PlusCoord5-c}}
\caption{Comparison of extra points in $5^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord5}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-6th-plus5vsplus10.png} \label{fig:PlusCoord6-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-6th-plus5vsplus10.png} \label{fig:PlusCoord6-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-6th-plus5vsplus10.png} \label{fig:PlusCoord6-c}}
\caption{Comparison of extra points in $6^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord6}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-7th-plus5vsplus10.png} \label{fig:PlusCoord7-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-7th-plus5vsplus10.png} \label{fig:PlusCoord7-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-7th-plus5vsplus10.png} \label{fig:PlusCoord7-c}}
\caption{Comparison of extra points in $7^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord7}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-8th-plus5vsplus10.png} \label{fig:PlusCoord8-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-8th-plus5vsplus10.png} \label{fig:PlusCoord8-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-8th-plus5vsplus10.png} \label{fig:PlusCoord8-c}}
\caption{Comparison of extra points in $8^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord8}
\end{figure}

These tests give us an idea of what coordinates need more integration points. The phase shifts, not the matrix elements, are what we are concerned with, so we ran a number of tests for the phase shifts for $\omega = 6$. Table \ref{tab:5thcoordExtraPoints} shows how the phase shift stabilizes when the $5^{th}$ coordinate has an extra 10-15 points. For table \ref{tab:678thcoordExtraPoints}, we have 15 extra points in the $1^{st}$ and $5^{th}$ coordinates, then add points to all of the $6^{th}$, $7^{th}$ and $8^{th}$ coordinates simultaneously. To the required precision (3 significant figures), the phase shift does not change with this last set of changes. From these runs, we finally determined that the $1^{st}$ and $5^{th}$ coordinates needed 15 extra integration points each, and we can save adding extra points to the other coordinates.

\begin{table}[H]
\centering
\begin{tabular}{c c}
\toprule
Extra points & Phase shift \\
\midrule
+5 & 0.02273585 \\
+10 & 0.02256803 \\
+15 & 0.02257100 \\
+20 & 0.02258546 \\
+25 & 0.02257720 \\
+30 & 0.02257869 \\
+35 & 0.02257844 \\
\bottomrule
\end{tabular}
\caption{$5^{th}$ Coordinate Comparisons}
\label{tab:5thcoordExtraPoints}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{c c}
\toprule
Extra points & Phase shift \\
\midrule
+5 & 0.02255816 \\
+10 & 0.02255351 \\
+15 & 0.02255181 \\
\bottomrule
\end{tabular}
\caption{Extra points in the $6^{th}$, $7^{th}$ and $8^{th}$ coordinates}
\label{tab:678thcoordExtraPoints}
\end{table}

\todoi{Table of final integration points}


\subsection{Quadrature Points}
\label{sec:QuadraturePoints}

\todoi{Put in information from ``Integration Point Numbers.nb''}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord\\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$ & 45 & 35 & 35 & 35 & 28 & 15 & & \\
 Long-long, $r_{23}^{-1}$ & 65 & 35 & 28 & 35 & 28 & 12 & 15 & 15 \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$ & 90 & 57 & 34 & 57 & 34 & 30 & 30 & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$ & 90 & 58 & 30 & 55 & 35 & 33 & 33 & 33 \\
 Long-short $q_i > 0$ & 90 & 57 & 34 & 57 & 34 & 30 & 30 & 30 \\
\bottomrule
\end{tabular}
\caption{Base set of effective coordinates for integrations}
\label{tab:BaseEffectiveCoords}
\end{table}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord\\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$			&  75 & 40 & 40 & 40 & 40 & 25 & & \\
 Long-long, $r_{23}^{-1}$				&  75 & 40 & 40 & 40 & 40 & 25 & 25 & 25 \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$	& 100 & 65 & 45 & 65 & 45 & 45 & 45 & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$	& 115 & 65 & 45 & 65 & 45 & 45 & 45 & 45 \\
 Long-short $q_i > 0$					& 100 & 65 & 45 & 65 & 45 & 45 & 45 & 45 \\
\bottomrule
\end{tabular}
\caption{Optimal set of effective coordinates for integrations}
\label{tab:OptimalEffectiveCoords}
\end{table}

Describing the number of points used in integrating the different coordinates 
in sections \ref{sec:LongLongInt} and \ref{sec:ShortLongInt} can be 
confusing, so we have taken to grouping the sets of points as in
\cref{tab:BaseEffectiveCoords,tab:OptimalEffectiveCoords}. Each column of 
this table is referred to as an ``effective coordinate''.

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord \\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$ & $x$ Lag & $y$ Lag & $z$ Lag & $r_3$ Leg & $r_3$ Leg & $r_{13}$ Leg & & \\
 Long-long, $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $\varphi_{12}$ Che & $r_{13}$ Leg & $r_{23}$ Leg \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $r_{13}$ Leg & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $\varphi_{13}$ Che & $r_{23}$ Leg \\
 Long-short $q_i > 0$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $r_{13}$ Leg & $\varphi_{23}$ Che \\
\bottomrule
\end{tabular}
\caption{Optimal set of effective coordinates for integrations}
\todoi{Order of these tables and caption on last one. Describe tables more.}
\label{tab:EffectiveCoords}
\end{table}


\section{Extra Schwartz Singularity Discussion}
\label{sec:ExtraSchwartz}

Interestingly, adding terms will change which Kohn method has a Schwartz 
singularity. \Cref{fig:PhaseTau-Omega} shows how increasing $\omega$ (adding 
terms) will change the $\tau$ in the generalized Kohn where a Schwartz 
singularity occurs. \Cref{fig:PhaseTau-Kappa} also shows how for the 
same set of short-range terms, varying $\kappa$ will change the $\tau$ value 
where a these singularities occur.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{PhaseTau-Omega}
	\caption{Phase shifts versus $\tau$ for $\kappa = 0.1$ and varying $\omega$ values}
	\label{fig:PhaseTau-Omega}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{PhaseTau-Kappa}
	\caption{Phase shifts versus $\tau$ for $\omega = 7$ and varying $\kappa$ values}
	\label{fig:PhaseTau-Kappa}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{Det-Omega=6,Kappa=01}
	\caption[Determinant of $\textbf{\emph{A}}$ versus $\tau$]{Determinant of $\textbf{\emph{A}}$ versus $\tau$ for $\omega = 6$ with 875 terms of Todd reordering}
	\label{fig:Det-Omega=6}
\end{figure}

Cooper et al. \cite{Cooper2009} showed that if $\Det{\textbf{\emph{A}}}$ is 
plotted with respect to $\tau$, there are actually two $\tau$ values where
$\Det{\textbf{\emph{A}}} \approx 0$, but only one of them corresponds to a 
Schwartz singularity. They refer to the second root as an anomaly-free 
singularity. \Cref{fig:Det-Omega=6} shows an equivalent plot for our 
data. The roots of $\Det{\textbf{\emph{A}}}$ are at approximately
$\tau = 0.125$ and $\tau = 1.144$. The first root corresponds well with the
singularity in the $\omega = 6$ graph in \cref{fig:PhaseTau-Omega}. The second
root is referred to as an ``anomaly-free singularity'' in
Cooper et al.\ \cite{Cooper2009} and does not correspond to a Schwartz
singularity. The roots are easily found by fitting the data to equation (20)
and solving equation (21) in Cooper et al.\ \cite{Cooper2009}.


\biblio
\end{document}
