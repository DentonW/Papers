\documentclass[Dissertation.tex]{subfiles} 
\begin{document}


\chapter{Computation}
\label{chp:Computation}

\todoi{Should mention the Mathematica notebooks to generate Fortran lists for short-range matrix elements and possibly other programs}
\todoi{Put in information from ``Integration Point Numbers.nb''}
\todoi{Be clear that in the long-range code, we only calculate the Kohn elements, and then we rearrange them to create the others in the phase shift code. (Mentioned in \cref{sec:Kohn})}


\section{Short-Range Terms}
\label{sec:CompShort}

Matrix elements in \cref{eq:GeneralKohnMatrix} involving only short-range terms are handled differently than those that involve long-range terms (short-long and long-long). \cref{eq:BoundHFull} and 

\todoi{More lead in}
The bound state problem is a generalized eigenvalue problem (see \cref{eq:BoundGenEig}), but to simplify the following discussion, I will refer to the set of matrices \textbf{H} and \textbf{S} as a single matrix. The diagrams in this section show a single matrix, but it is in actuality a pair of matrices forming the generalized eigenvalue problem.


\subsection{Short-Range -- Short-Range Integrations}
\label{sec:ShortInt}
The short-range -- short-range integrals make up the bulk of the $\textbf{\emph{A}}$ matrix
(\cref{eq:GenKohnMatrixAXB}). The PsH bound state problem in 
\cref{chp:PsHBound} consists of only these types of matrix elements
(see \cref{eq:BoundWavefn}). The form of these short-range integrals is
\beq
\label{eq:FourBody}
I \equiv I(k_i, l_i, m_i, n_i, p_i, q_i; \alpha, \beta, \gamma) = \int e^{-(\alpha r_1 + \beta r_2 + \gamma r_3)} r_1^{k_i} r_2^{l_i} r_{12}^{m_i} r_3^{n_i} r_{13}^{p_i} r_{23}^{q_i} d\textbf{r}_1 d\textbf{r}_2 d\textbf{r}_3,
\eeq
with real-valued $\alpha, \beta, \gamma > 0$.

This class of integrals, the Hylleraas three-electron or four-body 
integrals, has been studied extensively. See
Refs.~\cite{Drake1995,Frolov2003,Pelzl1998,Pachucki2004} for just some of 
the papers detailing strategies on how to compute these integrals. The first 
three papers here use the same infinite summation to numerically solve this 
integral but use different techniques to accelerate the convergence, as the 
summation converges slowly for some arguments. The last paper by Pachucki et 
al.\ uses a very different approach with recursion relations, described in 
\cref{sec:RecursionRelations}.

Each of these methods has some restriction on how singular these integrals 
can be. For Drake and Yan's asymptotic expansion \cite{Drake1995,Yan1997},
$k_i, l_i, n_i \geq -2$ and $m_i, p_i, q_i \geq -1$. Yan extends these to
$k_i, l_i, m_i, n_i, p_i, q_i \geq -3$ in an additional paper \cite{Yan2000a}. For 
the recursion relations of Pachucki et al.\ \cite{Pachucki2004},
$k_i, l_i, m_i, n_i, p_i, q_i \geq -1$. Pachucki and Puchalski have two other papers,
one extending this to $k_i, l_i, m_i, n_i, p_i, q_i \geq -2$ \cite{Pachucki2005} 
and another even extending these to $k_i, l_i, m_i, n_i, p_i, q_i \geq -3$ 
\cite{Pachucki2008}. Our work for the D-wave has some terms with $k_i$, $l_i$,
or $n_i = -2$, restricting what methods we can use. The biggest need for 
integrals more singular than $\frac{1}{r_i}$ or $\frac{1}{r_{ij}}$ in most 
work (such as lithium energies \cite{Yan1997a,Puchalski2010}) is for 
relativistic or quantum electrodynamics effects
\cite{Yan1997,Pachucki2008,Puchalski2010}.

{\"O}hrn and Nordling \cite{Ohrn1963} were the first to give a method to 
solve this type of integral by splitting it into summations over $W$ 
functions. To deal with odd powers of $r_{ij}$, these terms are usually 
expanded in a Laplace expansion using Perkins's expression \cite{Perkins1968}.
A related expression is given by Sack \cite{Sack1964}. Porras and King
\cite{Porras1994} also use another expansion with Gegenbauer polynomials.

From Drake and Yan's paper \cite{Drake1995}, splitting into $W$ functions gives
\begin{align}
\label{eq:FourBodyExpansion}
I &= (4\pi)^3 \sum_{q=0}^\infty \sum_{k_{12} = 0}^{L_{12}} \sum_{k_{12} = 0}^{L_{23}} \sum_{k_{12} = 0}^{L_{13}} \frac{1}{(2q+1)^2} C_{j_{12} q k_{12}} C_{j_{23} q k_{23}} C_{j_{13} q k_{13}} \nonumber \\
& \times [W(\tilde{k}_i + 2q + 2 k_{12} + 2 k_{13}, \tilde{l}_i + m_i - 2 k_{12} + 2 k_{23}, \tilde{n}_i + q_i - 2 q - 2 k_{23} + p_i - 2 k_{13}; \alpha, \beta, \gamma) \nonumber \\
      & + W(\tilde{k}_i + 2q + 2 k_{12} + 2 k_{13}, \tilde{n}_i + p_i - 2 k_{13} + 2 k_{23}, \tilde{l}_i + m_i - 2 q - 2 k_{23} + q_i - 2 k_{13}; \alpha, \gamma, \beta) \nonumber \\
      & + W(\tilde{l}_i + 2q + 2 k_{12} + 2 k_{23}, \tilde{k}_i + m_i - 2 k_{12} + 2 k_{13}, \tilde{n}_i + q_i - 2 q - 2 k_{23} + p_i - 2 k_{13}; \beta, \alpha, \gamma) \nonumber \\
      & + W(\tilde{l}_i + 2q + 2 k_{12} + 2 k_{23}, \tilde{n}_i + q_i - 2 k_{23} + 2 k_{13}, \tilde{k}_i + m_i - 2 q - 2 k_{23} + p_i - 2 k_{13}; \beta, \gamma, \alpha) \nonumber \\
      & + W(\tilde{n}_i + 2q + 2 k_{23} + 2 k_{13}, \tilde{k}_i + p_i - 2 k_{13} + 2 k_{12}, \tilde{l}_i + m_i - 2 q - 2 k_{23} + q_i - 2 k_{13}; \gamma, \alpha, \beta) \nonumber \\
      & + W(\tilde{n}_i + 2q + 2 k_{23} + 2 k_{13}, \tilde{l}_i + q_i - 2 k_{23} + 2 k_{12}, \tilde{k}_i + m_i - 2 q - 2 k_{23} + p_i - 2 k_{13}; \gamma, \beta, \alpha)]
\end{align}

The $k_{ij}$ summations are all finite. If all powers of $r_{ij}$ are odd, the $q$ summation is infinite, and if any of $r_{ij}$ is even, the $q$ summation becomes finite. For the finite $q$ sums, these direct sums are solved very accurately, but for the infinite sums, some integrals converge slowly. Particularly when all three $r_{ij}$ powers are odd and at least one is $-1$, the integrals converge the slowest. It is possible to restrict the basis set described by \cref{eq:OmegaDef} so that at least one of the $r_{ij}$ powers is even, making solving the integrals easier. However, this leads to slow convergence in the energy \cite{Drake1995}.

As Frolov and Bailey \cite{Frolov2003} note, these four-body integrals only work for systems with one infinitely heavy particle, such as PsH or Ps-H scattering. For systems with arbitrary masses, such as Ps$_2$ or Ps-Ps scattering, this has to be generalized to
\beq
\label{eq:FourBodyIntGen}
I = \int e^{-(\alpha r_1 + \beta r_2 + \gamma r_3 + a_{12} r_{12} + a_{13} r_{13} + a_{23} r_{23})} r_1^{k_i} r_2^{l_i} r_{12}^{m_i} r_3^{n_i} r_{13}^{p_i} r_{23}^{q_i} d\textbf{r}_1 d\textbf{r}_2 d\textbf{r}_3.
\eeq
Fromm and Hill give an analytic solution to this \cite{Fromm1987}, but it is extremely difficult to work with. Harris more recently solved this problem analytically using recursion relations \cite{Harris2009}, using a similar method to the recursion relations of Pachucki et al.\ \cite{Pachucki2004}. Both of these solutions restrict the powers of $r_i$ and $r_{ij}$ to $k_i, l_i, m_i, n_i, p_i, q_i \geq -1$. We have also looked at a subset of these integrals (refer to \cref{chp:Exponential}). As another extension of the Hylleraas basis set, some four-electron integrals can be reduced down to the three-electron integrals \cite{King1993,Pelzl2002}.


\subsubsection{Asymptotic Expansion}
\label{sec:AsymptoticExpansion}
For cases of \cref{eq:FourBodyExpansion} with all odd powers of $r_{ij}$, the $q$ summation is infinite, and the summation converges slowly. The convergence accelerator approach of Pelzl and King \cite{Pelzl1998,Pelzl2002} is one way to deal with this. I did limited testing with this approach but chose to instead use the asymptotic expansion method of Drake and Yan \cite{Drake1995}. In my testing, it was more numerically stable, and it has been generalized to arbitrary angular momenta \cite{Yan1997}.

As an example in Drake and Yan's paper \cite{Drake1995}, direct calculation of the $q$ summation for $I(0,0,0,-1,-1,-1; 1,1,1)$ only reaches an accuracy of $1.5 x 10^{-13}$ after 6860 terms. With their asymptotic expansion method, the integral has converged to approximately $2.2 x 10^{-16}$ after only 21 terms. The summation converges monotonically and asymptotically. Drake and Yan use this knowledge to speed up the convergence of the integration. Details of this method can be found in their paper \cite{Drake1995}.

I use this asymptotic expansion method in all calculations of the short--short integrals through the H-wave, and it performed very well. In fact, my quadruple precision code often calculates matrix elements to better than $1$ part in $10^{20}$ for the S-wave. \Cref{tab:AsympExpan} gives an example of the convergence of a single integral by only calculating the direct sum of \cref{eq:FourBodyExpansion} in the $S_d(N)$ column and with the asymptotic expansion in the $S_a(N)$ column. These four-body integrals are relatively quick to calculate, and most of the runs to compute them complete in a matter of hours.

\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$N$ & $S_d(N)$ & $\Delta S_d(N)$ & $S_a(N)$ \\
\midrule
17 & 684.106 432 091 &               & 684.113 411 842 629 912 374 349 \\
18 & 684.107 475 306 & 0.001 043 214 & 684.113 411 842 629 911 836 645 \\
19 & 684.108 320 637 & 0.000 845 331 & 684.113 411 842 629 911 829 661 \\
20 & 684.109 012 851 & 0.000 692 213 & 684.113 411 842 629 911 835 071 \\
21 & 684.109 585 093 & 0.000 572 241 & 684.113 411 842 629 911 836 095 \\
22 & 684.110 062 261 & 0.000 477 167 & 684.113 411 842 629 911 836 195 \\
23 & 684.110 463 302 & 0.000 401 041 & 684.113 411 842 629 911 836 186 \\
24 & 684.110 802 809 & 0.000 339 506 & 684.113 411 842 629 911 836 178 \\
25 & 684.111 092 142 & 0.000 289 333 & 684.113 411 842 629 911 836 174 \\
26 & 684.111 340 236 & 0.000 248 094 & 684.113 411 842 629 911 836 173 \\
27 & 684.111 554 183 & 0.000 213 946 & 684.113 411 842 629 911 836 172 \\
28 & 684.111 739 660 & 0.000 185 477 & 684.113 411 842 629 911 836 172 \\
29 & 684.111 901 249 & 0.000 161 588 & 684.113 411 842 629 911 836 172 \\
30 & 684.112 042 674 & 0.000 141 425 & 684.113 411 842 629 911 836 172 \\
\bottomrule
\end{tabular}
\caption[Convergence of direct sum, $S_d(N)$, against the asymptotic expansion, $S_a(N)$.]
{Convergence of direct sum, $S_d(N)$, against the 
asymptotic expansion, $S_a(N)$. This is an extension of Table I in Drake and 
Yan's work \cite{Drake1995} and uses $\Lambda = 15$. $\Delta S_d(N)$ gives 
the difference between successive direct sums.}
\label{tab:AsympExpan}
\end{table}

\subsubsection{Recursion Relations}
\label{sec:RecursionRelations}
After the S-wave calculations were completed, we learned of an analytic, 
instead of numerical, solution to these three-electron integrals, derived by 
Pachucki et al.\ \cite{Pachucki2004}. They
were not the first to derive an analytic solution to the three-electron 
integrals, but the first by Fromm and Hill \cite{Fromm1987} is not very 
practical to use, considering its very complicated form. Each set of $r_i$ 
and $r_{ij}$ powers requires a new rederivation from the Fromm and Hill 
result. The recursion relations from Pachucki et al.\ are complicated but also general.

Like many other types of recursion relations, these recursion relations may 
not be stable for higher $\omega$ values, depending on the calculated 
precision. I have tested through $\omega = 8$, and this method produced 
stable results under quadruple precision. In some of their work on Li and
Be$^+$, their group uses sextuple precision, as quadruple precision becomes 
insufficient near $\omega = 10$ \cite{Puchalski2006}, which is much higher 
than we can use in the Ps-H scattering calculations.

These are used as a check on the accuracy of the asymptotic expansion method in
\cref{sec:AsymptoticExpansion} for the S-wave and P-wave. The D-wave short-range
integrals cannot be evaluated using the recursion relations in
Ref.~\cite{Pachucki2004} due to the $r_i^{-2}$ terms that appear. Solving these
using the recursion relations would require implementing the extended method in
Ref.~\cite{Pachucki2005}. However, the asymptotic expansion method has proven
to be stable and accurate so far.


\subsubsection{W Functions}
\label{sec:WFunctions}

While evaluating the integrals in the PsH short-range code, we noticed that the
$W$ functions in \cref{eq:FourBodyExpansion} could be called more than once by
multiple integrals. For the S-wave, there are 34 terms in \cref{eq:GradGradShort},
and each matrix element in \textbf{H} in \cref{eq:BoundGenEig} requires these 34
integrals to be evaluated. The overall powers of $r_1$, $r_2$, $r_{12}$, etc.\ in
\cref{eq:FourBody} can be the same for a set of $\phi_i$ with $\phi_j$. Also for a
set of integrals without the overall powers the same, there is the possibility of
two different integrals calling the same $W$ function.

To speed up the program significantly, we precompute the $W$ functions and store
the results in a look up table. The 4-dimensional matrix is constructed with the
limits derived in \cref{chp:WLimits}. Even though there is a possibility for the
$W$ function arguments to be anywhere in this space, not all of the $W$ matrix
elements will be used.

The current S-wave, P-wave and D-wave short-range codes first do a dummy run
where the integrals are not actually calculated, but any $W$ matrix elements
that need to be computed are marked. Only these are computed, and then the code
runs through again using this $W$ matrix look up table. \Cref{tab:WFuncUnusedS}
shows that only $10.6\%$ of the $W$ matrix elements are actually needed for the
S-wave at a relatively high value of $\omega$. Similarly for the D-wave,
\cref{tab:WFuncUnusedD} shows that only $10.5\%$ are needed.

\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$\omega$ & Used Terms & Total Terms & Percentage Used \\
\midrule
1 & 	115   &  20,000 & 	0.575\% \\
2 & 	921   &  27,040 & 	3.41\% \\
3 & 	1,939 &  34,992 & 	5.54\% \\
4 & 	3,140 &  43,904 & 	7.15\% \\
5 & 	4,573 &  53,824 &	8.49\% \\
6 & 	6,246 &  64,800 &	9.64\% \\
7 & 	8,147 &  76,880 &	10.6\% \\
\bottomrule
\end{tabular}
\caption{S-wave W function terms used}
\label{tab:WFuncUnusedS}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$\omega$ & Used Terms & Total Terms & Percentage Used \\
\midrule
0 & 150		&	196,290 &	0.0764\% \\
1 & 1,670	&	259,932 &	0.642\% \\
2 & 14,076	&	332,262 &	4.24\% \\
3 & 23,384	&	413,520 &	5.65\% \\
4 & 36,418	&	503,946 &	7.23\% \\
5 & 51,420  &	603,780 &	8.52\% \\
6 & 68,438	&	713,262 &	9.60\% \\
7 & 87,520	&	832,632 &  10.5\% \\
\bottomrule
\end{tabular}
\caption{D-wave W function terms used}
\label{tab:WFuncUnusedD}
\end{table}

Another more sophisticated method that we have started using is given in
\cref{sec:PrimeFactor}. This looks instead at the overall integrations,
not the $W$ function arguments.



\subsection{Linear Dependence in the Bound State Calculation}
With infinite precision in calculations, all terms from the basis set could be used. However, due to the limited precision inherent in computer calculations, when using large basis sets, near linear dependences will exist in the matrices. The goal is to identify and eliminate terms that exhibit near linear dependence with other terms. These terms are not exactly linearly dependent, if infinite precision was possible, but they are linearly dependent to computer precision.

To calculate the eigenvalues of the generalized eigenproblem, the LAPACK routine \texttt{dsygv} is used \cite{dsygv}. For the basis set consisting of terms from $\omega$ = 5, LAPACK computes the eigenvalues without errors. Adding in terms corresponding to $\omega = 6$ for some sets of nonlinear parameters causes \texttt{dsygv} to fail with an error set in the last parameter, Info. This error is always an integer greater than the number of terms, indicating from the library documentation that ``the leading minor of order i of B is not positive definite'' \cite{dsygv}. L\"uchow and Kleindienst also encountered a similar problem using NAG and EISPACK \cite{Luchow1993}.

This error does suggest that one approach to identifying problematic terms is to check for positive definiteness of the overlap matrix $\left\langle \phi_i | \phi_j \right\rangle$. This is one method that Yan et. al use to isolate problematic terms \cite{Yan1999}. They test the eigenvalues of the overlap matrix to see if any are small or negative, though there is no mention in their paper of what value of ``small'' is used. In several papers by Yan and others \cite{Yan1998,Yan1998a,Yan1999,Drake1995,Yan1997a}, terms with $j1 > j2$ are omitted if $l_1 = l_2$ and $\alpha \approx \beta$, along with $j_1 = j_2$ if $j_{23} > j_{31}$. No reasoning is given in these papers as to why these terms should cause near linear dependence or why they were chosen to be omitted. Right now, it is assumed that these terms were determined by trial and error or by noticing patterns in terms that produced near linear dependence.

As mentioned in the previous paragraph, the overlap matrix will no longer be positive-definite if a term is included in the basis set which introduces a near linear dependence. We attempted to use this fact to remove problematic terms, but too many terms were removed, leading to an energy that converged too slowly. This may need to be looked at again.

Another technique Yan et al.\ used was to partition the basis set into five sectors, each with a different set of nonlinear parameters and maximum $\omega$ \cite{Yan1999}. The sectors also have restrictions on the interparticle $r_{ij}$ terms, mainly limiting the power of $r_{23}$ and $r_{31}$, which are the electron-positron coordinates in their paper (corresponding to $r_{12}$ and $r_{13}$ in our work). The techniques used by Yan, Drake and Ho for restricting the set of terms are not used in our work.


\subsection{Allan Todd's Algorithm}
\label{sec:ToddBound}
In trying to determine the energy eigenvalues, we noticed that the ordering 
of the terms could determine whether there was linear dependence in the 
matrices. Allan Todd's algorithm was attractive, because it reorders the 
matrices to obtain the best possible energy, and it is a purely computational 
approach \cite{Todd2007}. So far, we have not seen any physical reason why 
certain terms should introduce a near linear dependence. A description of his 
algorithm as implemented by us follows.

The total number of terms to look at is $N = N(\omega)$ (see \cref{eq:NumberTermsOmega}).
 N matrices of size 1x1 are created for each term. This 
is done for the overlap and the
 $\left\langle \phi \left| \,H \right| \phi \right\rangle$
 matrices together. The LAPACK \texttt{dsygv} routine is used to 
determine the lowest eigenvalue for each of these N sets. These energy 
eigenvalues are compared against one another, and the term with the lowest 
energy is chosen. In the next step, the first basis function from the 
previous step is combined with each unused term to create N-1 matrices of 
size 2x2. Again, the energy eigenvalues for each of the N-1 matrices are 
compared against each other, and the term yielding the lowest energy is 
chosen as the second basis function. This is done again with 3x3 matrices for 
each of the N-2 remaining terms combined with the basis functions chosen in 
the first two steps. This procedure is repeated until all terms have been 
used or the remaining terms are problematic.

In his original algorithm, Allan Todd looked at the eigenvalues computed from 
the upper and lower triangular matrices. Normally, the overlap and H matrices 
are symmetric, but this is not true to machine precision due to truncation 
and rounding. If the energy eigenvalues from the upper and lower triangles 
differ by more than $10^{-7}$ (in atomic units), the last added term is 
considered problematic and discarded.

In our testing for $\omega = 6$, no terms were omitted due to the reordering. 
As noted earlier, before implementing this algorithm, LAPACK would fail when 
trying to calculate the eigenvalues, so the ordering is important for getting 
the best possible energy. For $\omega$ = 7, 116 terms were omitted, out of a 
total of 1716 terms. The criteria that the eigenvalues for the upper and 
lower matrices differs by no more than a certain amount was not needed in 
this case. The Info parameter of the LAPACK dsygv function is checked for 
both the upper and lower matrix eigenvalue calculations, and the last added 
term is discarded if it causes an error due to linear dependence. When only 
the 116 problematic terms were left, every one of them caused LAPACK to 
error. If a term was problematic at any stage, it continued to be problematic 
in all further stages, so computation time can be decreased by immediately 
discarding it.

For larger basis sets, this algorithm becomes extremely slow, as determining 
the eigenvalues is an $O(N^3)$ operation. It can easily be parallelized, 
since we are computing the eigenvalues for a large number of matrices. Our 
program has been parallelized using OpenMP for intranode communications and 
Open MPI for internode communications. Todd's algorithm provides the best 
converged energy for a set of terms, albeit at a cost of computational speed.

\begin{figure}[H]
	\centering
	{\includegraphics[height=1.5in]{Todd}}
	\caption{Diagram of Todd's procedure}
	\label{fig:Todd}
\end{figure}


\subsection{L\"uchow and Kleindienst Algorithm} \label{sec:LuchowBound}
The algorithm that L\"uchow and Kleindienst propose \cite{Luchow1992} is 
similar to Todd's algorithm. They use all terms through $\omega = 5$, since 
none of these have linear dependence within this set. This fact could be 
applied to Allan Todd's algorithm to speed up the calculation. The rest of 
the terms are partitioned up into blocks of 300-400 terms, except the last 
block, which will have modulus(N - 462, block size) terms. A key difference 
from Allan Todd's algorithm is that the terms should be ordered in terms of 
increasing $\omega$. In Allan Todd's algorithm, the initial ordering does not 
matter. The terms in the first block are added to the basis set for
$\omega = 5$, which has 462 terms. If we use a block with 300 terms, then 300 matrices 
of size $463 \times 463$ are created, and their eigenvalues are determined. Whichever 
term gives the lowest energy when added to this set is saved to the basis 
set. This continues with ever-increasing matrix sizes, until all terms in the 
block are used, or a linear dependence is noticed. L\"uchow and Kleindienst 
propose that linear dependences can be prevented by omitting terms that do 
not contribute significantly to the energy. The energy from the previous step 
is saved, and the energy from the current step is compared to this. If the 
difference in energies is small, e.g. $\Delta E < 10^{-10} E_h$ in their 
paper, then that term is omitted. Since the lowest energy is chosen at each 
step, when this $\Delta E$ is too small, the rest of the block can be omitted.

Two parameters that are adjustable in this algorithm are the block size and 
cutoff value for the energy. To determine the best values for each, we
set $\omega = 7$, and one of the parameters was held constant. To 
determine the optimal block size, an energy cutoff of $5\e{-9}$ was used.
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Block size & Used terms & Energy & Time (min)\\
\hline
200 & 1041 & -0.789 181 890 & 470 \\
250 & 1024 & -0.789 184 500 & 572 \\
300 & 960 &  -0.789 180 520 & 749 \\
350 & 999 &  -0.789 181 942 & unknown \\
400 & 1012 & -0.789 185 588 & 852 \\
500 & 985 &  -0.789 184 116 & 764 \\
\hline
\end{tabular}
\end{center}

For most trials, increasing the block size also increased the time it took to 
complete the computation. It can be seen that a block size of N-462 gives a 
modified version of Todd's algorithm, since we are testing all terms at each 
step. A block size of 1 would correspond to no reordering of terms, leading 
to including problematic terms. The cutoff value for $\Delta E$,
$E_{\text{cutoff}}$, is $10^{-10} E_h$ in L\"uchow and Kleindienst's paper
\cite{Luchow1992}, but a value of $10^{-10}$ in our calculations led to LAPACK 
errors due to linear dependences. The block size was set to 400, and it was
determined that an energy cutoff of $10^{-9}$ was the largest value that caused linear 
dependences.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Energy cutoff & Used terms & Energy\\
\hline
2\e{-9} & 1116 & -0.789 185 858 \\
3\e{-9} & 1053 & -0.789 186 346 \\
4\e{-9} & 1043 & -0.789 185 173 \\
5\e{-9} & 1012 & -0.789 185 588 \\
1\e{-8} & 963 &  -0.789 188 024 \\
\hline
\end{tabular}
\end{center}

Unfortunately, the intuition that a larger block size and smaller energy cutoff will yield a better final energy is not necessarily correct. The best value of the energy was actually obtained for the largest tested energy cutoff of $10^{-8}$. A block size of 500 also gave a worse energy than did a block size of 400. More tests need to be performed to determine the optimal value of both parameters, and these could possibly change for higher values of $\omega$ or for a different system.

We decided to use the Todd algorithm (\ref{sec:ToddBound}) due to the better energy obtained and the larger set of terms returned. The Todd algorithm is also more consistent between runs. The trade-off is the increased time to perform a calculation, especially for large $\omega$.

\begin{figure}[H]
	\centering
	{\includegraphics[height=2in]{Luchow}}
	\caption{Diagram of L\"uchow and Kleindienst procedure}
	\label{fig:Luchow}
\end{figure}



\section{Long-Range Terms}
\label{sec:CompLong}

collectively referred to as long-range integrations


\subsection{Perimetric Coordinates}
\label{sec:PerimetricCoords}

Perimetric coordinates are used for the long-long integrations for the S-wave. If perimetric coordinates are used for $r_1$, $r_2$ and $r_{12}$, then these are defined by \cite{Armour1991}

\begin{align}
\label{eq:PerimetricCoords1}
\nonumber x &= r_1 + r_2 - r_{12} \\
\nonumber y &= r_2 + r_{12} - r_1 \\
z &= r_{12} + r_1 - r_2.
\end{align}

These can alternately be written as
\begin{align}
\label{eq:PerimetricCoords2}
\nonumber r_1 &= \frac{x+z}{2} \\
\nonumber r_2 &= \frac{x+y}{2} \\
\nonumber r_{12} &= \frac{y+z}{2}.
\end{align}

From equation \cref{eq:dtau1}, the volume element after integration over the external angles is
\beq
d\tau = 8\pi^2 dr_1 r_2 dr_2 r_3 dr_3 r_{12} dr_{12} r_{13} dr_{13} d\phi_{23}.
\eeq
We need to perform a change of variables to use perimetric coordinates for $r_1$, $r_2$ and $r_{12}$. The Jacobian is
\beq
\label{eq:PerimetricJacobian}
J(x,y,z) = 
\left| {\begin{array}{ccc}
 \frac{\partial r_1}{\partial x} & \frac{\partial r_1}{\partial y} & \frac{\partial r_1}{\partial z}  \\
 \frac{\partial r_2}{\partial x} & \frac{\partial r_2}{\partial y} & \frac{\partial r_2}{\partial z}  \\
 \frac{\partial r_{12}}{\partial x} & \frac{\partial r_{12}}{\partial y} & \frac{\partial r_{12}}{\partial z}  \\
 \end{array} } \right|
=
\left| {\begin{array}{ccc}
 \frac{1}{2} & 0 & \frac{1}{2} \\
 \frac{1}{2} & \frac{1}{2} & 0 \\
 0 & \frac{1}{2} & \frac{1}{2}
 \end{array} } \right|
=
\frac{1}{4}.
\eeq

\noindent This gives a transformed volume element of
\beq
\label{eq:PerimetricVolEl}
d\tau = 2\pi^2 r_2 r_3 r_{12} r_{13} dx\, dy\, dz\, dr_3\, dr_{13}\, d\phi_{23}.
\eeq

\noindent The limits for each of the perimetric coordinates are 0 to $\infty$.


\subsection{Gaussian Quadratures}
\label{sec:GaussQuad}
\todoi{Mention looking into \cite{Bailey2004,Ma1996,Mori2001,Odrzywolek2011,Fukuda2005}}
Gaussian quadratures are used to integrate many classes of integrals. In their most general form, these quadratures are given by \cite[p.887]{Abramowitz1965}
\beq
\label{eq:GeneralQuadratures}
\int_a^b W(x) f(x) dx \approx \sum_{i=1}^n w_i f(x_i).
\eeq
Gaussian quadratures are particularly attractive, since they give exact results for polynomials up to degree $2n-1$. The weight function $W(x)$ can be chosen for certain classes of integrals. Three main types of weight functions are used in this work.


\subsection{Gauss-Legendre Quadrature}
\label{sec:GaussLegendre}
If the weight function is chosen as $W(x)=1$, and the integration interval is $(-1,1)$, this is known as Gauss-Legendre quadrature (or sometimes known simply as Gaussian quadrature). The orthogonal polynomials used are the Legendre polynomials, $P_n(x)$. Equation \ref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussLeg}
\int_{-1}^1 f(x) dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where the $x_i$ abscissas are the $i^{th}$ zeros of $L_n(x)$, and the weights are given by
\beq
\label{eq:GaussLegWeights}
w_i = \frac{2}{(1-x_i^2)[P^\prime_n(x_i)]^2}.
\eeq
The limits of integration must be from $-1$ to $1$, but this is generalized by using the transformation \cite{Abramowitz1965}
\begin{align}
\label{eq:GaussLegGen}
\int_a^b f(x) dx &= \frac{b-a}{2} \int_{-1}^1 f \left(\frac{b-a}{2} x + \frac{a+b}{2}\right) dx \\
&\approx \frac{b-a}{2} \sum_{i=1}^n w_i f \left(\frac{b-a}{2} x_i + \frac{a+b}{2}\right).
\end{align}


\subsection{Gauss-Laguerre Quadrature}
\label{sec:GaussLag}
The Gauss-Legendre quadrature cannot be used on semi-infinite intervals, so we use the Gauss-Laguerre quadrature in these cases. The orthogonal polynomials in this case are the Laguerre polynomials, $L_n(x)$, and the weight function is $W(x) = e^{-x}$. Equation \ref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussLag}
\int_0^\infty e^{-x} f(x) dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where the $x_i$ abscissas are the $i^{th}$ zeros of $L_n(x)$, and the weights are given by
\beq
\label{eq:GaussLagWeights}
w_i = \frac{x_i}{(n+1)^2 [L_{n+1}(x_i)]^2}.
\eeq

When the integration is over the interval $(a,\infty)$, equation \ref{eq:GaussLag} is easily transformed by
\beq
\label{eq:GaussLagGen1}
\int_a^\infty e^{-x} f(x) dx = \int_0^\infty e^{-(x+a)} f(x+a) dx = e^{-a} \int_0^\infty e^{-x} f(x+a) dx \approx e^{-a} \sum_{i=1}^n w_i f(x_i+a).
\eeq

\noindent A more general form of this is obtained by using a coefficient in the exponential, i.e.
\beq
\label{eq:GaussLagGen2}
\int_a^\infty e^{-m x} f(x) dx = \frac{1}{m} \int_a^\infty e^{-y} f\left(\frac{y}{m}\right) dy,
\eeq
where we have defined $y = m x$.  This allows for equation \ref{eq:GaussLagGen1} to be generalized to
\begin{align}
\label{eq:GaussLagGen}
\nonumber \int_a^\infty e^{-m x} f(x) dx &= \frac{1}{m} \int_{ma}^\infty e^{-y} f\left(\frac{y}{m}\right) dy = \frac{1}{m} \int_0^\infty e^{-(y+ma)} f\left(\frac{y}{m}+a\right) dy \\
& = \frac{e^{-ma}}{m} \int_0^\infty e^{-y} f\left(\frac{y}{m}+a\right) dy \approx \frac{e^{-ma}}{m} \sum_{i=1}^n w_i f\left(\frac{y_i}{m}+a\right).
\end{align}
The $y_i$ abscissas and $w_i$ weights are the same as the less general case in equations \ref{eq:GaussLag} and \ref{eq:GaussLagWeights}.  This general form of Gauss-Laguerre quadrature is what we use for our semi-infinite integrations.


\subsection{Chebyshev--Gauss Quadrature}
\label{sec:ChebyshevGauss1}
If the weight function is chosen as $W(x)=\frac{1}{\sqrt{1-x^2}}$, and the integration interval is $(-1,1)$, this is known as Chebyshev-Gauss quadrature. The orthogonal polynomials used are the Chebyshev polynomials of the first kind, $T_n(x)$.  Equation \ref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussCheb}
\int_{-1}^1 \frac{f(x)}{\sqrt{1-x^2}} dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where
\beq
\label{eq:GaussChebAbsWeights}
x_i = \cos\left(\frac{2i-1}{n}\pi\right) \text{ and } w_i = \frac{\pi}{n}.
\eeq
This quadrature is used for the internal angular integrations.  A discussion of how to use this for these integrations is found in appendix \ref{sec:ChebyshevGauss}.

\subsection{Long-Range -- Long-Range}
\label{sec:LongLongInt}

The scattering program calculates only the short-long and long-long matrix elements. The volume element in equation \ref{eq:dtau} has an internal angle of $\phi_{23}$ to integrate over.  When a term has a a negative power of $r_{23}$, a large number of integration points must be used for reasonable accuracy. Instead, we split the integration such that one part is missing the the $r_{23}^{-1}$ term and the other contains only the $r_{23}^{-1}$ term.

The first integration excluding the $r_{23}^{-1}$ term has negative powers of $r_i$ and $r_{ij}$ cancelled by the corresponding terms in the volume element given by equation \ref{}.  For the integration over the $r_{23}^{-1}$ term, we use an alternative volume element, namely that given by equation \ref{}.  The $r_{23}^{-1}$ is then cancelled by the $r_{23}$ in this volume element.

\todoi{Add the section about the volume elements in an appendix.}
\todoi{$\mathcal{L}$ instead of L}

\subsection{Integration without the \texorpdfstring{$r_{23}^{-1}$} {1/r23} term}
\label{sec:LongLongNoR23}
The simplest long-long matrix element to evaluate is $(\bar{S},\mathcal{L} \bar{S})$.  From \cref{eq:SbarLSbar}, not including its $r_{23}^{-1}$ term, this is
\beq
(\bar{S},\mathcal{L} \bar{S})_A = \pm \left(S^\prime,\mathcal{L} S\right) = \pm \left(S^\prime, \left[ \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}}\right] S\right).
\eeq

For this type of integration, we use perimetric coordinates as described in section \ref{sec:PerimetricCoords}.
\begin{align}
\label{eq:SBarSBarInt}
(\bar{S},\mathcal{L} \bar{S})_A = \pm &2\pi^2 \int_0^\infty \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_0^{2\pi}  S^\prime S \left[ \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}}\right] \\
&\times r_2 r_3 r_{12} r_{13}\, d\phi_{23}\, dr_{13}\, dr_3\, dz\, dy\, dx
\end{align}

The $\phi_{23}$ integration is done analytically. Since $S$ and $S^\prime$ have no $r_{23}$ dependence and there is no $r_{23}$ term in the brackets, the integration over $\phi_{23}$ is simply $2\pi$.  The $r_{13}$ integration uses the Gauss-Laguerre quadrature from section \ref{sec:GaussLegendre}. The $x$, $y$ and $z$ integrations use Gauss-Laguerre quadrature (section \ref{sec:GaussLag}), since they are semi-infinite.

The $r_3$ integration could also be performed using just the Gauss-Laguerre 
quadrature. However, the integrand for the $r_3$ integration has a 
discontinuity in its slope at $r_3=r_1$, creating a cusp
(see \cref{sec:Cusps}), so the accuracy is improved greatly if we split the
integration interval into two parts and employ different quadratures for each.
The integration is split to use Gauss-Legendre on the interval $(0,r_1)$ and
Gauss-Laguerre on the interval $(r_1,\infty)$.

\subsection{Integration over the \texorpdfstring{$r_{23}^{-1}$} {1/r23} term}
%\subsection[Integration over the 1/r23 term]{Integration over the $r_{23}^{-1}$ term}
\label{sec:LongLongR23}
The other part of the $(\bar{S},\mathcal{L} \bar{S})$ integral contains the $r_{23}^{-1}$ term.

\beq
(\bar{S},\mathcal{L} \bar{S})_B = \pm \left(S^\prime, \left[ \frac{2}{r_{23}}\right] S\right)
\eeq

\textbf{@TODO:} Why exactly do we use perimetric coordinates?

\noindent The volume element for this integral is $d\tau^\prime$ from equation \ref{}.  The integration also does not need to be converted to perimetric coordinates, so its form is
\beq
(\bar{S},\mathcal{L} \bar{S})_B = \pm 8\pi^2 \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_{|r_2 - r_3|}^{|r_2 + r_3|} \int_0^{2\pi}  S^\prime S \frac{2}{r_{23}} r_1 r_2 r_{13} r_{23}\, d\phi_{12}\, dr_{23}\, dr_{13}\, dr_2\, dr_3\, dr_1.
\eeq

The $r_{13}$ and $r_{23}$ integrals have finite limits, so here we use Gauss-Legendre quadrature.  Again, for the internal angular integration, this time over $\phi_{12}$, we use Chebyshev-Gauss quadrature.  The cusp in the $r_3$ integration is at $r_3 = r_1$, and the cusp in the $r_2$ integration is at $r_2 = r_3$.  Similar to before, we split up these integrations by using Gauss-Legendre before the cusp and Gauss-Laguerre after the cusp.

The $(\bar{C},\mathcal{L} \bar{S})$ and $(\bar{C},\mathcal{L} \bar{C})$ terms are integrated in the same manner as the $(\bar{S},\mathcal{L} \bar{S})$ integral just described.


\subsection{Short-Range -- Long-Range}
\label{sec:ShortLongInt}
We will consider only the $(\bar{\phi}_i,\mathcal{L} \bar{S}_\ell)$ integrations here, as the $(\bar{\phi}_i,\mathcal{L} \bar{C}_\ell)$ integrals are evaluated in the same manner. As in the case of long-range -- long-range integrations in section \ref{sec:LongLongInt}, we split up the integration into two parts -- one containing the $r_{23}^{-1}$ term and another containing the rest of the terms. The short-range terms have the added benefit of the possibility of the polynomial $r_{23}^{\,q_i}$ being present, which cancels the $r_{23}^{-1}$ term or gives it an overall positive power.


From \ref{eq:PhiBarLSBar2b}, \ref{eq:LS2} and \ref{eq:LSP2}, 

\begin{align}
\label{eq:PhiLSBarInt}
\nonumber (\bar{\phi}_i, L\bar{S}_\ell) &= \frac{2}{\sqrt{2}} \left(\phi_i,\mathcal{L} \bar{S}_\ell\right) \\
 &= \frac{2}{\sqrt{2}} \int \phi_i \left[ \left( \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}} + \frac{2}{r_{23}} \right)S_\ell \pm \left( \frac{2}{r_1} - \frac{2}{r_3} - \frac{2}{r_{12}} + \frac{2}{r_{23}} \right) S_\ell^\prime \right]  d\tau.
\end{align}

\subsubsection{Case I: \texorpdfstring{$q_i > 0$}{qi > 0}}
\label{sec:Swaveqigt0}
When $q_i > 0$ in $\phi_i$ (equation \ref{eq:PhiDef}), the power of $r_{23}$ is equal to or greater than 0.  Gaussian quadratures can safely integrate this type of term, so we integrate the full expression in equation \ref{eq:PhiLSBarInt}.
\begin{align}
\label{eq:PhiLSBarIntFull}
\nonumber (\bar{\phi}_i, L\bar{S}_\ell) =& \, \frac{2}{\sqrt{2}} \cdot 8\pi^2  \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_2|}^{|r_1 + r_2|} \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_0^{2\pi} \phi_i \\
&\times \left[ \left( \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}} + \frac{2}{r_{23}} \right)S_\ell \pm \left( \frac{2}{r_1} - \frac{2}{r_3} - \frac{2}{r_{12}} + \frac{2}{r_{23}} \right) S_\ell^\prime \right] \\
&\times r_2 r_3 r_{12} r_{13}\, d\phi_{23}\, dr_{13}\, dr_{12}\, dr_3\, dr_2\, dr_1
\end{align}

Similar to the long-long integrations from section \ref{sec:LongLongInt}, the $r_1$ integration is performed using the Gauss-Laguerre quadrature.  The $r_2$ integral is broken into two parts at the cusp of $r_2 = r_1$, with the Gauss-Legendre quadrature before the cusp and the Gauss-Laguerre quadrature after the cusp.  In the $r_3$ coordinate, there is a cusp at $r_3 = r_2$, so the integration is also split up into Gauss-Legendre before the cusp and Gauss-Laguerre after the cusp.  The finite intervals for $r_{12}$ and $r_{13}$ ensure that we can use Gauss-Legendre quadratures for these coordinates.  The $\phi_{23}$ integration uses the Chebyshev-Gauss quadrature.

\subsubsection{Case II: \texorpdfstring{$q_i = 0$}{qi = 0}}
When $q_i = 0$, the overall power of the $r_{23}^{-1}$ term is $-1$, so we cannot use the Gaussian quadratures in the form of \ref{eq:PhiLSBarIntFull}.  Similar to the long-long integrations, the $r_{23}^{-1}$ term is integrated separately, using the same type of integrations as equation \ref{eq:PhiLSBarIntFull}.  Refer to the previous section for the description of the quadratures used.
\begin{align}
\label{eq:PhiLSBarIntNoR23}
\nonumber (\bar{\phi}_i, L\bar{S}_\ell) =& \,\frac{2}{\sqrt{2}} \int \phi_i \left[ \left( \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}} \right)S_\ell \pm \left( \frac{2}{r_1} - \frac{2}{r_3} - \frac{2}{r_{12}} \right) S_\ell^\prime \right]  d\tau \\
=&\, \frac{2}{\sqrt{2}} \cdot 8\pi^2  \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_2|}^{|r_1 + r_2|} \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_0^{2\pi} \phi_i \\
&\times \left[ \left( \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}} \right)S_\ell \pm \left( \frac{2}{r_1} - \frac{2}{r_3} - \frac{2}{r_{12}} \right) S_\ell^\prime \right]  r_2 r_3 r_{12} r_{13}\, d\phi_{23}\, dr_{13}\, dr_{12}\, dr_3\, dr_2\, dr_1
\end{align}

The integration over the $r_{23}^{-1}$ term is done the same way as the second integration of the long-long matrix elements in section \ref{sec:LongLongInt}.  The $r_{23}$ in the $d\tau^\prime$ volume element cancels the $r_{23}^{-1}$ term.  Refer to section \ref{sec:LongLongR23} for a description of the quadratures used here.
\begin{align}
\label{eq:PhiLSBarIntR23}
(\bar{\phi}_i, L\bar{S}_\ell) =& \,\frac{2}{\sqrt{2}} \int \phi_i \left[ \frac{2}{r_{23}}\left(S_\ell \pm S_\ell^\prime\right) \right] d\tau^\prime  \nonumber \\
=&\, \frac{2}{\sqrt{2}} \cdot 8\pi^2  \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_{|r_2 - r_3|}^{|r_2 + r_3|} \int_0^{2\pi} \phi_i \left[ \frac{2}{r_{23}}\left(S_\ell \pm S_\ell^\prime\right) \right]  \nonumber \\
&\times  r_1 r_2 r_{13} r_{23}\, d\phi_{12}\, dr_{23}\, dr_{13}\, dr_2\, dr_3\, dr_1
\end{align}


\subsection{Quadrature Points}
\label{sec:QuadraturePoints}


\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord\\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$ & 45 & 35 & 35 & 35 & 28 & 15 & & \\
 Long-long, $r_{23}^{-1}$ & 65 & 35 & 28 & 35 & 28 & 12 & 15 & 15 \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$ & 90 & 57 & 34 & 57 & 34 & 30 & 30 & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$ & 90 & 58 & 30 & 55 & 35 & 33 & 33 & 33 \\
 Long-short $q_i > 0$ & 90 & 57 & 34 & 57 & 34 & 30 & 30 & 30 \\
\bottomrule
\end{tabular}
\caption{Base set of effective coordinates for integrations}
\label{tab:BaseEffectiveCoords}
\end{table}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord\\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$			&  75 & 40 & 40 & 40 & 40 & 25 & & \\
 Long-long, $r_{23}^{-1}$				&  75 & 40 & 40 & 40 & 40 & 25 & 25 & 25 \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$	& 100 & 65 & 45 & 65 & 45 & 45 & 45 & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$	& 115 & 65 & 45 & 65 & 45 & 45 & 45 & 45 \\
 Long-short $q_i > 0$					& 100 & 65 & 45 & 65 & 45 & 45 & 45 & 45 \\
\bottomrule
\end{tabular}
\caption{Optimal set of effective coordinates for integrations}
\label{tab:OptimalEffectiveCoords}
\end{table}

Describing the number of points used in integrating the different coordinates in sections \ref{sec:LongLongInt} and \ref{sec:ShortLongInt} can be confusing, so we have taken to grouping the sets of points as in tables \ref{tab:BaseEffectiveCoords} and \ref{tab:OptimalEffectiveCoords}. Each column of this table is referred to as an ``effective coordinate''.

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord \\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$ & $x$ Lag & $y$ Lag & $z$ Lag & $r_3$ Leg & $r_3$ Leg & $r_{13}$ Leg & & \\
 Long-long, $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $\phi_{12}$ Che & $r_{13}$ Leg & $r_{23}$ Leg \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $r_{13}$ Leg & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $\phi_{13}$ Che & $r_{23}$ Leg \\
 Long-short $q_i > 0$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $r_{13}$ Leg & $\phi_{23}$ Che \\
\bottomrule
\end{tabular}
\caption{Optimal set of effective coordinates for integrations}
\label{tab:EffectiveCoords}
\end{table}


\subsection{Selection of Quadrature Points}
\label{sec:SelQuadPoints1}
The number of quadrature points for each coordinate in the 6-dimensional integrations is critical to have fully converged results. In our testing, the $r_1$ coordinate (the coordinate of $e^+$) was the most important, requiring more integration points than any other. The $r_2$ and $r_3$ coordinates were the second most important, and the interparticle terms ($r_{12}$, $r_{13}$ and $r_{23}$) were the least important. These results only apply to the long-range--long-range and long-range--short-range terms, as the short-range--short-range terms are integrated using the same asymptotic expansion method as the bound state problem (see section \ref{sec:MatrixShort}).

To determine this, we held the number of integration points fixed, except for one coordinate, which we increased in steps. The difference in the output between steps was used to analyze how important each coordinate was. Also, some terms are more sensitive to the number of integration points than other. For instance, the $(\bar{S},\mathcal{L} \bar{S})$ term converges relatively quickly, while the $(\bar{C},\mathcal{L} \bar{C})$ term requires more integration points.

We also used a sample input file from Van Reeth for the number of quadrature points he used in his code. In figure \ref{fig:OriginalPhaseShifts-pvr}, this set of points was used. After approximately 525 short-range terms, it is immediately apparent that the phase shifts diverge, with a particularly large deviation in the inverse Kohn output. The Kohn and complex Kohn methods converge again shortly before 600 terms and stay converged up to 1000 terms.

Adding 5 points to all integrations yields a much more well-behaved graph in figure \ref{fig:OriginalPhaseShifts-pvrplus5}. There are slight variations, one of which can be seen in the inset graph at about 540 terms, but the results are overall converged up to 1100 terms.

Increasing the number of points can have a huge impact on performance. Adding 5 points to an $\omega = 0$ calculation takes 3.5 hours, while adding 10 points makes it take 6.1 hours, or approximately twice the time. So we have to make a trade-off between accuracy and performance.

To determine an optimal set of points, 10 points were added to Van Reeth's set in all coordinates, and a run was done with this set. Then 5 points were subtracted from one coordinate from this set for each run, once for each coordinate. The difference in the matrix elements and phase shifts was small for the 2nd, 3rd and 4th coordinates. A final run using 5 points extra from Van Reeth's set for the 2nd, 3rd and 4th coordinates, along with 10 points extra in all other coordinates, showed small differences from the first set that had 10 points extra in all coordinates. Reducing the points of any of the other coordinates resulted in larger differences from the set with 10 extra points in all coordinates. This final set, referred to as the ``optimal set'', is used in all of our current runs. Increasing the number of integration points further than could potentially lead to better accuracy, though numerical instabilities start appearing with 15 extra points over the base set.


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{OriginalPhaseShifts-pvr}
	\caption{Phase shifts with original ordering $(\omega = 7)$}
	\label{fig:OriginalPhaseShifts-pvr}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{OriginalPhaseShifts-pvrplus5}
	\caption{Phase shifts with original ordering and 5 extra points $(\omega = 7)$}
	\label{fig:OriginalPhaseShifts-pvrplus5}
\end{figure}



\subsection{Selection of Quadrature Points}
\label{sec:SelQuadPoints2}

\begin{figure}[H]
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/ColorKey.pdf}}
	\caption{Color key for differences}
	\label{fig:ColorKey}
\end{figure}

For the P-wave, we noticed that the phase shifts for smaller $\kappa$ values were more sensitive to the number of integration points than higher $\kappa$ values. In general, the smaller the value of $\kappa$, the smaller the phase shift will be. The smaller phase shifts for the P-wave also caused it to be more sensitive than the S-wave.

\subsection{Comparison Program}
I wrote the program \emph{Comparison} \cite{} to visually compare two runs with different numbers of integration points. This program calculates the relative difference between similar matrix elements of the two input files. The relative difference is given by
\beq
\text{diff}_{rel} = \left| \frac{elem_1 - elem_2}{(elem_1 + elem_2) / 2} \right|.
\eeq
Since the values of matrix elements can range over many orders of magnitude, the relative difference is an appropriate measure of the change caused by varying the number of integration points. The other option would have been to use the absolute difference, which is given by 
\beq
\text{diff}_{abs} = \left| elem_1 - elem_2 \right|.
\eeq
However, this is a poor measure of the differences, and the relative differences give much more useful information for our purposes.

The \emph{Comparison} program calculates these relative differences for each matrix element between the two input files and creates an image using the colors in figure \ref{fig:ColorKey}. So if a column of pixels in the output image is yellow, then the relative difference for that matrix element is on the order of $10^{-6}$. This is an example image from running this program:
\begin{figure}[H]
	\centering
	\resizebox{0.8\textwidth}{!}{\includegraphics{QuadPoints/Example.png}}
	\caption{Example matrix element comparison image}
	\label{fig:QuadExample}
\end{figure}
This only looks at the output from the long-range programs, and this is the first row (or column) of the $A$ matrix. The \emph{Comparison} program outputs a vertical line for each matrix element to make them more visible than outputting a single pixel. More examples and descriptions of the command line syntax are available on the \htmladdnormallink{Wiki}{http://cas-bs5cph1.phys.unt.edu/wiki/index.php/Long-range_graphical_comparison_program} \cite{Wiki}.

\subsection{Application to the P-Wave}

Using the comparison program, we found that the $1/r_{23}$ term in the potential needed more integration points when $q_i = 0$. The integrations for the other three potential terms already produced reasonably converged results. All runs in this section are performed with $\kappa = 0.1$.

If we took the set of integration points that we used in the S-wave problem as our base set, then compared this with the same set but an extra 5 points in the $1^{st}$ coordinate, the resulting difference image for the $A$ matrix looks like this:
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/BasevsBaseplus5.png}}
	\caption{Base set versus base set plus 5 in $r_1$}
	\label{fig:BasevsBaseplus5}
\end{figure}
\noindent Notice that there is little blue and plenty of yellows, oranges and reds. The goal is to get as much black and blue as possible, though it is impossible to get entirely black and blue with our limited precision. It is evident that the matrix elements are not well-converged when we compare the phase shifts from these two runs. For the first run with the base set, the phase shift is 0.02346176. The corresponding phase shift for the second run is 0.02294583. We desire three significant figures in our phase shifts, so this is certainly not good enough.

Doing a similar run with the base set and another with the base set plus 10 in $r_1$, the difference image is given by figure \ref{fig:BasevsBaseplus10}.
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/BasevsBaseplus10.png}}
	\caption{Base set versus base set plus 10 in $r_1$}
	\label{fig:BasevsBaseplus10}
\end{figure}
\noindent This image is not much different from the previous image. Even when we compare the ``plus 5'' run with the ``plus 10'' run \textbf{@TODO: Where is this image?}, the image does not change much. The phase shift for the ``plus 10'' run is 0.02257283, which is still significantly different from the ``plus 5'' run.

We tried adding up to 25 extra points to the $1^{st}$ coordinate of the base set, but the matrix elements (and consequently, the phase shifts) still did not converge well enough. To perform the Gauss-Laguerre and Gauss-Legendre quadratures described in section \ref{sec:GaussQuad}, we have to compute the abscissae and weights for the Laguerre and Legendre polynomials. The weights depend on the abscissae, and the abscissae depend only on $n$, the number of quadrature points to use. This allows us to hardcode values for the abscissae and weights to speed up computations marginally. Previously, the long-range code used hardcoded values for some multiples of 5 but not all that were used. This code also did not have values hardcoded when the value of $n$ was not a multiple of 5. The integration points were then changed to use values that were multiples of 5, and all were hardcoded. The hardcoded values were also done in extended precision by using \emph{Mathematica}.

Figure \ref{fig:Base5vsBase5hardcoderound} shows the changes going from a previous ``plus 5'' run to a ``plus 5'' run with all hardcoded abscissae and weights. There is a relatively large difference in the matrix elements by making this change.
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/Base5vsBase5hardcoderound.png}}
	\caption{Changes going to hardcoded abscissae and weights}
	\label{fig:Base5vsBase5hardcoderound}
\end{figure}

The results were much better for the base set versus the ``plus 5'' set when hardcoded values for the abscissae and weight were used. Figure \ref{fig:BasehardcodevsBase5hardcode} shows this comparison, which has the large sections of black. All black and dark blue values are for terms where $q_i > 0$. This integration is done separately, as discussed in section \ref{sec:Swaveqigt0} for the S-wave.
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\includegraphics{QuadPoints/BasehardcodevsBase5hardcode.png}}
	\caption[Base set with hardcoded values vs. base set plus 5 with hardcoded values]{Base set with hardcoded values versus base set plus 5 with hardcoded values}
	\label{fig:BasehardcodevsBase5hardcode}
\end{figure}

We then completed a series of runs where we increased the number of integration points for each coordinate for $\omega = 2$. Figure \ref{fig:PlusCoord1-d} shows that the $1/r_{23}$ integration needs an extra 15 points in the first coordinate. The runs for the other coordinates were all performed with an extra 15 points in the first coordinate as well. From figures \ref{fig:PlusCoord2} through \ref{fig:PlusCoord8}, we see that the $5^{th}$ through $8^{th}$ coordinates possibly need more integration points.

\begin{figure}[H]
\centering
\subfloat[Part 1][Base vs. plus 5]{\includegraphics[height=1.2in]{QuadPoints/R23-BasevsBaseplus5-1st.png} \label{fig:PlusCoord1-a}}
\subfloat[Part 2][Plus 5 vs. plus 10]{\includegraphics[height=1.2in]{QuadPoints/R23-Baseplus5vsBaseplus10-1st.png} \label{fig:PlusCoord1-b}}
\subfloat[Part 3][Plus 10 vs. plus 15]{\includegraphics[height=1.2in]{QuadPoints/R23-Baseplus10vsBaseplus15-1st.png} \label{fig:PlusCoord1-c}}
\subfloat[Part 4][Plus 15 vs. plus 20]{\includegraphics[height=1.2in]{QuadPoints/R23-Baseplus15vsBaseplus20-1st.png} \label{fig:PlusCoord1-d}}
\caption{Comparison of extra points in $1^{st}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord1}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-2nd-plus5vsplus10.png} \label{fig:PlusCoord2-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-2nd-plus5vsplus10.png} \label{fig:PlusCoord2-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-2nd-plus5vsplus10.png} \label{fig:PlusCoord2-c}}
\caption{Comparison of extra points in $2^{nd}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord2}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-3rd-plus5vsplus10.png} \label{fig:PlusCoord3-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-3rd-plus5vsplus10.png} \label{fig:PlusCoord3-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-3rd-plus5vsplus10.png} \label{fig:PlusCoord3-c}}
\caption{Comparison of extra points in $3^{rd}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord3}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-4th-plus5vsplus10.png} \label{fig:PlusCoord4-a}}
\caption{Comparison of extra points in $4^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord4}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-5th-plus5vsplus10.png} \label{fig:PlusCoord5-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-5th-plus5vsplus10.png} \label{fig:PlusCoord5-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-5th-plus5vsplus10.png} \label{fig:PlusCoord5-c}}
\caption{Comparison of extra points in $5^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord5}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-6th-plus5vsplus10.png} \label{fig:PlusCoord6-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-6th-plus5vsplus10.png} \label{fig:PlusCoord6-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-6th-plus5vsplus10.png} \label{fig:PlusCoord6-c}}
\caption{Comparison of extra points in $6^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord6}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-7th-plus5vsplus10.png} \label{fig:PlusCoord7-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-7th-plus5vsplus10.png} \label{fig:PlusCoord7-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-7th-plus5vsplus10.png} \label{fig:PlusCoord7-c}}
\caption{Comparison of extra points in $7^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord7}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Part 1][Plus 5 vs. plus 10]{\includegraphics[height=0.8in]{QuadPoints/r23-8th-plus5vsplus10.png} \label{fig:PlusCoord8-a}}
\subfloat[Part 2][Plus 10 vs. plus 15]{\includegraphics[height=0.8in]{QuadPoints/r23-8th-plus5vsplus10.png} \label{fig:PlusCoord8-b}}
\subfloat[Part 3][Plus 15 vs. plus 20]{\includegraphics[height=0.8in]{QuadPoints/r23-8th-plus5vsplus10.png} \label{fig:PlusCoord8-c}}
\caption{Comparison of extra points in $8^{th}$ coordinate after hardcoded abscissae}
\label{fig:PlusCoord8}
\end{figure}

These tests give us an idea of what coordinates need more integration points. The phase shifts, not the matrix elements, are what we are concerned with, so we ran a number of tests for the phase shifts for $\omega = 6$. Table \ref{tab:5thcoordExtraPoints} shows how the phase shift stabilizes when the $5^{th}$ coordinate has an extra 10-15 points. For table \ref{tab:678thcoordExtraPoints}, we have 15 extra points in the $1^{st}$ and $5^{th}$ coordinates, then add points to all of the $6^{th}$, $7^{th}$ and $8^{th}$ coordinates simultaneously. To the required precision (3 significant figures), the phase shift does not change with this last set of changes. From these runs, we finally determined that the $1^{st}$ and $5^{th}$ coordinates needed 15 extra integration points each, and we can save adding extra points to the other coordinates.

\begin{table}[H]
\centering
\begin{tabular}{c c}
\toprule
Extra points & Phase shift \\
\midrule
+5 & 0.02273585 \\
+10 & 0.02256803 \\
+15 & 0.02257100 \\
+20 & 0.02258546 \\
+25 & 0.02257720 \\
+30 & 0.02257869 \\
+35 & 0.02257844 \\
\bottomrule
\end{tabular}
\caption{5th Coordinate Comparisons}
\label{tab:5thcoordExtraPoints}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{c c}
\toprule
Extra points & Phase shift \\
\midrule
+5 & 0.02255816 \\
+10 & 0.02255351 \\
+15 & 0.02255181 \\
\bottomrule
\end{tabular}
\caption{Extra points in the $6^{th}$, $7^{th}$ and $8^{th}$ coordinates}
\label{tab:678thcoordExtraPoints}
\end{table}

\todoi{Table of final integration points}


\subsection{Cusp Behavior}
\label{sec:Cusps}
\todoi{Discussion about the existence of the cusps in the integrand, possibly with graphs illustrating them.}

The short-long and long-long matrix element integrals have cusps in the integrands that must be dealt with. As an example, consider $(\bar{\phi}_i, L\bar{C}_0)$ from the S-wave. Using the notation of \cref{sec:LongLongR23}, where we are only computing the $r_{23}^{-1}$ terms (given fully later in \cref{eq:LCBar} on \pageref{eq:LCBar}),
\begin{align}
(\bar{\phi}_i, L\bar{C}_0)_B = 8\pi^2 & \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_2|}^{|r_1 + r_2|} \int_{|r_2 - r_3|}^{|r_2 + r_3|} \int_0^{2\pi} \bar{\phi}_i (\mathcal{L} \bar{C})_B  \nonumber \\
& \times r_1 r_3 r_{13} r_{23}\, d\phi_{13}\, dr_{23}\, dr_{12}\, dr_3\, dr_2\, dr_1.
\end{align}
Due to the integration limits of the $r_{13}$ and $r_{23}$ integrations, the $r_2$ integrand has a cusp at $r_2 = r_1$, and the $r_3$ integrand has a cusp at $r_3 = r_2$.

\Cref{fig:cusp} shows one such example of this cusp behavior for the $r_2$ integrand. All inner integrations ($\phi_{13}$, $r_{23}$, $r_{12}$, and $r_3$) are performed with a full set of integration points as described in \cref{tab:OptimalEffectiveCoords}.

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{Cusp}
	\caption{Example of cusp in S-wave short-long integration for $1/r_{23}$ term of $(\bar{\phi}_3, \mathcal{L}\bar{C}_0)$ with \mbox{$r_1 = 18.201$}}
	\label{fig:cusp}
\end{figure}

The $r_2$ integration could be performed using just the Gauss-Laguerre quadrature, since we are integrating over $(0,\infty)$. However, the cusp makes this integration slowly convergent. We instead split the integration interval into two parts and employ different quadratures for each. The integration is split to use Gauss-Legendre (\cref{eq:GaussLegGen}) on the interval $(0,r_1)$ and Gauss-Laguerre (\cref{eq:GaussLagGen}) on the interval $(r_1,\infty)$. Likewise, the $r_3$ integration is split into the intervals $(0,r_2)$ and $(r_2,\infty)$.

Doing this splitting requires many more function evaluations, so we use an approximation whenever $r_1$ is large enough. Specifically when $r_1$ is greater than a chosen distance, we use Gauss-Laguerre over the entire $(0,\infty)$ range. 

\todoi{Mention ECGs?}


In the example given in \cref{fig:cusp}, at $r_2 = 100$, the $r_2$ integrand is approximately $10^{-57}$, while at $r_2 = 25$, the integrand is approximately $10^{-9}$.


For the S-wave, all runs were performed with the cusp parameters set at $r_1 = 100$. When $r_1 > 100$, the $r_2$ and $r_3$ integrations are done using only Gauss-Laguerre, since the cusp is considered unimportant at that distance. When $r_1 \leq 100$, we use Gauss-Legendre before the cusp and Gauss-Laguerre after the cusp, as described in sections \ref{sec:LongLongNoR23} and \ref{sec:LongLongR23}. 

Van Reeth and Humberston used cusp parameters set at $r_1 = 25$, with little apparent loss of precision over using $r_1 = 100$ \cite{}. With $\kappa$ set at $0.1$, several tests were performed with different cusp parameters. With cusp parameters of 100, for $\omega = 6$, we are able to use 910 terms, giving a phase shift of $-0.427$. Doing the same run with cusp parameters of 25 allows us to use the same number of terms and gives a phase shift of $-0.427$ as well. In this particular example, the two sets of cusp parameters (25 and 100) differ in the sixth decimal place. As an example, for the inverse Kohn, $\delta = -0.42707291$ for the cusp parameters of 100, while $\delta = -0.42707370$ for cusp parameters of 25. To the required accuracy, the cusp parameters of 25 are sufficient for this example.

Smaller values for these cusp parameters were also tried, and these tests are summarized in table \ref{tab:SWaveCuspParameters}. When the cusp parameters are too small, such as the cases of 5, 10 and 15, the phase shifts do not converge well and start to diverge much earlier. The second column in this table shows the number of terms we can use for each set, but the number of terms to use is not exact due to the lack of good convergence, so the values here are approximate for small cusp parameters. The phase shift given for these three tests are therefore approximate as well. From this table, we conclude that 20 is the minimum value needed, at least for the $\kappa = 0.1$ case tested here for the $S$-wave. To be conservative, runs use cusp parameters of 100. 



\begin{table}[H]
\centering
\begin{tabular}{c c c}
\toprule
Cusp parameter & Terms possible & $\delta_0^+$ \\
\midrule
 5 & 531 & -0.4281 \\
10 & 684 & -0.4071 \\
15 & 684 & -0.4267 \\
20 & 910 & -0.4271 \\
25 & 910 & -0.4271 \\
100 & 910 & -0.4271 \\
\bottomrule
\end{tabular}
\caption{S-wave cusp parameters for $\kappa = 0.1$ at $\omega = 6$}
\label{tab:SWaveCuspParameters}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c c}
\toprule
Cusp parameter & $\delta_1^+$ \\
\midrule
 25 & 1.142923114 \\
 50 & 1.142923115 \\
100 & 1.142924602 \\
\bottomrule
\end{tabular}
\caption{P-wave cusp parameters for $S$ matrix $\kappa = 0.7$ at $\omega = 5$}
\label{tab:PWaveCuspParameters}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{c c c}
\toprule
Cusp parameter & $\delta_2^+$ & Time (hours) \\
\midrule
100 & 0.173 924 955 404 676 & 7.8 \\
150 & 0.173 924 955 404 797 & 8.9 \\
200 & 0.173 924 955 404 809 & 10.2 \\
250 & 0.173 924 955 404 748 & 10.8 \\
\bottomrule
\end{tabular}
\caption{D-wave cusp parameters for $S$ matrix $\kappa = 0.6$ at $\omega = 3$}
\label{tab:DWaveCuspParameters}
\end{table}


\section{Phase Shifts}
\label{sec:CompPhase}

\subsection{Phase Shift Convergence and Singularities}
Schwartz singularities are well-documented \cite{Lucchese1989,Cooper2009} and are non-physical ``resonances''. To avoid these singularities, we use multiple forms of the Kohn method: Kohn, inverse Kohn, generalized Kohn, generalized S-matrix complex Kohn, and generalized T-matrix complex Kohn. There may also be spurious results if the matrix elements are not converged, with one example being the feature at approximately 600 terms in figure \ref{fig:OriginalPhaseShifts-pvr} for the inverse Kohn. With the original ordering of the short-range terms, the phase shifts always break up at around 1100 terms, even when the matrix elements are converged. To alleviate this problem, we have implemented the Todd algorithm.

\subsection{Todd Algorithm Applied to the Scattering Problem}
\label{sec:ToddScattering}
From the bound state calculation using Todd's algorithm mentioned in section \ref{sec:ToddBound}, we know which short-range Hylleraas terms approximate the wavefunction of PsH best. We have observed that the short-range--short-range terms used in the same order as the output of the Todd algorithm will generate well-converged phase shifts. The separate code to determine the phase shifts uses the output from the bound state code. Again, the phase shift is plotted with respect to the number of short-range terms. The result is in figure \ref{fig:ToddPhaseShifts-pvrplus5}. The phase shifts are converged well until term 1216. A magnification of the graph in the small region around this term is provided in the inset graph.

At term 1216, the Kohn method result starts to diverge from the others. Shortly after term 1280, the five different methods for $\tau = 0.0 - 0.5$ start to diverge slightly. The differences here are very small (on the order of $10^{-5}$), unlike in figure \ref{fig:OriginalPhaseShifts-pvrplus5}. The jump in phase shifts seen in figure \ref{fig:OriginalPhaseShifts-pvrplus5} does not take place until much later and is not as large. Near 1650 terms, the phase shifts are not well converged, but they are also not as nearly ill-behaved as the results in figure \ref{fig:OriginalPhaseShifts-pvrplus5}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{ToddPhaseShifts-pvrplus5}
	\caption{Phase shifts with Todd terms and 5 extra points}
	\label{fig:ToddPhaseShifts-pvrplus5}
\end{figure}


\subsection{Todd Algorithm with Optimized Quadrature}


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{ToddOrderKohnPhaseShiftsOpt7}
	\caption{Phase shifts with Todd ordering and optimal set of points}
	\label{fig:ToddOrderKohnPhaseShiftsOpt7}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{ToddPhaseShifts-pvrbest}
	\caption{Phase shifts with resorted Todd terms and optimal set of points}
	\label{fig:ToddPhaseShifts-pvroptimized}
\end{figure}

Section \ref{sec:QuadraturePoints} has a discussion on the ``optimal set'' of quadrature points. When this optimal set is used with Todd's method, a very stable set of phase shifts is created. The graph in figure \ref{fig:ToddOrderKohnPhaseShiftsOpt7} shows how the phase shifts are well converged up to 1450 terms. At approximately term 1450, the phase shifts for the various Kohn methods begin to diverge. The Kohn results begin to diverge earlier than the other three variants in figure \ref{fig:ToddOrderKohnPhaseShiftsOpt7}. Notice that a similar behavior occurs in figure \ref{fig:ToddPhaseShifts-pvrplus5} at 1216 terms.

We have not developed an automated method of determining where this divergence occurs, so a visual inspection of the graph is needed. Graphs like the above are created in gnuplot, but we have developed a MATLAB script that makes this much easier. This script loads and plots data from all variations on the Kohn method, including the 35 values of $\tau$ for the generalized Kohn. MATLAB allows zooming of figures, so it is simple to find the term where the phase shifts begin to diverge.

For the extrapolation method discussed in section \ref{sec:Extrapolation}, it is useful to reorder the terms in the original ordering with increasing $\omega$. The first 1450 terms of figure \ref{fig:ToddOrderKohnPhaseShiftsOpt7} are used and then reordered, leading to the graph in figure \ref{fig:ToddPhaseShifts-pvroptimized}. There is a very small difference between the Kohn methods, but they do not diverge as in figure \ref{fig:ToddPhaseShifts-pvroptimized} after 1450 terms.


\todoi{Table of number of terms chosen from paper}


\subsection{Generalized Kohn}
\label{sec:CompGenKohn}
As mentioned in section \ref{sec:GenKohn}, the generalized Kohn method described by Cooper et al.\ allows for an adjustable parameter $\tau$ to be used. For our calculations of the Kohn and inverse Kohn, we set $\tau = 0$ and $\tau = \frac{\pi}{2}$, respectively. Before we started using the generalized Kohn method, it was difficult to tell if the difference between the Kohn and inverse Kohn phase shifts was due to a Schwartz singularity or a problem with linear dependence. With the generalized Kohn, we have a large number of individual Kohn methods to compare the results with.

\todo{Update}
The phase shift program steps through $\tau = 0$ to $\tau = 3.0$ in increments of $0.1$ for each N. There is also an additional modified phase shift program that steps through $\tau = 0.05$ to $\tau = 3.05$, also in increments of $0.1$, to get points in between the first set. The code can also be easily modified to use any value of $\tau$, which was used in figure \ref{fig:PhaseTau}.

The Kohn methods do not require a recalculation of the entire problem for each Kohn method. Once the original matrix equation (\cref{eq:GeneralKohnMatrix}) is calculated with the $\textbf{u}$ in \cref{eq:uKohn}, along with the $(\bar{S},\mathcal{L} \bar{S})$ term, the elements are reused to perform the other Kohn method calculations in \cref{sec:KohnApplied}.



\setlength{\abovecaptionskip}{0pt}   % 0.5cm as an example

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{PhaseTau}
	\caption[Phase shifts versus $\tau$ in the generalized Kohn method]{Phase shifts versus $\tau$ in the generalized Kohn method for $^1$S Ps-H scattering $(\omega = 7, \kappa = 0.1)$}
	\label{fig:PhaseTau}
\end{figure}




\section{Extrapolations}
\label{sec:Extrapolations}

\subsection{Extrapolation Description}

\todoi{Also extrapolate scattering lengths}

The tangent of the phase shifts is fitted to the function
\beq
\label{eq:PhaseExtrap}
\tan \delta_\ell^\pm(\omega) = \tan \delta_\ell^\pm(\omega \to \infty) + \frac{c}{\omega^p}.
\eeq
The $c$ and $p$ in this equation are fitting parameters. When plotted with respect to $\omega^{-p}$, the tangents of the phase shifts form nearly a straight line, with the y-intercept being the tangent of the extrapolated value of the phase shift, $\tan \delta^\pm(\omega \to \infty)$.

In Van Reeth and Humberston, the extrapolation is done using $\omega = 3$ through $\omega = 6$ \cite{VanReeth2003}. Our results go to $\omega = 7$, so we have completed the extrapolation using the sets $\omega = 3-6$, $\omega = 3-7$ and $\omega = 4-7$. The smallest residuals are normally found with the set $\omega = 4-7$. The values of the extrapolated phase shifts using this method are in table \ref{tab:SWavePhase}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{extrap-phase}
	\caption{Extrapolation for $^1$S at $\kappa = 0.01$. The extrapolation without resorting the short-range terms into increasing $\omega$ is given in (a), and the resorted version is in (b).}
	\label{fig:extrap-phase}
\end{figure}

As described in section \ref{sec:ToddScattering}, we omit certain terms using Todd's method. The output of this method from the bound state program described in section \ref{sec:ToddBound} is not ordered in terms of increasing $\omega$. This leads to difficulties when attempting to do the extrapolation in equation \ref{eq:PhaseExtrap}. The tangent of the phase shifts cannot be fitted to a straight line in this order. If we reorder the short-range terms back into their original order while still omitting terms, the tangent of the phase shifts can now be fitted to this straight line, as can be seen in figure \ref{}. The reordering does not affect the phase shifts in any case that we tested, since more terms are normally omitted in the scattering problem than just the bound state problem.

\subsection{Extrapolation Program}
I wrote a Python \cite{Python} program to extrapolate the phase shifts for a run for all Kohn method variants used, including the 35 values of $\tau$ used in each of the generalized Kohn, generalized S-matrix, and generalized T-matrix. The extrapolation is performed with a least-squares fitting using the \texttt{polyfit} function of the SciPy package \cite{SciPy}. This program can do the extrapolation over any interval of $\omega$ values requested. The extrapolations from the 109 total Kohn variants are compared to see if there is any large discrepancy between them, indicating numerical instability. The phase shift can have a singularity in the generalized Kohn method as seen in figure \ref{fig:PhaseTau}, so care must be taken to ensure extrapolations are not taken around this interval.



\section{Resonance Fitting}
\label{sec:ResonanceFit}

To find the resonance parameters (positions and widths), the phase shifts for multiple energy values are fitted to \cref{eq:ResonanceCurve}. There have been multiple programs described in the literature \cite{Tennyson1984, Stibbe1998, Sochi2013} to do these fittings, which I was not aware of when I originally wrote code to do these. Some of the difficulty with this type of fitting is choosing appropriate guesses for the resonance parameters. Ref.~\cite{Sochi2013} describes methods that some groups use to try to identify resonance parameters.

This nonlinear fitting can be difficult for general programs. I first tried the \texttt{Fit} and \texttt{FindFit} functions of Mathematica\textsuperscript{\textregistered} 8.0 \cite{Mathematica}, but these were unable to fit our data to this complicated nonlinear function. I next tried the \texttt{curve\_fit} function of SciPy \cite{SciPy} in Python\textsuperscript{\textregistered} 2.6 \cite{Python}, which was better able to fit the nonresonant (polynomial) part but had trouble with the $\arctan$ terms.

With the help of Ryan Bosca, I wrote a MATLAB\textsuperscript{\textregistered} \cite{matlab} script that uses the \texttt{nlinfit} routine of MATLAB. \texttt{nlinfit} is specifically designed for fitting to nonlinear functions, and its robust option allows for a variety of weighting functions to be used. This script does the fitting for all eight of the possible weightings: Bisquare, Andrews, Cauchy, Fair, Huber, Logistic, Talwar and Welsch. This fitting routine is also not as sensitive to the initial guesses as the Mathematica and SciPy routines.

Later on, I adapted this resonance fitting code to be called from IPython \cite{ipython} using the mlabwrap \cite{mlabwrap} Python to MATLAB wrapper. This allows fits and graphs of the fittings to be in the same IPython notebook, including the flexibility of querying the MySQL database for the phase shift data for any partial wave at any number of terms (see \cref{sec:XMLSQLIPy}). The mlabwrap package is difficult to install properly, requiring a compilation against MATLAB. The mlabwrap-purepy package \cite{mlabwrappurepy} has been created to simplify this, but we have not tried it yet.

The results of fitting the phase shifts from the $S$ matrix are shown in \cref{fig:swave-resonance-uncorrected-fits1,fig:swave-resonance-uncorrected-fits2} for each of the weighting functions, and the resonance parameters are given in each subfigure. There is good agreement between the fits performed with each of the weighting functions, and the Fair is the furthest from the others. In our testing, the Cauchy is consistently one of the best choices for fitting. In addition, \cref{fig:swave-resonance-uncorrected-res} has plots of the residuals (the absolute value of the difference between the fitted curve and the actual phase shifts). Each graph also has the residual sum of squares (RSS) calculated. The RSS gives an easy way to compare the performance of the weightings with each other. The Fair has a notably larger RSS than all of the other fittings.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-uncorrected-fits1}
	\caption{First set of uncorrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-uncorrected-fits1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-uncorrected-fits2}
	\caption{Second set of uncorrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-uncorrected-fits2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-uncorrected-res}
	\caption{Residuals for uncorrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-uncorrected-res}
\end{figure}

One important thing to notice with the fits in \cref{fig:swave-resonance-uncorrected-fits1,fig:swave-resonance-uncorrected-fits2} is that the $S$ matrix phase shifts extend above the fitted curve at the resonances before matching up again on the right side of the resonance below $-3.0$. The red curves still fit to the data well, but the fits can be improved by correcting for these. 

In \cref{eq:ResonanceCurve}, the first three polynomial terms form the background, and the two $\arctan$ terms represent the resonances. The range of $\arctan$ is $(-\frac{\pi}{2},\frac{\pi}{2})$, so the $\arctan$ parts of this model cannot bring the phase shift from the background (near 2.0) all the way to 0.0, as it can only add up to $\frac{\pi}{2}$ to the background. It is important to realize that the Kohn methods will only return phase shifts in a certain range. From \cref{eq:GenKohnL}, we are not finding the phase shifts directly but are rather calculating $\tan \delta_\ell$. The phase shifts found this way sometimes have to have $\pi$ added or subtracted to get them in the proper range.

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{swave-phases-reson-pi}
	\caption[$^1$S resonance data showing correction]{$^1$S resonance data showing raw data from complex Kohn and the corrected version. The solid gray line shows the polynomial background, and the vertical dashed lines give the calculated resonance positions.}
	\label{fig:swave-phases-reson-pi}
\end{figure}

\Cref{fig:swave-phases-reson-pi} shows the results of subtracting $\pi$ from phase shifts that are more than $\frac{\pi}{2}$ above the background. The original and the corrected data are both shown on this plot, and the background is given as a gray line. Note that the slope of the original data changes when crossing over the vertical dashed lines, which gives the resonance positions. When corrected, these upper points are moved down to their appropriate place, shown as x's on the graph. These match up better with the fitting curve. This fitting is an iterative process, because the background polynomial has to be determined with \cref{eq:ResonanceCurve} before we can do this correction. Then the phase shifts are fitted again after performing the correction.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-fits1}
	\caption{First set of corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-fits1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-fits2}
	\caption{Second set of corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-fits2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-res}
	\caption{Residuals for corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-res}
\end{figure}

Finally, this corrected and re-fitted data is shown in \cref{fig:swave-resonance-corrected-fits1,fig:swave-resonance-corrected-fits2}. The different weighting methods agree extremely well now, with the only notable differences being in the $^2\Gamma$ width of the second resonance for the Bisquare, Andrews, and Talwar fits. This is still a very small difference, and the other resonance parameters are almost all identical. \Cref{fig:swave-resonance-corrected-res} gives the residuals and RSS for the corrected phase shifts, similar to \cref{fig:swave-resonance-uncorrected-res}. The Bisquare, Andrews, and Talwar weightings have larger RSS values than the other weightings, indicating that their fits are not quite as accurate, but they are still relatively accurate.

The differences between the different weighting methods gives us one way to determine errors for our resonance parameters. Additionally, comparing the different Kohn methods gives us an idea of the error. The real-valued generalized Kohn methods give more disagreement, so we compute the resonance parameters for each of these Kohn methods. We also have to take into account Schwartz singularities. In \cref{fig:schwartz-singularity}(a) on \pageref{fig:schwartz-singularity}, the second resonance parameters are not as accurate. The fitting routine described here chooses fitting parameters of $^2E_R = \SI{5.0295}{eV}$ and $^2\Gamma = \SI{0.06011}{eV}$.
In \cref{fig:schwartz-singularity}(b), where there are no Schwartz singularities, $^2E_R = \SI{5.0278}{eV}$ and $^2\Gamma = \SI{0.06075}{eV}$. This highlights the importance of using multiple Kohn methods and trying to detect Schwartz singularities. After removing obvious Schwartz singularities, the mean value of the different Kohn methods for all weightings is taken for each resonance parameter, and these are the results listed for each of the partial waves in \cref{sec:SWaveResonances,sec:PWaveResonances,sec:DWaveResonance,sec:FWaveResonance}. The errors given in these tables are simply the standard deviation with all Kohn methods and weightings used for each resonance parameter. For this, we do not use the generalized $S$ or $T$ matrices, because using these would decrease the error, as they agree to a significant precision.

\todoi{IPython notebook?}




\section{Resonance Fitting}
\label{sec:ResonanceFit}
To fit the data, I first tried the \texttt{Fit} and \texttt{FindFit} functions of Mathematica\textsuperscript{\textregistered} 8.0 \cite{Mathematica}, but these were unable to fit our data to this complicated nonlinear function. I next tried the \texttt{curve\_fit} function of SciPy \cite{SciPy} in Python\textsuperscript{\textregistered} 2.6 \cite{Python}, which was better able to fit the nonresonant (polynomial) part.

I finally wrote a custom MATLAB\textsuperscript{\textregistered} \cite{matlab} script (with the help of Ryan Bosca) that uses the \texttt{nlinfit} routine of MATLAB. \texttt{nlinfit} is specifically designed for fitting to nonlinear functions, and its robust option allows for a variety of weighting functions to be used. Our script does the fitting for all eight of the possible weightings: Bisquare, Andrews, Cauchy, Fair, Huber, Logistic, Talwar and Welsch. This fitting routine is also not as sensitive to the initial conditions as the Mathematica and SciPy routines. The resulting graphs for $\omega = 7$ are in figures \ref{fig:ResOmega=7,pg1} and \ref{fig:ResOmega=7,pg2}.

Some fittings are better than others for this particular type of function. 
The Fair, Huber and Logistic weightings get the positions and widths of the 
resonances nearly correct, but the resulting curve clearly does not fit the 
data points leading to the first resonance, as can be seen in these figures. 
For a more rigorous examination of the fittings, the residuals were 
calculated for each point and graphed in figures
\ref{fig:ResOmega=6,Residuals} and \ref{fig:ResOmega=7,Residuals}.
I also calculated the residual sum of 
squares (RSS), which are shown in these figures. The Fair, Huber and Logistic 
weightings are clearly the worst fits for this function, though they are 
still respectable, giving parameters that are extremely close. It should be 
noted that the data will fit poorly to \cref{eq:ResonanceCurve} as 
the energy is decreased (i.e. far from the resonances), so the fits all have 
larger residuals for low energies.



\todoi{Mention how we add or subtract $\pi$}

\todoi{Talk about mlabwrap in Programs appendix}

\todoi{Cite Mathematica, SciPy and MATLAB. Show results of Mathematica and Python fittings. Do I need to write function names differently (e.g. FindFit)?}

\cite{Tennyson1984} \cite{Stibbe1998} \cite{Sochi2013}



\subsubsection{Resonance Graphs}
\label{sec:ResonanceGraphs}






\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-fits1}
	\caption{First set of corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-fits1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-fits2}
	\caption{Second set of corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-fits2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-res}
	\caption{Residuals for corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-res}
\end{figure}




\subsubsection{Resonance Values and Errors}
\label{sec:ResonanceErrors}

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{KohnFitting}
	\caption{Kohn fitting ($\tau = 0.0$) with different weighting functions for $\omega = 7$}
	\label{fig:KohnFitting}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{07Fitting}
	\caption{Fitting for $\tau = 0.7$ with different weighting functions for $\omega = 7$}
	\label{fig:07Fitting}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{pi4Fitting}
	\caption[Fitting for $\uppi/4$ with different weighting functions for $\omega = 7$]{Fitting for $\frac{\uppi}{4}$ with different weighting functions for $\omega = 7$}
	\label{fig:pi4Fitting}
\end{figure}

The results for the resonance parameters normally differ slightly for each of the Kohn methods.  To determine these, we gathered the data for all $\kappa$ values below the Ps(2s)+H(1s) channel threshold for a given Kohn method.  An example of the resultant graph for $\tau = 0.0$ (Kohn) can be seen in figure \ref{fig:KohnFitting}.  As described in section \ref{sec:ResonanceFit}, MATLAB's nonlinear fitting routine, nlinfit, is used with 8 different weighting functions to try various fits.  All 8 weighting functions give at least respectable fits to the data, but some fits are obviously worse than others.  For our data, the Fair, Logistic and Huber do not fit well for the second resonance.  The Cauchy will fit it close but not as close as the Andrews, Bisquare, Talwar and Welsch.  Out of the closest fittings, the Talwar is usually the worst of the four.

The closeness of the fits can be determined somewhat by plotting the residuals or looking at the values of the residual sum of squares, but these can be misleading near the resonances.  The MATLAB script that does the fittings also generates graphs for each fit with the data points superimposed.  The quality of the fitting is easily evaluated visually.  Due to the narrowness of the second resonance, the poor fits will usually overestimate its width and/or give the wrong position, causing the curve to not intersect the data points.

An example of a good set of fits is given in figure \ref{fig:KohnFitting}.  The graphs in this section are plotted with respect to the positronium energy, unlike the graphs in section \ref{sec:ResonanceGraphs}, which are plotted with respect to the momentum, $\kappa$.  All three of the plotted fitting curves follow the data nicely, with the exception of the extreme points near $\delta^+ = 0$.  This is not a failure of the fitting but instead a side effect of using the model given by \ref{eq:ResonanceCurve}.  The range of $\arctan$ is $(-\frac{\pi}{2},\frac{\pi}{2})$, so the $\arctan$ parts of this model cannot bring the phase shift from the background (near 2.0) all the way to 0.0, as it can only add up to $\frac{\pi}{2}$.  Regardless of this, the fits are nearly perfect in this example.

For some values of $\tau$, we have what appear to be Schwartz singularities.  Section \ref{sec:SchwartzSing} has more discussion on how these arise.  Figure \ref{fig:07Fitting} has a series of data points to the right of the second singularity that do not agree with runs for other values of $\tau$.  MATLAB attempts to fit with these, leading to incorrect parameters for the second resonance.  This type of behavior is observed for $\kappa = 0.7$ and $0.8$, so these runs are not included in the final results.  A similar problem is seen in figure \ref{fig:pi4Fitting}.  One data point at 5 eV is obviously shifted from where it ``should'' be.  The different weighting functions assign different importance to this point, so all three weighting functions gives different positions and widths of the second resonance.  This run for $\kappa = \frac{\pi}{4}$ was also discarded. 

The resonance parameters are calculated separately for each Kohn method (Kohn, inverse Kohn, etc.).  The ``good'' fits for all Kohn methods are all compiled in an Excel spreadsheet, where the mean, mode and errors are determined for each of the four resonance parameters.

The error for each parameter could be determined one of three ways.  The errors given in table \ref{tab:SWaveResonances} are simply the standard deviation.  We could have also used the standard error, which is given as $SE = \sigma / \sqrt{n}$.  The standard error is typically too small, since we are using approximately 100 tests.  For instance, using $\omega = 7$, for $^1E_R$, we get a standard error of $0.00001$, which implies a greater precision and agreement than our results have.  Another possible method for determining the error that we have looked at is to find the maximum deviation from the mean.  However, this overemphasizes the importance of outliers.  Using $\omega = 6$, the maximum deviation from the mean is $0.00586$ for $^2E_R$, which is $4.0$ times as large as the standard deviation. 


\todoi{Extrapolations here?}




\biblio
\end{document}