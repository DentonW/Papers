% -*- root: Dissertation.tex -*-
\documentclass[Dissertation.tex]{subfiles} 
\begin{document}


\chapter{Computation}
\label{chp:Computation}

\todoi{Should mention the Mathematica notebooks to generate Fortran lists for short-range matrix elements and possibly other programs}
\todoi{Put in information from ``Integration Point Numbers.nb''}
\todoi{Be clear that in the long-range code, we only calculate the Kohn elements, and then we rearrange them to create the others in the phase shift code. (Mentioned in \cref{sec:Kohn})}


\section{Short-Range Terms}
\label{sec:CompShort}

Matrix elements in \cref{eq:GeneralKohnMatrix} involving only short-range terms are handled differently than those that involve long-range terms (short-long and long-long). \cref{eq:BoundHFull} and 

\todoi{More lead in}
The bound state problem is a generalized eigenvalue problem (see \cref{eq:BoundGenEig}), but to simplify the following discussion, I will refer to the set of matrices \textbf{H} and \textbf{S} as a single matrix. The diagrams in this section show a single matrix, but it is in actuality a pair of matrices forming the generalized eigenvalue problem.


\subsection{Short-Range -- Short-Range Integrations}
\label{sec:ShortInt}
The short-range -- short-range integrals make up the bulk of the $\textbf{\emph{A}}$ matrix
(\cref{eq:GenKohnMatrixAXB}). The PsH bound state problem in 
\cref{chp:PsHBound} consists of only these types of matrix elements
(see \cref{eq:BoundWavefn}). The form of these short-range integrals is
\beq
\label{eq:FourBody}
I \equiv I(k_i, l_i, m_i, n_i, p_i, q_i; \alpha, \beta, \gamma) = \int e^{-(\alpha r_1 + \beta r_2 + \gamma r_3)} r_1^{k_i} r_2^{l_i} r_{12}^{m_i} r_3^{n_i} r_{13}^{p_i} r_{23}^{q_i} d\textbf{r}_1 d\textbf{r}_2 d\textbf{r}_3,
\eeq
with real-valued $\alpha, \beta, \gamma > 0$.

This class of integrals, the Hylleraas three-electron or four-body 
integrals, has been studied extensively. See
Refs.~\cite{Drake1995,Frolov2003,Pelzl1998,Ruiz2009,Pachucki2004} for just some of 
the papers detailing strategies on how to compute these integrals. The first 
three papers here use the same infinite summation to numerically solve this 
integral but use different techniques to accelerate the convergence, as the 
summation converges slowly for some arguments. The last paper by Pachucki et 
al.\ uses a very different approach with recursion relations, described in 
\cref{sec:RecursionRelations}.

Each of these methods has some restriction on how singular these integrals 
can be. For Drake and Yan's asymptotic expansion \cite{Drake1995,Yan1997},
$k_i, l_i, n_i \geq -2$ and $m_i, p_i, q_i \geq -1$. Yan extends these to
$k_i, l_i, m_i, n_i, p_i, q_i \geq -3$ in an additional paper \cite{Yan2000a}. For 
the recursion relations of Pachucki et al.\ \cite{Pachucki2004},
$k_i, l_i, m_i, n_i, p_i, q_i \geq -1$. Pachucki and Puchalski have two other papers,
one extending this to $k_i, l_i, m_i, n_i, p_i, q_i \geq -2$ \cite{Pachucki2005} 
and another even extending these to $k_i, l_i, m_i, n_i, p_i, q_i \geq -3$ 
\cite{Pachucki2008}. Our work for the D-wave has some terms with $k_i$, $l_i$,
or $n_i = -2$, restricting what methods we can use. The biggest need for 
integrals more singular than $\frac{1}{r_i}$ or $\frac{1}{r_{ij}}$ in most 
work (such as lithium energies \cite{Yan1997a,Puchalski2010}) is for 
relativistic or quantum electrodynamics effects
\cite{Yan1997,Pachucki2008,Puchalski2010}.

{\"O}hrn and Nordling \cite{Ohrn1963} were the first to give a method to 
solve this type of integral by splitting it into summations over $W$ 
functions. To deal with odd powers of $r_{ij}$, these terms are usually 
expanded in a Laplace expansion using Perkins's expression \cite{Perkins1968}.
A related expression is given by Sack \cite{Sack1964}. Porras and King
\cite{Porras1994} also use another expansion with Gegenbauer polynomials.

From Drake and Yan's paper \cite{Drake1995}, splitting into $W$ functions gives
\begin{align}
\label{eq:FourBodyExpansion}
I &= (4\pi)^3 \sum_{q=0}^\infty \sum_{k_{12} = 0}^{L_{12}} \sum_{k_{12} = 0}^{L_{23}} \sum_{k_{12} = 0}^{L_{13}} \frac{1}{(2q+1)^2} C_{j_{12} q k_{12}} C_{j_{23} q k_{23}} C_{j_{13} q k_{13}} \nonumber \\
& \times [W(\tilde{k}_i + 2q + 2 k_{12} + 2 k_{13}, \tilde{l}_i + m_i - 2 k_{12} + 2 k_{23}, \tilde{n}_i + q_i - 2 q - 2 k_{23} + p_i - 2 k_{13}; \alpha, \beta, \gamma) \nonumber \\
      & + W(\tilde{k}_i + 2q + 2 k_{12} + 2 k_{13}, \tilde{n}_i + p_i - 2 k_{13} + 2 k_{23}, \tilde{l}_i + m_i - 2 q - 2 k_{23} + q_i - 2 k_{13}; \alpha, \gamma, \beta) \nonumber \\
      & + W(\tilde{l}_i + 2q + 2 k_{12} + 2 k_{23}, \tilde{k}_i + m_i - 2 k_{12} + 2 k_{13}, \tilde{n}_i + q_i - 2 q - 2 k_{23} + p_i - 2 k_{13}; \beta, \alpha, \gamma) \nonumber \\
      & + W(\tilde{l}_i + 2q + 2 k_{12} + 2 k_{23}, \tilde{n}_i + q_i - 2 k_{23} + 2 k_{13}, \tilde{k}_i + m_i - 2 q - 2 k_{23} + p_i - 2 k_{13}; \beta, \gamma, \alpha) \nonumber \\
      & + W(\tilde{n}_i + 2q + 2 k_{23} + 2 k_{13}, \tilde{k}_i + p_i - 2 k_{13} + 2 k_{12}, \tilde{l}_i + m_i - 2 q - 2 k_{23} + q_i - 2 k_{13}; \gamma, \alpha, \beta) \nonumber \\
      & + W(\tilde{n}_i + 2q + 2 k_{23} + 2 k_{13}, \tilde{l}_i + q_i - 2 k_{23} + 2 k_{12}, \tilde{k}_i + m_i - 2 q - 2 k_{23} + p_i - 2 k_{13}; \gamma, \beta, \alpha)].
\end{align}
The $C_{jqk}$ coefficients are given by Perkins \cite{Perkins1968} as
\beq
\label{eq:Ccoeff}
C_{jqk} = \frac{2q+1}{j+2} \Binomial{j+2}{2k+1} \prod_{t=0}^{\min{(q-1),\frac{1}{2}(j+1)}} \frac{2k+2t-j}{2k+2q-2t+1}.
\eeq
The $W$ functions are expressed as an infinite summation of the $_2F_1$ hypergeometric functions \cite{Drake1995}:
\begin{align}
\label{eq:Wfunc}
W(l,m,n;\alpha,\beta,\gamma) = &\frac{\Factorial{l}}{(\alpha+\beta+\gamma)^{l+m+n+3}} \sum_{p=0}^\infty \frac{\Factorial{(l+m+n+p+2)}}{\Factorial{(l+1+p)} (l+m+2+p)} \left( \frac{\alpha}{\alpha+\beta+\gamma} \right)^p  \nonumber \\
& \times \Hypergeometric{2}{1}{1,l+m+n+p+3}{l+m+p+3}{\frac{\alpha+\beta}{\alpha+\beta+\gamma}}.
\end{align}
To reduce computation time, we also use the recursion relation in their paper of
\beq
\Hypergeometric{2}{1}{1,a}{c}{z} = 1 + \left( \frac{a}{c} \right) z \,\, \Hypergeometric{2}{1}{1,a+1}{c+1}{z}.
\eeq
A derivation of this is given in \cref{sec:Hypergeometric}.

The $k_{ij}$ summations in \cref{eq:FourBodyExpansion} are all finite. If all powers of $r_{ij}$ are odd, the $q$ summation is infinite, and if any of $r_{ij}$ is even, the $q$ summation becomes finite. For the finite $q$ sums, these direct sums are solved very accurately, but for the infinite sums, some integrals converge slowly. Particularly when all three $r_{ij}$ powers are odd and at least one is $-1$, the integrals converge the slowest. It is possible to restrict the basis set described by \cref{eq:OmegaDef} so that at least one of the $r_{ij}$ powers is even, making solving the integrals easier. However, this leads to slow convergence in the energy \cite{Drake1995}.

As Frolov and Bailey \cite{Frolov2003} note, these four-body integrals only work for systems with one infinitely heavy particle, such as PsH or Ps-H scattering. For systems with arbitrary masses, such as Ps$_2$ or Ps-Ps scattering, this has to be generalized to
\beq
\label{eq:FourBodyIntGen}
I = \int e^{-(\alpha r_1 + \beta r_2 + \gamma r_3 + a_{12} r_{12} + a_{13} r_{13} + a_{23} r_{23})} r_1^{k_i} r_2^{l_i} r_{12}^{m_i} r_3^{n_i} r_{13}^{p_i} r_{23}^{q_i} d\textbf{r}_1 d\textbf{r}_2 d\textbf{r}_3.
\eeq
Fromm and Hill give an analytic solution to this \cite{Fromm1987}, but it is extremely difficult to work with. Harris more recently solved this problem analytically using recursion relations \cite{Harris2009}, using a similar method to the recursion relations of Pachucki et al.\ \cite{Pachucki2004}. Both of these solutions restrict the powers of $r_i$ and $r_{ij}$ to $k_i, l_i, m_i, n_i, p_i, q_i \geq -1$. We have also looked at a subset of these integrals (refer to \cref{chp:Exponential}). As another extension of the Hylleraas basis set, some four-electron integrals can be reduced down to the three-electron integrals \cite{King1993,Pelzl2002}.


\subsubsection{Asymptotic Expansion}
\label{sec:AsymptoticExpansion}
For cases of \cref{eq:FourBodyExpansion} with all odd powers of $r_{ij}$, the $q$ summation is infinite, and the summation converges slowly. The convergence accelerator approach of Pelzl and King \cite{Pelzl1998,Pelzl2002} is one way to deal with this. I did limited testing with this approach but chose to instead use the asymptotic expansion method of Drake and Yan \cite{Drake1995}. In my testing, it was more numerically stable, and it has been generalized to arbitrary angular momenta \cite{Yan1997}.

As an example in Drake and Yan's paper \cite{Drake1995}, direct calculation of the $q$ summation for $I(0,0,0,-1,-1,-1; 1,1,1)$ only reaches an accuracy of $1.5 x 10^{-13}$ after 6860 terms. With their asymptotic expansion method, the integral has converged to approximately $2.2 x 10^{-16}$ after only 21 terms. The summation converges monotonically and asymptotically. Drake and Yan use this knowledge to speed up the convergence of the integration. Details of this method can be found in their paper \cite{Drake1995}.

I use this asymptotic expansion method in all calculations of the short--short integrals through the H-wave, and it performed very well. In fact, my quadruple precision code often calculates matrix elements to better than $1$ part in $10^{20}$ for the S-wave. \Cref{tab:AsympExpan} gives an example of the convergence of a single integral by only calculating the direct sum of \cref{eq:FourBodyExpansion} in the $S_d(N)$ column and with the asymptotic expansion in the $S_a(N)$ column. These four-body integrals are relatively quick to calculate, and most of the runs to compute them complete in a matter of hours.

\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$N$ & $S_d(N)$ & $\Delta S_d(N)$ & $S_a(N)$ \\
\midrule
17 & 684.106 432 091 &               & 684.113 411 842 629 912 374 349 \\
18 & 684.107 475 306 & 0.001 043 214 & 684.113 411 842 629 911 836 645 \\
19 & 684.108 320 637 & 0.000 845 331 & 684.113 411 842 629 911 829 661 \\
20 & 684.109 012 851 & 0.000 692 213 & 684.113 411 842 629 911 835 071 \\
21 & 684.109 585 093 & 0.000 572 241 & 684.113 411 842 629 911 836 095 \\
22 & 684.110 062 261 & 0.000 477 167 & 684.113 411 842 629 911 836 195 \\
23 & 684.110 463 302 & 0.000 401 041 & 684.113 411 842 629 911 836 186 \\
24 & 684.110 802 809 & 0.000 339 506 & 684.113 411 842 629 911 836 178 \\
25 & 684.111 092 142 & 0.000 289 333 & 684.113 411 842 629 911 836 174 \\
26 & 684.111 340 236 & 0.000 248 094 & 684.113 411 842 629 911 836 173 \\
27 & 684.111 554 183 & 0.000 213 946 & 684.113 411 842 629 911 836 172 \\
28 & 684.111 739 660 & 0.000 185 477 & 684.113 411 842 629 911 836 172 \\
29 & 684.111 901 249 & 0.000 161 588 & 684.113 411 842 629 911 836 172 \\
30 & 684.112 042 674 & 0.000 141 425 & 684.113 411 842 629 911 836 172 \\
\bottomrule
\end{tabular}
\caption[Convergence of direct sum, $S_d(N)$, against the asymptotic expansion, $S_a(N)$.]
{Convergence of direct sum, $S_d(N)$, against the 
asymptotic expansion, $S_a(N)$. This is an extension of Table I in Drake and 
Yan's work \cite{Drake1995} and uses $\Lambda = 15$. $\Delta S_d(N)$ gives 
the difference between successive direct sums.}
\label{tab:AsympExpan}
\end{table}

\subsubsection{Recursion Relations}
\label{sec:RecursionRelations}
After the S-wave calculations were completed, we learned of an analytic, 
instead of numerical, solution to these three-electron integrals, derived by 
Pachucki et al.\ \cite{Pachucki2004}. They
were not the first to derive an analytic solution to the three-electron 
integrals, but the first by Fromm and Hill \cite{Fromm1987} is not very 
practical to use, considering its very complicated form. Each set of $r_i$ 
and $r_{ij}$ powers requires a new rederivation from the Fromm and Hill 
result. The recursion relations from Pachucki et al.\ are complicated but also general.

Like many other types of recursion relations, these recursion relations may 
not be stable for higher $\omega$ values, depending on the calculated 
precision. I have tested through $\omega = 8$, and this method produced 
stable results under quadruple precision. In some of their work on Li and
Be$^+$, their group uses sextuple precision, as quadruple precision becomes 
insufficient near $\omega = 10$ \cite{Puchalski2006}, which is much higher 
than we can use in the Ps-H scattering calculations.

These are used as a check on the accuracy of the asymptotic expansion method in
\cref{sec:AsymptoticExpansion} for the S-wave and P-wave. The D-wave short-range
integrals cannot be evaluated using the recursion relations in
Ref.~\cite{Pachucki2004} due to the $r_i^{-2}$ terms that appear. Solving these
using the recursion relations would require implementing the extended method in
Ref.~\cite{Pachucki2005}. However, the asymptotic expansion method has proven
to be stable and accurate so far.


\subsubsection{W Functions}
\label{sec:WFunctions}

While evaluating the integrals in the PsH short-range code, we noticed that the
$W$ functions in \cref{eq:FourBodyExpansion} could be called more than once by
multiple integrals. For the S-wave, there are 34 terms in \cref{eq:GradGradShort},
and each matrix element in \textbf{H} in \cref{eq:BoundGenEig} requires these 34
integrals to be evaluated. The overall powers of $r_1$, $r_2$, $r_{12}$, etc.\ in
\cref{eq:FourBody} can be the same for a set of $\phi_i$ with $\phi_j$. Also for a
set of integrals without the overall powers the same, there is the possibility of
two different integrals calling the same $W$ function.

To speed up the program significantly, we precompute the $W$ functions and store
the results in a look up table. The 4-dimensional matrix is constructed with the
limits derived in \cref{chp:WLimits}. Even though there is a possibility for the
$W$ function arguments to be anywhere in this space, not all of the $W$ matrix
elements will be used.

The current S-wave, P-wave and D-wave short-range codes first do a dummy run
where the integrals are not actually calculated, but any $W$ matrix elements
that need to be computed are marked. Only these are computed, and then the code
runs through again using this $W$ matrix look up table. \Cref{tab:WFuncUnusedS}
shows that only $10.6\%$ of the $W$ matrix elements are actually needed for the
S-wave at a relatively high value of $\omega$. Similarly for the D-wave,
\cref{tab:WFuncUnusedD} shows that only $10.5\%$ are needed.

\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$\omega$ & Used Terms & Total Terms & Percentage Used \\
\midrule
1 & 	115   &  20,000 & 	0.575\% \\
2 & 	921   &  27,040 & 	3.41\% \\
3 & 	1,939 &  34,992 & 	5.54\% \\
4 & 	3,140 &  43,904 & 	7.15\% \\
5 & 	4,573 &  53,824 &	8.49\% \\
6 & 	6,246 &  64,800 &	9.64\% \\
7 & 	8,147 &  76,880 &	10.6\% \\
\bottomrule
\end{tabular}
\caption{S-wave W function terms used}
\label{tab:WFuncUnusedS}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
$\omega$ & Used Terms & Total Terms & Percentage Used \\
\midrule
0 & 150		&	196,290 &	0.0764\% \\
1 & 1,670	&	259,932 &	0.642\% \\
2 & 14,076	&	332,262 &	4.24\% \\
3 & 23,384	&	413,520 &	5.65\% \\
4 & 36,418	&	503,946 &	7.23\% \\
5 & 51,420  &	603,780 &	8.52\% \\
6 & 68,438	&	713,262 &	9.60\% \\
7 & 87,520	&	832,632 &  10.5\% \\
\bottomrule
\end{tabular}
\caption{D-wave W function terms used}
\label{tab:WFuncUnusedD}
\end{table}

Another more sophisticated method that we have started using is given in
\cref{sec:PrimeFactor}. This looks instead at the overall integrations,
not the $W$ function arguments.



\subsection{Linear Dependence in the Bound State Calculation}
With infinite precision in calculations, all terms from the basis set could be used. However, due to the limited precision inherent in computer calculations, when using large basis sets, near linear dependences will exist in the matrices. The goal is to identify and eliminate terms that exhibit near linear dependence with other terms. These terms are not exactly linearly dependent, if infinite precision was possible, but they are linearly dependent to computer precision.

To calculate the eigenvalues of the generalized eigenproblem, the LAPACK 
routine \texttt{dsygv} is used \cite{dsygv}. For the basis set consisting of 
terms from $\omega$ = 5, LAPACK computes the eigenvalues without errors. 
Adding in terms corresponding to $\omega = 6$ for some sets of nonlinear 
parameters causes \texttt{dsygv} to fail with an error set in the last 
parameter, Info. This error is always an integer greater than the number of 
terms, indicating from the library documentation that ``the leading minor of 
order i of B is not positive definite'' \cite{dsygv}. L\"uchow and 
Kleindienst also encountered a similar problem using NAG and EISPACK
\cite{Luchow1993}.

This error does suggest that one approach to identifying problematic terms is 
to check for positive definiteness of the overlap matrix
$\left\langle \phi_i | \phi_j \right\rangle$. This is one method that
Yan et. al use to isolate 
problematic terms \cite{Yan1999}. They test the eigenvalues of the overlap 
matrix to see if any are small or negative, though there is no mention in 
their paper of what value of ``small'' is used. We attempted to use this fact 
to remove problematic terms, but too many terms were removed, leading to an 
energy that converged too slowly. In several papers by Yan and others
\cite{Yan1998,Yan1998a,Yan1999,Drake1995,Yan1997a}, terms with $j1 > j2$ are 
omitted if $l_1 = l_2$ and $\alpha \approx \beta$, along with $j_1 = j_2$ if
$j_{23} > j_{31}$. These terms were likely determined by trial and error 
or by noticing patterns in terms that produced near linear dependence.

Another technique Yan et al.\ used was to partition the basis set into five 
sectors, each with a different set of nonlinear parameters and maximum
$\omega$ \cite{Yan1999}. The sectors also have restrictions on the interparticle
$r_{ij}$ terms, mainly limiting the power of $r_{23}$ and $r_{31}$, which are the 
electron-positron coordinates in their paper (corresponding to $r_{12}$ and
$r_{13}$ in our work). The techniques used by Yan, Drake and Ho for restricting 
the set of terms are not used in our work.


\subsection{Allan Todd's Algorithm}
\label{sec:ToddBound}
In trying to determine the energy eigenvalues, we noticed that the ordering 
of the terms could determine whether there was linear dependence in the 
matrices. Allan Todd's algorithm was attractive, because it reorders the 
matrices to obtain the best possible energy, and it is a purely computational 
approach \cite{Todd2007}. So far, we have not seen any physical reason why 
certain terms should introduce a near linear dependence. A description of his 
algorithm as implemented by us follows.

The total number of terms to look at is $N = N(\omega)$ (see \cref{eq:NumberTermsOmega}).
 N matrices of size 1x1 are created for each term. This 
is done for the overlap and the
 $\left\langle \phi \left| \,H \right| \phi \right\rangle$
 matrices together. The LAPACK \texttt{dsygv} routine is used to 
determine the lowest eigenvalue for each of these N sets. These energy 
eigenvalues are compared against one another, and the term with the lowest 
energy is chosen. In the next step, the first basis function from the 
previous step is combined with each unused term to create N-1 matrices of 
size 2x2. Again, the energy eigenvalues for each of the N-1 matrices are 
compared against each other, and the term yielding the lowest energy is 
chosen as the second basis function. This is done again with 3x3 matrices for 
each of the N-2 remaining terms combined with the basis functions chosen in 
the first two steps. This procedure is repeated until all terms have been 
used or the remaining terms are problematic.

In his original algorithm, Allan Todd looked at the eigenvalues computed from 
the upper and lower triangular matrices. Normally, the overlap and H matrices 
are symmetric, but this is not true to machine precision due to truncation 
and rounding. If the energy eigenvalues from the upper and lower triangles 
differ by more than $10^{-7}$ (in atomic units), the last added term is 
considered problematic and discarded.

In our testing for $\omega = 6$, no terms were omitted due to the reordering. 
As noted earlier, before implementing this algorithm, LAPACK would fail when 
trying to calculate the eigenvalues, so the ordering is important for getting 
the best possible energy. For $\omega$ = 7, 116 terms were omitted, out of a 
total of 1716 terms. The criteria that the eigenvalues for the upper and 
lower matrices differs by no more than a certain amount was not needed in 
this case. The Info parameter of the LAPACK dsygv function is checked for 
both the upper and lower matrix eigenvalue calculations, and the last added 
term is discarded if it causes an error due to linear dependence. When only 
the 116 problematic terms were left, every one of them caused LAPACK to 
error. If a term was problematic at any stage, it continued to be problematic 
in all further stages, so computation time can be decreased by immediately 
discarding it.

For larger basis sets, this algorithm becomes extremely slow, as determining 
the eigenvalues is an $O(N^3)$ operation. It can easily be parallelized, 
since we are computing the eigenvalues for a large number of matrices. Our 
program has been parallelized using OpenMP \cite{OpenMP} for intranode
communications and MPI \cite{MPI} for internode communications. Todd's
algorithm provides the best converged energy for a set of terms, albeit
at a cost of computational speed.

\begin{figure}[H]
	\centering
	{\includegraphics[height=1.5in]{Todd}}
	\caption{Diagram of Todd's procedure}
	\label{fig:Todd}
\end{figure}


\subsection{L\"uchow and Kleindienst Algorithm} \label{sec:LuchowBound}
The algorithm that L\"uchow and Kleindienst propose \cite{Luchow1992} is 
similar to Todd's algorithm. They use all terms through $\omega = 5$, since 
none of these have linear dependence within this set. This fact could be 
applied to Allan Todd's algorithm to speed up the calculation. The rest of 
the terms are partitioned up into blocks of 300-400 terms, except the last 
block, which will have modulus(N - 462, block size) terms. A key difference 
from Allan Todd's algorithm is that the terms should be ordered in terms of 
increasing $\omega$. In Allan Todd's algorithm, the initial ordering does not 
matter. The terms in the first block are added to the basis set for
$\omega = 5$, which has 462 terms. If we use a block with 300 terms, then 300 matrices 
of size $463 \times 463$ are created, and their eigenvalues are determined. Whichever 
term gives the lowest energy when added to this set is saved to the basis 
set. This continues with ever-increasing matrix sizes, until all terms in the 
block are used, or a linear dependence is noticed. L\"uchow and Kleindienst 
propose that linear dependences can be prevented by omitting terms that do 
not contribute significantly to the energy. The energy from the previous step 
is saved, and the energy from the current step is compared to this. If the 
difference in energies is small, e.g. $\Delta E < 10^{-10} E_h$ in their 
paper, then that term is omitted. Since the lowest energy is chosen at each 
step, when this $\Delta E$ is too small, the rest of the block can be omitted.

Two parameters that are adjustable in this algorithm are the block size and 
cutoff value for the energy. To determine the best values for each, we
set $\omega = 7$, and one of the parameters was held constant. To 
determine the optimal block size, an energy cutoff of $5\e{-9}$ was used.
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Block size & Used terms & Energy & Time (min)\\
\hline
200 & 1041 & -0.789 181 890 & 470 \\
250 & 1024 & -0.789 184 500 & 572 \\
300 & 960 &  -0.789 180 520 & 749 \\
350 & 999 &  -0.789 181 942 & unknown \\
400 & 1012 & -0.789 185 588 & 852 \\
500 & 985 &  -0.789 184 116 & 764 \\
\hline
\end{tabular}
\end{center}

For most trials, increasing the block size also increased the time it took to 
complete the computation. It can be seen that a block size of N-462 gives a 
modified version of Todd's algorithm, since we are testing all terms at each 
step. A block size of 1 would correspond to no reordering of terms, leading 
to including problematic terms. The cutoff value for $\Delta E$,
$E_{\text{cutoff}}$, is $10^{-10} E_h$ in L\"uchow and Kleindienst's paper
\cite{Luchow1992}, but a value of $10^{-10}$ in our calculations led to LAPACK 
errors due to linear dependences. The block size was set to 400, and it was
determined that an energy cutoff of $10^{-9}$ was the largest value that caused linear 
dependences.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Energy cutoff & Used terms & Energy\\
\hline
2\e{-9} & 1116 & -0.789 185 858 \\
3\e{-9} & 1053 & -0.789 186 346 \\
4\e{-9} & 1043 & -0.789 185 173 \\
5\e{-9} & 1012 & -0.789 185 588 \\
1\e{-8} & 963 &  -0.789 188 024 \\
\hline
\end{tabular}
\end{center}

Unfortunately, the intuition that a larger block size and smaller energy 
cutoff will yield a better final energy is not necessarily correct. The best 
value of the energy was actually obtained for the largest tested energy 
cutoff of $10^{-8}$. A block size of 500 also gave a worse energy than did a 
block size of 400. More tests need to be performed to determine the optimal 
value of both parameters, and these could possibly change for higher values 
of $\omega$ or for a different system.

We decided to use the Todd algorithm (\cref{sec:ToddBound}) due to the better 
energy obtained and the larger set of terms returned. The Todd algorithm is 
also more consistent between runs. The trade-off is the increased time to 
perform a calculation, especially for large $\omega$.

\begin{figure}[H]
	\centering
	{\includegraphics[height=2in]{Luchow}}
	\caption{Diagram of L\"uchow and Kleindienst procedure}
	\label{fig:Luchow}
\end{figure}


\subsection{Restricted Set}
\label{sec:Restricted}
Van Reeth and Humberston \cite{VanReeth2003} found that restricting the power 
of the $r_3$ coordinate could significantly improve their numerics, allowing 
them to use more short-range terms. Specifically, we restrict the $r_3$ power 
(see \cref{eq:PhiDef}) so that $n_i \leq 2$ if $\omega \geq 3$, and we refer 
to this as the restricted set.

We normally use Todd's algorithm, described in \cref{sec:ToddBound}, or we 
use the full unrestricted set when possible. For the cases of $^1$F and $^3$F 
for low $\kappa$, described in \cref{sec:FNonlinear}, we use the restricted 
set.


\section{Long-Range Terms}
\label{sec:CompLong}

collectively referred to as long-range integrations


\subsection{Perimetric Coordinates}
\label{sec:PerimetricCoords}

Perimetric coordinates are used for the long-long integrations in the $S$-wave code. If perimetric coordinates are used for $r_1$, $r_2$ and $r_{12}$, then these are defined by \cite{Armour1991}

\begin{align}
\label{eq:PerimetricCoords1}
\nonumber x &= r_1 + r_2 - r_{12} \\
\nonumber y &= r_2 + r_{12} - r_1 \\
z &= r_{12} + r_1 - r_2.
\end{align}
These can alternately be written as
\begin{align}
\label{eq:PerimetricCoords2}
\nonumber r_1 &= \frac{x+z}{2} \\
\nonumber r_2 &= \frac{x+y}{2} \\
\nonumber r_{12} &= \frac{y+z}{2}.
\end{align}

From \cref{eq:dtau1}, the volume element after integration over the external angles is
\beq
d\tau = 8\pi^2 dr_1 r_2 dr_2 r_3 dr_3 r_{12} dr_{12} r_{13} dr_{13} d\phi_{23}.
\eeq
We need to perform a change of variables to use perimetric coordinates for $r_1$, $r_2$ and $r_{12}$. The Jacobian is
\beq
\label{eq:PerimetricJacobian}
J(x,y,z) = 
\left| {\begin{array}{ccc}
 \frac{\partial r_1}{\partial x} & \frac{\partial r_1}{\partial y} & \frac{\partial r_1}{\partial z}  \\
 \frac{\partial r_2}{\partial x} & \frac{\partial r_2}{\partial y} & \frac{\partial r_2}{\partial z}  \\
 \frac{\partial r_{12}}{\partial x} & \frac{\partial r_{12}}{\partial y} & \frac{\partial r_{12}}{\partial z}  \\
 \end{array} } \right|
=
\left| {\begin{array}{ccc}
 \frac{1}{2} & 0 & \frac{1}{2} \\
 \frac{1}{2} & \frac{1}{2} & 0 \\
 0 & \frac{1}{2} & \frac{1}{2}
 \end{array} } \right|
=
\frac{1}{4}.
\eeq

\noindent This gives a transformed volume element of
\beq
\label{eq:PerimetricVolEl}
d\tau = 2\pi^2 r_2 r_3 r_{12} r_{13} dx\, dy\, dz\, dr_3\, dr_{13}\, d\phi_{23}.
\eeq

\noindent The limits for each of the perimetric coordinates are 0 to $\infty$.


\subsection{Gaussian Quadratures}
\label{sec:GaussQuad}
\todoi{Mention looking into \cite{Bailey2004,Ma1996,Mori2001,Odrzywolek2011,Fukuda2005}}
Gaussian quadratures are used to integrate many classes of integrals. In their most general form, these quadratures are given by \cite[p.887]{Abramowitz1965}
\beq
\label{eq:GeneralQuadratures}
\int_a^b W(x) f(x) dx \approx \sum_{i=1}^n w_i f(x_i).
\eeq
Gaussian quadratures are particularly attractive, since they give exact results for polynomials up to degree $2n-1$. The weight function $W(x)$ can be chosen for certain classes of integrals. Three main types of weight functions are used in this work.


\subsubsection{Gauss-Legendre Quadrature}
\label{sec:GaussLegendre}
If the weight function is chosen as $W(x)=1$, and the integration interval is $(-1,1)$, this is known as Gauss-Legendre quadrature (or sometimes known simply as Gaussian quadrature). The orthogonal polynomials used are the Legendre polynomials, $P_n(x)$. \Cref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussLeg}
\int_{-1}^1 f(x) dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where the $x_i$ abscissas are the $i^{th}$ zeros of $L_n(x)$, and the weights are given by
\beq
\label{eq:GaussLegWeights}
w_i = \frac{2}{(1-x_i^2)[P^\prime_n(x_i)]^2}.
\eeq
The limits of integration must be from $-1$ to $1$, but this is generalized by using the transformation \cite{Abramowitz1965}
\begin{align}
\label{eq:GaussLegGen}
\int_a^b f(x) dx &= \frac{b-a}{2} \int_{-1}^1 f \left(\frac{b-a}{2} x + \frac{a+b}{2}\right) dx \\
&\approx \frac{b-a}{2} \sum_{i=1}^n w_i f \left(\frac{b-a}{2} x_i + \frac{a+b}{2}\right).
\end{align}


\subsubsection{Gauss-Laguerre Quadrature}
\label{sec:GaussLag}
The Gauss-Legendre quadrature cannot be used on semi-infinite intervals, so we use the Gauss-Laguerre quadrature in these cases. The orthogonal polynomials in this case are the Laguerre polynomials, $L_n(x)$, and the weight function is $W(x) = e^{-x}$. \Cref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussLag}
\int_0^\infty e^{-x} f(x) dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where the $x_i$ abscissas are the $i^{th}$ zeros of $L_n(x)$, and the weights are given by
\beq
\label{eq:GaussLagWeights}
w_i = \frac{x_i}{(n+1)^2 [L_{n+1}(x_i)]^2}.
\eeq

When the integration is over the interval $(a,\infty)$, \cref{eq:GaussLag} is easily transformed by
\beq
\label{eq:GaussLagGen1}
\int_a^\infty e^{-x} f(x) dx = \int_0^\infty e^{-(x+a)} f(x+a) dx = e^{-a} \int_0^\infty e^{-x} f(x+a) dx \approx e^{-a} \sum_{i=1}^n w_i f(x_i+a).
\eeq

\noindent A more general form of this is obtained by using a coefficient in the exponential, i.e.
\beq
\label{eq:GaussLagGen2}
\int_a^\infty e^{-m x} f(x) dx = \frac{1}{m} \int_a^\infty e^{-y} f\left(\frac{y}{m}\right) dy,
\eeq
where we have defined $y = m x$.  This allows for \cref{eq:GaussLagGen1} to be generalized to
\begin{align}
\label{eq:GaussLagGen}
\nonumber \int_a^\infty e^{-m x} f(x) dx &= \frac{1}{m} \int_{ma}^\infty e^{-y} f\left(\frac{y}{m}\right) dy = \frac{1}{m} \int_0^\infty e^{-(y+ma)} f\left(\frac{y}{m}+a\right) dy \\
& = \frac{e^{-ma}}{m} \int_0^\infty e^{-y} f\left(\frac{y}{m}+a\right) dy \approx \frac{e^{-ma}}{m} \sum_{i=1}^n w_i f\left(\frac{y_i}{m}+a\right).
\end{align}
The $y_i$ abscissas and $w_i$ weights are the same as the less general case in
\cref{eq:GaussLag,eq:GaussLagWeights}. This general form of Gauss-Laguerre
quadrature is what we use for our semi-infinite integrations.


\subsubsection{Chebyshev--Gauss Quadrature}
\label{sec:ChebyshevGauss1}
If the weight function is chosen as $W(x)=\frac{1}{\sqrt{1-x^2}}$, and the
integration interval is $(-1,1)$, this is known as Chebyshev-Gauss quadrature.
The orthogonal polynomials used are the Chebyshev polynomials of the first
kind, $T_n(x)$. \Cref{eq:GeneralQuadratures} becomes
\beq
\label{eq:GaussCheb}
\int_{-1}^1 \frac{f(x)}{\sqrt{1-x^2}} dx \approx \sum_{i=1}^n w_i f(x_i),
\eeq
where
\beq
\label{eq:GaussChebAbsWeights}
x_i = \cos\left(\frac{2i-1}{n}\pi\right) \text{ and } w_i = \frac{\pi}{n}.
\eeq
This quadrature is used for the internal angular integrations. A discussion of
how to use this for these integrations is found in \cref{sec:ChebyshevGauss}.

\subsection{Long-Range -- Long-Range}
\label{sec:LongLongInt}

The scattering program calculates only the short-long and long-long matrix 
elements. The volume element in equation \ref{eq:dtau} has an internal angle 
of $\phi_{23}$ to integrate over.  When a term has a a negative power of
$r_{23}$, a large number of integration points must be used for reasonable 
accuracy. Instead, we split the integration such that one part is missing the 
the $r_{23}^{-1}$ term and the other contains only the $r_{23}^{-1}$ term.

The first integration excluding the $r_{23}^{-1}$ term has negative powers of 
$r_i$ and $r_{ij}$ cancelled by the corresponding terms in the volume element 
given by equation \ref{}.  For the integration over the $r_{23}^{-1}$ term, 
we use an alternative volume element, namely that given by equation \ref{}.  
The $r_{23}^{-1}$ is then cancelled by the $r_{23}$ in this volume element.

\todoi{Add the section about the volume elements in an appendix.}
\todoi{$\mathcal{L}$ instead of L}

\subsubsection{Integration without the \texorpdfstring{$r_{23}^{-1}$} {1/r23} term}
\label{sec:LongLongNoR23}
The simplest long-long matrix element to evaluate is $(\bar{S},\mathcal{L} \bar{S})$.  From \cref{eq:SbarLSbar}, not including its $r_{23}^{-1}$ term, this is
\beq
(\bar{S},\mathcal{L} \bar{S})_A = \pm \left(S^\prime,\mathcal{L} S\right)_A = \pm \left(S^\prime, \left[ \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}}\right] S\right).
\eeq

For this type of integration, we use perimetric coordinates as described in section \ref{sec:PerimetricCoords}.
\begin{align}
\label{eq:SBarSBarInt}
(\bar{S},\mathcal{L} \bar{S})_A = \pm &2\pi^2 \int_0^\infty \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_0^{2\pi}  S^\prime S \left[ \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}}\right] \\
&\times r_2 r_3 r_{12} r_{13}\, d\phi_{23}\, dr_{13}\, dr_3\, dz\, dy\, dx
\end{align}

The $\phi_{23}$ integration is done analytically. Since $S$ and $S^\prime$ have no $r_{23}$ dependence and there is no $r_{23}$ term in the brackets, the integration over $\phi_{23}$ is simply $2\pi$.  The $r_{13}$ integration uses the Gauss-Laguerre quadrature from section \ref{sec:GaussLegendre}. The $x$, $y$ and $z$ integrations use Gauss-Laguerre quadrature (section \ref{sec:GaussLag}), since they are semi-infinite.

The $r_3$ integration could also be performed using just the Gauss-Laguerre 
quadrature. However, the integrand for the $r_3$ integration has a 
discontinuity in its slope at $r_3=r_1$, creating a cusp
(see \cref{sec:Cusps}), so the accuracy is improved greatly if we split the
integration interval into two parts and employ different quadratures for each.
The integration is split to use Gauss-Legendre on the interval $(0,r_1)$ and
Gauss-Laguerre on the interval $(r_1,\infty)$.

\subsubsection{Integration over the \texorpdfstring{$r_{23}^{-1}$} {1/r23} term}
%\subsection[Integration over the 1/r23 term]{Integration over the $r_{23}^{-1}$ term}
\label{sec:LongLongR23}
The other part of the $(\bar{S},\mathcal{L} \bar{S})$ integral contains the $r_{23}^{-1}$ term.

\beq
(\bar{S},\mathcal{L} \bar{S})_B = \pm \left(S^\prime, \left[ \frac{2}{r_{23}}\right] S\right)
\eeq

\todoi{Why exactly do we use perimetric coordinates?}

\noindent The volume element for this integral is $d\tau^\prime$ from equation \ref{}.  The integration also does not need to be converted to perimetric coordinates, so its form is
\begin{align}
(\bar{S},\mathcal{L} \bar{S})_B = \pm 8\pi^2 \int_0^\infty & \int_0^\infty \int_0^\infty \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_{|r_2 - r_3|}^{|r_2 + r_3|} \int_0^{2\pi}  S^\prime S \frac{2}{r_{23}} r_1 r_2 r_{13} r_{23} \nonumber \\
& \times d\phi_{12}\, dr_{23}\, dr_{13}\, dr_2\, dr_3\, dr_1.
\end{align}
\todoi{Check that we do this over $\phi_{12}$.}
\todoi{$\varphi$ instead of $\phi$}

The $r_{13}$ and $r_{23}$ integrals have finite limits, so here we use Gauss-Legendre quadrature.  Again, for the internal angular integration, this time over $\phi_{12}$, we use Chebyshev-Gauss quadrature.  The cusp in the $r_3$ integration is at $r_3 = r_1$, and the cusp in the $r_2$ integration is at $r_2 = r_3$.  Similar to before, we split up these integrations by using Gauss-Legendre before the cusp and Gauss-Laguerre after the cusp.

The $(\bar{C},\mathcal{L} \bar{S})$ and $(\bar{C},\mathcal{L} \bar{C})$ terms are integrated in the same manner as the $(\bar{S},\mathcal{L} \bar{S})$ integral just described.


\subsection{Short-Range -- Long-Range}
\label{sec:ShortLongInt}
We will consider only the $(\bar{\phi}_i,\mathcal{L} \bar{S}_\ell)$ integrations here, as the $(\bar{\phi}_i,\mathcal{L} \bar{C}_\ell)$ integrals are evaluated in the same manner. As in the case of long-range -- long-range integrations in section \ref{sec:LongLongInt}, we split up the integration into two parts -- one containing the $r_{23}^{-1}$ term and another containing the rest of the terms. The short-range terms have the added benefit of the possibility of the polynomial $r_{23}^{\,q_i}$ being present, which cancels the $r_{23}^{-1}$ term or gives it an overall positive power.


From \ref{eq:PhiBarLSBar2b}, \ref{eq:LS2} and \ref{eq:LSP2}, 

\begin{align}
\label{eq:PhiLSBarInt}
\nonumber (\bar{\phi}_i, L\bar{S}_\ell) &= \frac{2}{\sqrt{2}} \left(\phi_i,\mathcal{L} \bar{S}_\ell\right) \\
 &= \frac{2}{\sqrt{2}} \int \phi_i \left[ \left( \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}} + \frac{2}{r_{23}} \right)S_\ell \pm \left( \frac{2}{r_1} - \frac{2}{r_3} - \frac{2}{r_{12}} + \frac{2}{r_{23}} \right) S_\ell^\prime \right]  d\tau.
\end{align}

\subsubsection{Case I: \texorpdfstring{$q_i > 0$}{qi > 0}}
\label{sec:Swaveqigt0}
When $q_i > 0$ in $\phi_i$ (equation \ref{eq:PhiDef}), the power of $r_{23}$ is equal to or greater than 0.  Gaussian quadratures can safely integrate this type of term, so we integrate the full expression in equation \ref{eq:PhiLSBarInt}.
\begin{align}
\label{eq:PhiLSBarIntFull}
\nonumber (\bar{\phi}_i, L\bar{S}_\ell) =& \, \frac{2}{\sqrt{2}} \cdot 8\pi^2  \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_2|}^{|r_1 + r_2|} \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_0^{2\pi} \phi_i \\
&\times \left[ \left( \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}} + \frac{2}{r_{23}} \right)S_\ell \pm \left( \frac{2}{r_1} - \frac{2}{r_3} - \frac{2}{r_{12}} + \frac{2}{r_{23}} \right) S_\ell^\prime \right] \\
&\times r_2 r_3 r_{12} r_{13}\, d\phi_{23}\, dr_{13}\, dr_{12}\, dr_3\, dr_2\, dr_1
\end{align}

Similar to the long-long integrations from section \ref{sec:LongLongInt}, the $r_1$ integration is performed using the Gauss-Laguerre quadrature.  The $r_2$ integral is broken into two parts at the cusp of $r_2 = r_1$, with the Gauss-Legendre quadrature before the cusp and the Gauss-Laguerre quadrature after the cusp.  In the $r_3$ coordinate, there is a cusp at $r_3 = r_2$, so the integration is also split up into Gauss-Legendre before the cusp and Gauss-Laguerre after the cusp.  The finite intervals for $r_{12}$ and $r_{13}$ ensure that we can use Gauss-Legendre quadratures for these coordinates.  The $\phi_{23}$ integration uses the Chebyshev-Gauss quadrature.

\subsubsection{Case II: \texorpdfstring{$q_i = 0$}{qi = 0}}
When $q_i = 0$, the overall power of the $r_{23}^{-1}$ term is $-1$, so we cannot use the Gaussian quadratures in the form of \ref{eq:PhiLSBarIntFull}.  Similar to the long-long integrations, the $r_{23}^{-1}$ term is integrated separately, using the same type of integrations as equation \ref{eq:PhiLSBarIntFull}.  Refer to the previous section for the description of the quadratures used.
\begin{align}
\label{eq:PhiLSBarIntNoR23}
\nonumber (\bar{\phi}_i, L\bar{S}_\ell) =& \,\frac{2}{\sqrt{2}} \int \phi_i \left[ \left( \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}} \right)S_\ell \pm \left( \frac{2}{r_1} - \frac{2}{r_3} - \frac{2}{r_{12}} \right) S_\ell^\prime \right]  d\tau \\
\nonumber =&\, \frac{2}{\sqrt{2}} \cdot 8\pi^2  \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_2|}^{|r_1 + r_2|} \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_0^{2\pi} \phi_i \\
&\times \left[ \left( \frac{2}{r_1} - \frac{2}{r_2} - \frac{2}{r_{13}} \right)S_\ell \pm \left( \frac{2}{r_1} - \frac{2}{r_3} - \frac{2}{r_{12}} \right) S_\ell^\prime \right]  r_2 r_3 r_{12} r_{13}\, d\phi_{23}\, dr_{13}\, dr_{12}\, dr_3\, dr_2\, dr_1
\end{align}
\todoi{Volume element changes between this and the next equation}

The integration over the $r_{23}^{-1}$ term is done the same way as the second integration of the long-long matrix elements in section \ref{sec:LongLongInt}.  The $r_{23}$ in the $d\tau^\prime$ volume element cancels the $r_{23}^{-1}$ term.  Refer to section \ref{sec:LongLongR23} for a description of the quadratures used here.
\begin{align}
\label{eq:PhiLSBarIntR23}
(\bar{\phi}_i, L\bar{S}_\ell) =& \,\frac{2}{\sqrt{2}} \int \phi_i \left[ \frac{2}{r_{23}}\left(S_\ell \pm S_\ell^\prime\right) \right] d\tau^\prime  \nonumber \\
=&\, \frac{2}{\sqrt{2}} \cdot 8\pi^2  \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_3|}^{|r_1 + r_3|} \int_{|r_2 - r_3|}^{|r_2 + r_3|} \int_0^{2\pi} \phi_i \left[ \frac{2}{r_{23}}\left(S_\ell \pm S_\ell^\prime\right) \right]  \nonumber \\
&\times  r_1 r_2 r_{13} r_{23}\, d\phi_{12}\, dr_{23}\, dr_{13}\, dr_2\, dr_3\, dr_1
\end{align}

\todoi{Check that we do this over $\phi_{12}$.}



\subsection{Quadrature Points}
\label{sec:QuadraturePoints}


\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord\\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$ & 45 & 35 & 35 & 35 & 28 & 15 & & \\
 Long-long, $r_{23}^{-1}$ & 65 & 35 & 28 & 35 & 28 & 12 & 15 & 15 \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$ & 90 & 57 & 34 & 57 & 34 & 30 & 30 & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$ & 90 & 58 & 30 & 55 & 35 & 33 & 33 & 33 \\
 Long-short $q_i > 0$ & 90 & 57 & 34 & 57 & 34 & 30 & 30 & 30 \\
\bottomrule
\end{tabular}
\caption{Base set of effective coordinates for integrations}
\label{tab:BaseEffectiveCoords}
\end{table}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord\\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$			&  75 & 40 & 40 & 40 & 40 & 25 & & \\
 Long-long, $r_{23}^{-1}$				&  75 & 40 & 40 & 40 & 40 & 25 & 25 & 25 \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$	& 100 & 65 & 45 & 65 & 45 & 45 & 45 & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$	& 115 & 65 & 45 & 65 & 45 & 45 & 45 & 45 \\
 Long-short $q_i > 0$					& 100 & 65 & 45 & 65 & 45 & 45 & 45 & 45 \\
\bottomrule
\end{tabular}
\caption{Optimal set of effective coordinates for integrations}
\label{tab:OptimalEffectiveCoords}
\end{table}

Describing the number of points used in integrating the different coordinates 
in sections \ref{sec:LongLongInt} and \ref{sec:ShortLongInt} can be 
confusing, so we have taken to grouping the sets of points as in
\cref{tab:BaseEffectiveCoords,tab:OptimalEffectiveCoords}. Each column of 
this table is referred to as an ``effective coordinate''.

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{c c c c c c c c c}
\toprule
 & Coord & Coord & Coord & Coord & Coord & Coord & Coord & Coord \\
Integral & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
 Long-long, no $r_{23}^{-1}$ & $x$ Lag & $y$ Lag & $z$ Lag & $r_3$ Leg & $r_3$ Leg & $r_{13}$ Leg & & \\
 Long-long, $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $\phi_{12}$ Che & $r_{13}$ Leg & $r_{23}$ Leg \\
\midrule
 Long-short $q_i = 0$, no $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $r_{13}$ Leg & \\
 Long-short $q_i = 0$, $r_{23}^{-1}$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $\phi_{13}$ Che & $r_{23}$ Leg \\
 Long-short $q_i > 0$ & $r_1$ Lag & $r_2$ Leg & $r_2$ Lag & $r_3$ Leg & $r_3$ Lag & $r_{12}$ Leg & $r_{13}$ Leg & $\phi_{23}$ Che \\
\bottomrule
\end{tabular}
\caption{Optimal set of effective coordinates for integrations}
\label{tab:EffectiveCoords}
\end{table}


\subsection{Cusp Behavior}
\label{sec:Cusps}
\todoi{Discussion about the existence of the cusps in the integrand, possibly with graphs illustrating them.}

The short-long and long-long matrix element integrals have cusps in the
integrands that must be dealt with. As an example, consider
$(\bar{\phi}_i, L\bar{C}_0)$ from the S-wave. Using the notation of
\cref{sec:LongLongR23}, where we are only computing the $r_{23}^{-1}$ terms
(given fully later in \cref{eq:LCBar} on \pageref{eq:LCBar}),
\begin{align}
(\bar{\phi}_i, L\bar{C}_0)_B = 8\pi^2 & \int_0^\infty \int_0^\infty \int_0^\infty \int_{|r_1 - r_2|}^{|r_1 + r_2|} \int_{|r_2 - r_3|}^{|r_2 + r_3|} \int_0^{2\pi} \bar{\phi}_i (\mathcal{L} \bar{C})_B  \nonumber \\
& \times r_1 r_3 r_{13} r_{23}\, d\phi_{13}\, dr_{23}\, dr_{12}\, dr_3\, dr_2\, dr_1.
\end{align}
Due to the integration limits of the $r_{13}$ and $r_{23}$ integrations, the
$r_2$ integrand has a cusp at $r_2 = r_1$, and the $r_3$ integrand has a cusp
at $r_3 = r_2$.

\Cref{fig:cusp} shows one such example of this cusp behavior for the $r_2$ 
integrand. All inner integrations ($\phi_{13}$, $r_{23}$, $r_{12}$, and $r_3$)
are performed with a full set of integration points as described in
\cref{tab:OptimalEffectiveCoords}.

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{cusp}
	\caption[Example of cusp in S-wave short-long integration]{Example of cusp
in S-wave short-long integration for $1/r_{23}$ term of
$(\bar{\phi}_3, \mathcal{L}\bar{C}_0)$ with \mbox{$r_1 = 18.201$}}
	\label{fig:cusp}
\end{figure}

The $r_2$ integration could be performed using just the Gauss-Laguerre 
quadrature, since we are integrating over $[0,\infty)$. However, the cusp 
makes this integration slowly convergent. We instead split the integration 
interval into two parts and employ different quadratures for each. The 
integration is split to use Gauss-Legendre (\cref{eq:GaussLegGen}) on the 
interval $[0,r_1]$ and Gauss-Laguerre (\cref{eq:GaussLagGen}) on the interval 
$[r_1,\infty)$. Likewise, the $r_3$ integration is split into the intervals
$[0,r_2]$ and $[r_2,\infty)$.

Doing this splitting requires many more function evaluations, so we use an 
approximation whenever $r_1$ is large enough. Specifically when $r_1$ is 
greater than a chosen distance, we use Gauss-Laguerre over the entire
$[0,\infty)$ range. 

\todoi{Mention ECGs?}


In the example given in \cref{fig:cusp}, at $r_2 = 100$, the $r_2$ integrand 
is approximately $10^{-57}$, while at $r_2 = 25$, the integrand is 
approximately $10^{-9}$.


For the S-wave, all runs were performed with the cusp parameters set at
$r_1 = 100$. When $r_1 > 100$, the $r_2$ and $r_3$ integrations are done using only 
Gauss-Laguerre, since the cusp is considered unimportant at that distance. 
When $r_1 \leq 100$, we use Gauss-Legendre before the cusp and Gauss-Laguerre 
after the cusp, as described in \cref{sec:LongLongNoR23,sec:LongLongR23}. 

Van Reeth and Humberston used cusp parameters set at $r_1 = 25$, with little 
apparent loss of precision over using $r_1 = 100$ \cite{VanReethPrivate}. 
With $\kappa$ set at $0.1$, several tests were performed with different cusp 
parameters. With cusp parameters of 100, for $\omega = 6$, we are able to use 
910 terms, giving a phase shift of $-0.427$. Doing the same run with cusp 
parameters of 25 allows us to use the same number of terms and gives a phase 
shift of $-0.427$ as well. In this particular example, the two sets of cusp 
parameters (25 and 100) differ in the sixth decimal place. As an example, for 
the inverse Kohn, $\delta = -0.42707291$ for the cusp parameters of 100, 
while $\delta = -0.42707370$ for cusp parameters of 25. To the required 
accuracy, the cusp parameters of 25 are sufficient for this example.

Smaller values for these cusp parameters were also tried, and these tests are 
summarized in table \ref{tab:SWaveCuspParameters}. When the cusp parameters 
are too small, such as the cases of 5, 10 and 15, the phase shifts do not 
converge well and start to diverge much earlier. The second column in this 
table shows the number of terms we can use for each set, but the number of 
terms to use is not exact due to the lack of good convergence, so the values 
here are approximate for small cusp parameters. The phase shift given for 
these three tests are therefore approximate as well. From this table, we 
conclude that 20 is the minimum value needed, at least for the $\kappa = 0.1$ 
case tested here for the $S$-wave. To be conservative, runs use cusp 
parameters of 100. 



\begin{table}[H]
\centering
\begin{tabular}{c c c}
\toprule
Cusp parameter & Terms possible & $\delta_0^+$ \\
\midrule
 5 & 531 & -0.4281 \\
10 & 684 & -0.4071 \\
15 & 684 & -0.4267 \\
20 & 910 & -0.4271 \\
25 & 910 & -0.4271 \\
100 & 910 & -0.4271 \\
\bottomrule
\end{tabular}
\caption{S-wave cusp parameters for $\kappa = 0.1$ at $\omega = 6$}
\label{tab:SWaveCuspParameters}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c c}
\toprule
Cusp parameter & $\delta_1^+$ \\
\midrule
 25 & 1.142923114 \\
 50 & 1.142923115 \\
100 & 1.142924602 \\
\bottomrule
\end{tabular}
\caption{P-wave cusp parameters for $S$ matrix $\kappa = 0.7$ at $\omega = 5$}
\label{tab:PWaveCuspParameters}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{c c c}
\toprule
Cusp parameter & $\delta_2^+$ & Time (hours) \\
\midrule
100 & 0.173 924 955 404 676 & 7.8 \\
150 & 0.173 924 955 404 797 & 8.9 \\
200 & 0.173 924 955 404 809 & 10.2 \\
250 & 0.173 924 955 404 748 & 10.8 \\
\bottomrule
\end{tabular}
\caption{D-wave cusp parameters for $S$ matrix $\kappa = 0.6$ at $\omega = 3$}
\label{tab:DWaveCuspParameters}
\end{table}


\subsection{Extra Exponential}
To further improve the convergence of the short-long matrix elements in 
equation \cref{eq:GeneralKohnMatrix}, we investigated the integrands.
The biggest source of 
difficulty in converging these results comes through the Gauss-Laguerre 
quadratures in the $r_1$, $r_2$ and $r_3$ integrations. Specifically, the 
region near the origin is not adequately represented. The integrands fall off 
quickly due to the exponential falloffs in $r_1$, $r_2$ and $r_3$, so it is 
not as important to have abscissae far away from the origin.
We are using approximately 7 times as many integration points total as the
earlier Kohn and inverse Kohn work \cite{VanReeth2003,VanReeth2004}
(see \cref{}), but this brute force approach of adding quadrature points can 
increase the computational time greatly. We took another approach to 
further increase the accuracy. For each of the Gauss-Laguerre quadratures, we 
introduce an extra $e^{-\lambda r_i}$ and remove it with $e^{\lambda r_i}$ 
after the quadrature, bringing our abscissae closer to the origin without 
increasing the number of integration points.

The basic form of the Gauss-Laguerre quadrature is given in \cref{eq:GaussLag} as
\beq
\label{eq:GaussLag}
\int_0^\infty e^{-x} f(x) dx \approx \sum_{i=1}^n w_i f(x_i).
\eeq
Introducing an extra $e^{-\lambda x}$ and removing it with $e^{\lambda x}$, we can bring the abscissae closer in by
\beq
\label{eq:GaussLagLambda}
\int_0^\infty e^{-x} f(x) dx \approx \sum_{i=1}^n \frac{w_i}{\lambda} f\! \left(\frac{x_i}{\lambda}\right) \ee^{\lambda x_i}.
\eeq

As an example, the full expression for the $r_2$ Gauss-Laguerre quadrature is given \todo{$y_i$ defined?} by
\beq
\int_a^\infty e^{-\beta r_2} f(r_2) dr_2 \approx \frac{e^{-\beta a}}{\beta} \sum_{i=1}^n w_i f\left(\frac{y_i}{\beta}+a\right).
\eeq
and with the exponential in $\lambda$, this becomes
\beq
\int_a^\infty e^{-\beta r_2} f(r_2) dr_2 \approx \frac{e^{-a (\beta + \lambda)}}{\beta + \lambda} \sum_{i=1}^n w_i f\left(\frac{r_2 + a(\beta + \lambda)}{\beta + \lambda} \right) \ee^{\lambda r_2}.
\eeq

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{lambda}
	\caption{Effect of introducing $\lambda$ in $r_1$ exponential}
	\label{fig:lambda}
\end{figure}

\Cref{fig:lambda} shows the effect of introducing this $\lambda$ into the $r_1$
exponential. We use 50 quadrature points for each curve, meaning that there 
are points represented past the cutoff of 18 au in the graph. For the black 
curve with $\lambda = 0$, the curve is very jagged around the peak of 
approximately 4 au. With $\lambda = 1.0$, the curve is much smoother, and as
$\lambda$ is increased to 2.0, the peak is represented even better. If $\lambda$
is too large, the area near the origin may be overrepresented and the area 
farther out may be underrepresented. We have chosen $\lambda = 1.0$ for all 
of our runs for the $r_1$, $r_2$ and $r_3$ coordinates, which gives better
representation near the origin but does not  run the risk of neglecting the
small contribution of the curve for large $r_i$ values. Our $D$-wave and
general long-range codes could use different $\lambda$ for each of the
$r_1$, $r_2$ and $r_3$ integrations, but we set them equal here. The matrix
elements converge better for $\lambda = 1.0$ than for $\lambda = 0$.
\todoi{Can we quantify this?}


\section{Phase Shifts}
\label{sec:CompPhase}

\subsection{Phase Shift Convergence and Singularities}
Schwartz singularities are well-documented \cite{Lucchese1989,Cooper2009} and 
are non-physical ``resonances''. To avoid these singularities, we use 
multiple forms of the Kohn method: Kohn, inverse Kohn, generalized Kohn, 
generalized S-matrix complex Kohn, and generalized T-matrix complex Kohn. 
There may also be spurious results if the matrix elements are not converged, 
with one example being the feature at approximately 600 terms in
\cref{fig:OriginalPhaseShifts-pvr} for the inverse Kohn. With the original
ordering of the short-range terms, the phase shifts always break up at around
1100 terms, even when the matrix elements are converged. To alleviate this 
problem, we have implemented the Todd algorithm.

\subsection{Todd Algorithm Applied to the Scattering Problem}
\label{sec:ToddScattering}
\todoi{Reword for other than just $S$}
From the bound state calculation using Todd's algorithm mentioned in
\cref{sec:ToddBound}, we know which short-range Hylleraas terms approximate 
the wavefunction of PsH best. We have observed that the short-short
terms used in the same order as the output of the Todd algorithm will 
generate well-converged phase shifts. The separate code to determine the 
phase shifts uses the output from the bound state code. Again, the phase 
shift is plotted with respect to the number of short-range terms. The result 
is in \cref{fig:ToddPhaseShifts-pvrplus5}. The phase shifts are 
converged well until term 1216. A magnification of the graph in the small 
region around this term is provided in the inset graph.

At term 1216, the Kohn method result starts to diverge from the others. 
Shortly after term 1280, the five different methods for $\tau = 0.0 - 0.5$ 
start to diverge slightly. The differences here are very small (on the order 
of $10^{-5}$), unlike in \cref{fig:OriginalPhaseShifts-pvrplus5}. The 
jump in phase shifts seen in \cref{fig:OriginalPhaseShifts-pvrplus5} 
does not take place until much later and is not as large. Near 1650 terms, 
the phase shifts are not well converged, but they are also not as nearly
ill-behaved as the results in \cref{fig:OriginalPhaseShifts-pvrplus5}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{ToddPhaseShifts-pvrplus5}
	\caption{Phase shifts with Todd terms and 5 extra points}
	\label{fig:ToddPhaseShifts-pvrplus5}
\end{figure}


\subsection{Todd Algorithm with Optimized Quadrature}


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{ToddOrderKohnPhaseShiftsOpt7}
	\caption{Phase shifts with Todd ordering and optimal set of points}
	\label{fig:ToddOrderKohnPhaseShiftsOpt7}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{ToddPhaseShifts-pvrbest}
	\caption{Phase shifts with resorted Todd terms and optimal set of points}
	\label{fig:ToddPhaseShifts-pvroptimized}
\end{figure}

Section \ref{sec:QuadraturePoints} has a discussion on the ``optimal set'' of quadrature points. When this optimal set is used with Todd's method, a very stable set of phase shifts is created. The graph in figure \ref{fig:ToddOrderKohnPhaseShiftsOpt7} shows how the phase shifts are well converged up to 1450 terms. At approximately term 1450, the phase shifts for the various Kohn methods begin to diverge. The Kohn results begin to diverge earlier than the other three variants in figure \ref{fig:ToddOrderKohnPhaseShiftsOpt7}. Notice that a similar behavior occurs in figure \ref{fig:ToddPhaseShifts-pvrplus5} at 1216 terms.

We have not developed an automated method of determining where this divergence occurs, so a visual inspection of the graph is needed. Graphs like the above are created in gnuplot, but we have developed a MATLAB script that makes this much easier. This script loads and plots data from all variations on the Kohn method, including the 35 values of $\tau$ for the generalized Kohn. MATLAB allows zooming of figures, so it is simple to find the term where the phase shifts begin to diverge.

For the extrapolation method discussed in section \ref{sec:Extrapolation}, it is useful to reorder the terms in the original ordering with increasing $\omega$. The first 1450 terms of figure \ref{fig:ToddOrderKohnPhaseShiftsOpt7} are used and then reordered, leading to the graph in figure \ref{fig:ToddPhaseShifts-pvroptimized}. There is a very small difference between the Kohn methods, but they do not diverge as in figure \ref{fig:ToddPhaseShifts-pvroptimized} after 1450 terms.


\todoi{Table of number of terms chosen from paper}


\subsection{Generalized Kohn}
\label{sec:CompGenKohn}
As mentioned in section \ref{sec:GenKohn}, the generalized Kohn method described by Cooper et al.\ allows for an adjustable parameter $\tau$ to be used. For our calculations of the Kohn and inverse Kohn, we set $\tau = 0$ and $\tau = \frac{\pi}{2}$, respectively. Before we started using the generalized Kohn method, it was difficult to tell if the difference between the Kohn and inverse Kohn phase shifts was due to a Schwartz singularity or a problem with linear dependence. With the generalized Kohn, we have a large number of individual Kohn methods to compare the results with.

\todo{Update}
The phase shift program steps through $\tau = 0$ to $\tau = 3.0$ in increments of $0.1$ for each N. There is also an additional modified phase shift program that steps through $\tau = 0.05$ to $\tau = 3.05$, also in increments of $0.1$, to get points in between the first set. The code can also be easily modified to use any value of $\tau$, which was used in figure \ref{fig:PhaseTau}.

The Kohn methods do not require a recalculation of the entire problem for each Kohn method. Once the original matrix equation (\cref{eq:GeneralKohnMatrix}) is calculated with the $\textbf{u}$ in \cref{eq:uKohn}, along with the $(\bar{S},\mathcal{L} \bar{S})$ term, the elements are reused to perform the other Kohn method calculations in \cref{sec:KohnApplied}.



\setlength{\abovecaptionskip}{0pt}   % 0.5cm as an example

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{PhaseTau}
	\caption[Phase shifts versus $\tau$ in the generalized Kohn method]{Phase shifts versus $\tau$ in the generalized Kohn method for $^1$S Ps-H scattering $(\omega = 7, \kappa = 0.1)$}
	\label{fig:PhaseTau}
\end{figure}

\todoi{Redo \cref{fig:PhaseTau} in IPython}

\todoi{Put in \tt{fig:swave-phase-divergence} from PRA paper (and already in Dissertation.ipynb)}




\section{Convergence and Extrapolations}
\label{sec:Extrapolations}

For comparing quantities such as the convergence of the matrix elements or 
the convergence of the differential cross sections, we use the percent
difference. There is no standard symbol for this, so we define it as
\begin{equation}
\label{eq:PercentDiff}
\% \text{ Diff} = \rhd(a,b) = \left| \frac{a - b}{(a + b) / 2} \right| \times 100\%.
\end{equation}
When we can compare convergence with respect to $\omega$, we define a
convergence ratio as
\begin{equation}
\label{eq:ConvRatio}
\mathcal{R}(\omega) = \frac{\delta_\ell^\pm(\omega)-\delta_\ell^\pm(\omega-1)}
  {\delta_\ell^\pm(\omega-1)-\delta_\ell^\pm(\omega-2)}.
\end{equation}
This is similar to the inverse of ratio for the energy eigenvalues given in
Ref.~\cite{Yan1999}. If $\mathcal{R}(\omega) \geq 1$, there is no convergence
pattern. A ratio of less than 1 shows convergence but does not guarantee a
reliable extrapolation. We find that $\mathcal{R}(\omega) \lesssim 0.5$ is
needed to properly extrapolate phase shifts.

\subsection{Extrapolation Description}

\todoi{Also extrapolate scattering lengths}

The tangent of the phase shifts is fitted to the function
\beq
\label{eq:PhaseExtrap}
\tan \delta_\ell^\pm(\omega) = \tan \delta_\ell^\pm(\omega \to \infty) + \frac{c}{\omega^p}.
\eeq
The $c$ and $p$ in this equation are fitting parameters. When plotted with respect to $\omega^{-p}$, the tangents of the phase shifts form nearly a straight line, with the y-intercept being the tangent of the extrapolated value of the phase shift, $\tan \delta^\pm(\omega \to \infty)$.

In Van Reeth and Humberston, the extrapolation is done using $\omega = 3$ through $\omega = 6$ \cite{VanReeth2003}. Our results go to $\omega = 7$, so we have completed the extrapolation using the sets $\omega = 3-6$, $\omega = 3-7$ and $\omega = 4-7$. The smallest residuals are normally found with the set $\omega = 4-7$. The values of the extrapolated phase shifts using this method are in table \ref{tab:SWavePhase}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{extrap-phase}
	\caption[Extrapolation for $^1$S at $\kappa = 0.01$]{Extrapolation for $^1$S at $\kappa = 0.01$. The extrapolation without resorting the short-range terms into increasing $\omega$ is given in (a), and the resorted version is in (b).}
	\label{fig:extrap-phase}
\end{figure}

As described in section \ref{sec:ToddScattering}, we omit certain terms using Todd's method. The output of this method from the bound state program described in section \ref{sec:ToddBound} is not ordered in terms of increasing $\omega$. This leads to difficulties when attempting to do the extrapolation in equation \ref{eq:PhaseExtrap}. The tangent of the phase shifts cannot be fitted to a straight line in this order. If we reorder the short-range terms back into their original order while still omitting terms, the tangent of the phase shifts can now be fitted to this straight line, as can be seen in figure \ref{fig:extrap-phase}. The reordering does not affect the phase shifts in any case that we tested, since more terms are normally omitted in the scattering problem than just the bound state problem.

\subsection{Extrapolation Program}
I wrote a Python \cite{Python} program to extrapolate the phase shifts for a run for all Kohn method variants used, including the 35 values of $\tau$ used in each of the generalized Kohn, generalized S-matrix, and generalized T-matrix. The extrapolation is performed with a least-squares fitting using the \texttt{polyfit} function of the SciPy package \cite{SciPy}. This program can do the extrapolation over any interval of $\omega$ values requested. The extrapolations from the 109 total Kohn variants are compared to see if there is any large discrepancy between them, indicating numerical instability. The phase shift can have a singularity in the generalized Kohn method as seen in figure \ref{fig:PhaseTau}, so care must be taken to ensure extrapolations are not taken around this interval.



\section{Resonance Fitting}
\label{sec:ResonanceFit}

To find the resonance parameters (positions and widths), the phase shifts for multiple energy values are fitted to \cref{eq:ResonanceCurve}. There have been multiple programs described in the literature \cite{Tennyson1984, Stibbe1998, Sochi2013} to do these fittings, which I was not aware of when I originally wrote code to do these. Some of the difficulty with this type of fitting is choosing appropriate guesses for the resonance parameters. Ref.~\cite{Sochi2013} describes methods that some groups use to try to identify resonance parameters.

This nonlinear fitting can be difficult for general programs. I first tried the \texttt{Fit} and \texttt{FindFit} functions of Mathematica\textsuperscript{\textregistered} 8.0 \cite{Mathematica}, but these were unable to fit our data to this complicated nonlinear function. I next tried the \texttt{curve\_fit} function of SciPy \cite{SciPy} in Python\textsuperscript{\textregistered} 2.6 \cite{Python}, which was better able to fit the nonresonant (polynomial) part but had trouble with the $\arctan$ terms.

With the help of Ryan Bosca, I wrote a MATLAB\textsuperscript{\textregistered} \cite{matlab} script that uses the \texttt{nlinfit} routine of MATLAB. \texttt{nlinfit} is specifically designed for fitting to nonlinear functions, and its robust option allows for a variety of weighting functions to be used. This script does the fitting for all eight of the possible weightings: Bisquare, Andrews, Cauchy, Fair, Huber, Logistic, Talwar and Welsch. This fitting routine is also not as sensitive to the initial guesses as the Mathematica and SciPy routines.

Later on, I adapted this resonance fitting code to be called from IPython \cite{ipython} using the mlabwrap \cite{mlabwrap} Python to MATLAB wrapper. This allows fits and graphs of the fittings to be in the same IPython notebook, including the flexibility of querying the MySQL database for the phase shift data for any partial wave at any number of terms (see \cref{sec:XMLSQLIPy}). The mlabwrap package is difficult to install properly, requiring a compilation against MATLAB. The mlabwrap-purepy package \cite{mlabwrappurepy} has been created to simplify this, but we have not tried it yet.

The results of fitting the phase shifts from the $S$ matrix are shown in \cref{fig:swave-resonance-uncorrected-fits1,fig:swave-resonance-uncorrected-fits2} for each of the weighting functions, and the resonance parameters are given in each subfigure. There is good agreement between the fits performed with each of the weighting functions, and the Fair is the furthest from the others. In our testing, the Cauchy is consistently one of the best choices for fitting. In addition, \cref{fig:swave-resonance-uncorrected-res} has plots of the residuals (the absolute value of the difference between the fitted curve and the actual phase shifts). Each graph also has the residual sum of squares (RSS) calculated. The RSS gives an easy way to compare the performance of the weightings with each other. The Fair has a notably larger RSS than all of the other fittings.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-uncorrected-fits1}
	\caption{First set of uncorrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-uncorrected-fits1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-uncorrected-fits2}
	\caption{Second set of uncorrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-uncorrected-fits2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-uncorrected-res}
	\caption{Residuals for uncorrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-uncorrected-res}
\end{figure}

One important thing to notice with the fits in \cref{fig:swave-resonance-uncorrected-fits1,fig:swave-resonance-uncorrected-fits2} is that the $S$ matrix phase shifts extend above the fitted curve at the resonances before matching up again on the right side of the resonance below $-3.0$. The red curves still fit to the data well, but the fits can be improved by correcting for these. 

In \cref{eq:ResonanceCurve}, the first three polynomial terms form the background, and the two $\arctan$ terms represent the resonances. The range of $\arctan$ is $(-\frac{\pi}{2},\frac{\pi}{2})$, so the $\arctan$ parts of this model cannot bring the phase shift from the background (near 2.0) all the way to 0.0, as it can only add up to $\frac{\pi}{2}$ to the background. It is important to realize that the Kohn methods will only return phase shifts in a certain range. From \cref{eq:GenKohnL}, we are not finding the phase shifts directly but are rather calculating $\tan \delta_\ell$. The phase shifts found this way sometimes have to have $\pi$ added or subtracted to get them in the proper range.

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{swave-phases-reson-pi}
	\caption[$^1$S resonance data showing correction]{$^1$S resonance data showing raw data from complex Kohn and the corrected version. The solid gray line shows the polynomial background, and the vertical dashed lines give the calculated resonance positions.}
	\label{fig:swave-phases-reson-pi}
\end{figure}

\Cref{fig:swave-phases-reson-pi} shows the results of subtracting $\pi$ from phase shifts that are more than $\frac{\pi}{2}$ above the background. The original and the corrected data are both shown on this plot, and the background is given as a gray line. Note that the slope of the original data changes when crossing over the vertical dashed lines, which gives the resonance positions. When corrected, these upper points are moved down to their appropriate place, shown as x's on the graph. These match up better with the fitting curve. This fitting is an iterative process, because the background polynomial has to be determined with \cref{eq:ResonanceCurve} before we can do this correction. Then the phase shifts are fitted again after performing the correction.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-fits1}
	\caption{First set of corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-fits1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-fits2}
	\caption{Second set of corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-fits2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{swave-resonance-corrected-res}
	\caption{Residuals for corrected resonance fitting graphs for $^1$S at $\omega = 7$}
	\label{fig:swave-resonance-corrected-res}
\end{figure}

Finally, this corrected and re-fitted data is shown in \cref{fig:swave-resonance-corrected-fits1,fig:swave-resonance-corrected-fits2}. The different weighting methods agree extremely well now, with the only notable differences being in the $^2\Gamma$ width of the second resonance for the Bisquare, Andrews, and Talwar fits. This is still a very small difference, and the other resonance parameters are almost all identical. \Cref{fig:swave-resonance-corrected-res} gives the residuals and RSS for the corrected phase shifts, similar to \cref{fig:swave-resonance-uncorrected-res}. The Bisquare, Andrews, and Talwar weightings have larger RSS values than the other weightings, indicating that their fits are not quite as accurate, but they are still relatively accurate.

The differences between the different weighting methods gives us one way to determine errors for our resonance parameters. Additionally, comparing the different Kohn methods gives us an idea of the error. The real-valued generalized Kohn methods give more disagreement, so we compute the resonance parameters for each of these Kohn methods. We also have to take into account Schwartz singularities. In \cref{fig:schwartz-singularity}(a) on \pageref{fig:schwartz-singularity}, the second resonance parameters are not as accurate. The fitting routine described here chooses fitting parameters of $^2E_R = \SI{5.0295}{eV}$ and $^2\Gamma = \SI{0.06011}{eV}$.
In \cref{fig:schwartz-singularity}(b), where there are no Schwartz singularities, $^2E_R = \SI{5.0278}{eV}$ and $^2\Gamma = \SI{0.06075}{eV}$. This highlights the importance of using multiple Kohn methods and trying to detect Schwartz singularities. After removing obvious Schwartz singularities, the mean value of the different Kohn methods for all weightings is taken for each resonance parameter, and these are the results listed for each of the partial waves in \cref{sec:SWaveResonances,sec:PWaveResonances,sec:DWaveResonance,sec:FWaveResonance}. The errors given in these tables are simply the standard deviation with all Kohn methods and weightings used for each resonance parameter. For this, we do not use the generalized $S$ or $T$ matrices, because using these would decrease the error, as they agree to a significant precision.

\todoi{IPython notebook?}



\biblio
\end{document}